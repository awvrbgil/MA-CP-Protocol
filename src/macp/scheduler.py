"""
===============================================================================
å¤šAIåä½œè°ƒåº¦å™¨ v5.0 - ç»ˆæžä¼˜åŒ–ç‰ˆ (å•æ–‡ä»¶ç‰ˆæœ¬)
MACP: Multi-Agent Collaboration Platform (å¤šAIåä½œå¹³å°)
===============================================================================

æ ¸å¿ƒåŠŸèƒ½ï¼š
â”œâ”€â”€ ðŸ¤– AIè¾©è®ºç³»ç»Ÿ - æ”¯æŒ9ç§ä¸“ä¸šè§’è‰²ï¼Œå¤šå›žåˆæ™ºèƒ½è¾©è®º
â”œâ”€â”€ ðŸŽ¯ å…±è¯†åº¦æ£€æµ‹ - AIæ·±åº¦åˆ†æžï¼Œå®žæ—¶ç›‘æŽ§è¾©è®ºå…±è¯†
â”œâ”€â”€ ðŸ¢ æµ·é¾Ÿæ±¤æ¸¸æˆ - AIæŽ¨ç†é—®ç­”äº’åŠ¨æ¨¡å¼
â”œâ”€â”€ ðŸ“Š å¹¶è¡Œæé—® - åŒæ—¶å‘å¤šä¸ªAIæ¨¡åž‹æé—®
â”œâ”€â”€ ðŸ”„ æ™ºèƒ½ç»“æŸ - å…±è¯†åº¦è¾¾70%è‡ªåŠ¨æ€»ç»“
â””â”€â”€ ðŸ“ åŽ†å²è®°å½• - å®Œæ•´çš„å¯¹è¯å’Œè¾©è®ºä¿å­˜

æŠ€æœ¯ç‰¹æ€§ï¼š
â”œâ”€â”€ ðŸ—ï¸ æ¨¡å—åŒ–æž¶æž„ - å•æ–‡ä»¶æ•´åˆï¼Œæ˜“äºŽéƒ¨ç½²
â”œâ”€â”€ ðŸ” ç±»åž‹å®‰å…¨ - å®Œæ•´çš„ç±»åž‹æ³¨è§£
â”œâ”€â”€ ðŸ“‹ é”™è¯¯å¤„ç† - å®Œå–„çš„å¼‚å¸¸ç®¡ç†å’Œæ—¥å¿—
â”œâ”€â”€ âš¡ æ€§èƒ½ä¼˜åŒ– - å¹¶è¡Œå¤„ç†å’Œæ™ºèƒ½ç¼“å­˜
â””â”€â”€ ðŸŽ¨ ç”¨æˆ·å‹å¥½ - ç›´è§‚çš„å‘½ä»¤è¡Œç•Œé¢

ä½¿ç”¨çŽ¯å¢ƒï¼š
â”œâ”€â”€ OllamaæœåŠ¡ - æœ¬åœ°AIæ¨¡åž‹è¿è¡ŒçŽ¯å¢ƒ
â”œâ”€â”€ Python 3.7+ - è¿è¡ŒçŽ¯å¢ƒè¦æ±‚
â””â”€â”€ requestsåº“ - ç½‘ç»œè¯·æ±‚ä¾èµ–

ä½œè€…ï¼šåŒ¿åå¼€å‘è€…
åˆ›å»ºæ—¶é—´ï¼š2026å¹´1æœˆ8æ—¥
ç‰ˆæœ¬ï¼šv5.0
===============================================================================
"""

import concurrent.futures
import json
import time
from datetime import datetime
import os
import sys
import re
import logging
from typing import Dict, Any, List, Optional, Tuple

# ==================== ã€ä¾èµ–æ£€æŸ¥ã€‘ ====================
try:
    import requests
except ImportError:
    print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–åº“ 'requests'ï¼Œè¯·è¿è¡Œ: pip install requests")
    sys.exit(1)

# ============ ç³»ç»Ÿåˆå§‹åŒ–å’Œå…¼å®¹æ€§å¤„ç† ============

# å¤„ç†Windowsç³»ç»Ÿçš„ç¼–ç é—®é¢˜ï¼Œç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸
# Windowsé»˜è®¤ä½¿ç”¨GBKç¼–ç ï¼Œè€ŒPythonå­—ç¬¦ä¸²æ˜¯UTF-8
if os.name == 'nt':  # æ£€æŸ¥æ˜¯å¦ä¸ºWindowsç³»ç»Ÿ
    import io
    # é‡æ–°åŒ…è£…æ ‡å‡†è¾“å‡ºæµï¼Œä½¿ç”¨UTF-8ç¼–ç 
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# å¯¼å…¥ç½‘ç»œè¯·æ±‚åº“ï¼Œç”¨äºŽä¸ŽOllama APIé€šä¿¡
import requests

# ==================== ã€è‡ªå®šä¹‰å¼‚å¸¸ç±»ã€‘ ====================
# å®šä¹‰MACPç³»ç»Ÿä¸“ç”¨çš„å¼‚å¸¸ç±»åž‹ï¼Œä¾¿äºŽé”™è¯¯å¤„ç†å’Œè°ƒè¯•

class AICouncilException(Exception):
    """AIå§”å‘˜ä¼šè°ƒåº¦å™¨åŸºç¡€å¼‚å¸¸ç±»

    æ‰€æœ‰MACPç›¸å…³å¼‚å¸¸çš„åŸºç±»ï¼Œæä¾›ç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†æŽ¥å£
    """
    pass

class OllamaConnectionError(AICouncilException):
    """OllamaæœåŠ¡è¿žæŽ¥é”™è¯¯

    å½“æ— æ³•è¿žæŽ¥åˆ°Ollama APIæœåŠ¡æ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    é€šå¸¸å‘ç”Ÿåœ¨OllamaæœåŠ¡æœªå¯åŠ¨æˆ–ç½‘ç»œè¿žæŽ¥é—®é¢˜æ—¶
    """
    def __init__(self, message: str = "æ— æ³•è¿žæŽ¥åˆ°OllamaæœåŠ¡"):
        super().__init__(message)

class ModelNotFoundError(AICouncilException):
    """AIæ¨¡åž‹æœªæ‰¾åˆ°é”™è¯¯

    å½“è¯·æ±‚çš„AIæ¨¡åž‹åœ¨Ollamaä¸­ä¸å­˜åœ¨æ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    åŒ…å«å…·ä½“çš„æ¨¡åž‹åç§°ä¿¡æ¯ï¼Œä¾¿äºŽç”¨æˆ·ä¸‹è½½ç›¸åº”æ¨¡åž‹
    """
    def __init__(self, model_name: str):
        super().__init__(f"æ¨¡åž‹ '{model_name}' æœªæ‰¾åˆ°")
        self.model_name = model_name

class InvalidRoleError(AICouncilException):
    """æ— æ•ˆè§’è‰²é”™è¯¯

    å½“ç”¨æˆ·é€‰æ‹©çš„è¾©è®ºè§’è‰²ä¸å­˜åœ¨æˆ–æ— æ•ˆæ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    åŒ…å«å…·ä½“çš„è§’è‰²åç§°ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·é€‰æ‹©æ­£ç¡®çš„è§’è‰²
    """
    def __init__(self, role_name: str):
        super().__init__(f"æ— æ•ˆè§’è‰²: '{role_name}'")
        self.role_name = role_name

class ConsensusTimeoutError(AICouncilException):
    """å…±è¯†æ£€æµ‹è¶…æ—¶é”™è¯¯

    å½“AIå…±è¯†åˆ†æžè¿‡ç¨‹è¶…æ—¶æˆ–å¤±è´¥æ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    é€šå¸¸åœ¨ç½‘ç»œè¯·æ±‚è¶…æ—¶æˆ–AIåˆ†æžæœåŠ¡å¼‚å¸¸æ—¶å‘ç”Ÿ
    """
    def __init__(self, message: str = "å…±è¯†æ£€æµ‹è¶…æ—¶"):
        super().__init__(message)

class ConfigurationError(AICouncilException):
    """é…ç½®é”™è¯¯

    å½“ç³»ç»Ÿé…ç½®å‡ºçŽ°é—®é¢˜æ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    ä¾‹å¦‚é…ç½®æ–‡ä»¶æŸåã€é…ç½®é¡¹æ— æ•ˆç­‰æƒ…å†µ
    """
    def __init__(self, message: str = "é…ç½®é”™è¯¯"):
        super().__init__(message)

# ==================== ã€æ—¥å¿—ç³»ç»Ÿã€‘ ====================
# ç»Ÿä¸€çš„æ—¥å¿—è®°å½•ç³»ç»Ÿï¼Œç”¨äºŽè·Ÿè¸ªç³»ç»Ÿè¿è¡ŒçŠ¶æ€ã€é”™è¯¯å’Œæ€§èƒ½æŒ‡æ ‡

class Logger:
    """MACPæ—¥å¿—ç®¡ç†å™¨

    æä¾›åˆ†çº§æ—¥å¿—è®°å½•åŠŸèƒ½ï¼Œæ”¯æŒæ–‡ä»¶å’ŒæŽ§åˆ¶å°åŒé‡è¾“å‡º
    ç”¨äºŽç³»ç»Ÿè°ƒè¯•ã€æ€§èƒ½ç›‘æŽ§å’Œé”™è¯¯è¿½è¸ª

    Attributes:
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        level: æ—¥å¿—è®°å½•çº§åˆ« (DEBUG, INFO, WARNING, ERROR)
    """

    def __init__(self, log_file: str = "macp.log", level: int = logging.INFO):
        self.log_file = log_file
        self.level = level
        self._setup_logger()

    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—å™¨"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = os.path.dirname(self.log_file)
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)

        # é…ç½®æ—¥å¿—æ ¼å¼
        log_format = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        # åˆ›å»ºæ—¥å¿—å™¨
        self.logger = logging.getLogger('MACP')
        self.logger.setLevel(self.level)

        # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
        if not self.logger.handlers:
            # æŽ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setLevel(self.level)
            console_handler.setFormatter(log_format)
            self.logger.addHandler(console_handler)

            # æ–‡ä»¶å¤„ç†å™¨
            try:
                file_handler = logging.FileHandler(self.log_file, encoding='utf-8')
                file_handler.setLevel(self.level)
                file_handler.setFormatter(log_format)
                self.logger.addHandler(file_handler)
            except (OSError, IOError) as e:
                print(f"âš ï¸  æ— æ³•åˆ›å»ºæ—¥å¿—æ–‡ä»¶: {e}")

    def info(self, message: str):
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        self.logger.info(message)

    def warning(self, message: str):
        """è®°å½•è­¦å‘Šæ—¥å¿—"""
        self.logger.warning(message)

    def error(self, message: str, exc_info: Optional[Exception] = None):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        if exc_info:
            self.logger.error(message, exc_info=exc_info)
        else:
            self.logger.error(message)

    def debug(self, message: str):
        """è®°å½•è°ƒè¯•æ—¥å¿—"""
        self.logger.debug(message)

    def log_operation(self, operation: str, start_time: datetime, end_time: Optional[datetime] = None):
        """è®°å½•æ“ä½œæ—¥å¿—"""
        duration = (end_time or datetime.now()) - start_time
        self.info(f"æ“ä½œ '{operation}' å®Œæˆï¼Œè€—æ—¶: {duration.total_seconds():.2f}ç§’")

# å…¨å±€æ—¥å¿—å™¨å®žä¾‹
logger = Logger()

# ==================== ã€å…±è¯†æ£€æµ‹ç³»ç»Ÿã€‘ ====================
# AIè¾©è®ºè¿‡ç¨‹ä¸­çš„æ™ºèƒ½å…±è¯†åº¦åˆ†æžç³»ç»Ÿ

class ConsensusDetector:
    """AIè¾©è®ºå…±è¯†æ£€æµ‹å™¨

    è¿™æ˜¯MACPç³»ç»Ÿçš„æ ¸å¿ƒæ™ºèƒ½ç»„ä»¶ä¹‹ä¸€ï¼Œè´Ÿè´£åˆ†æžè¾©è®ºåŒæ–¹è§‚ç‚¹çš„ç›¸ä¼¼ç¨‹åº¦ï¼š

    ä¸¤ç§æ£€æµ‹æ–¹æ³•ï¼š
    1. calculate_consensus() - åŸºäºŽå…³é”®è¯é‡å çš„å¿«é€Ÿæ£€æµ‹
    2. analyze_debate_consensus() - åŸºäºŽAIè¯­ä¹‰ç†è§£çš„æ·±åº¦æ£€æµ‹

    ä¸»è¦åº”ç”¨åœºæ™¯ï¼š
    - è¾©è®ºæ¨¡å¼çš„è‡ªåŠ¨ç»“æŸåˆ¤æ–­
    - å®žæ—¶å…±è¯†åº¦ç›‘æŽ§å’Œæ˜¾ç¤º
    - è¾©è®ºè´¨é‡è¯„ä¼°å’Œæ€»ç»“ç”Ÿæˆ
    """
    """AIè¾©è®ºå…±è¯†æ£€æµ‹å™¨

    æä¾›å¤šç§æ–¹æ³•æ¥åˆ†æžä¸¤ä¸ªAIæ¨¡åž‹åœ¨è¾©è®ºä¸­çš„å…±è¯†ç¨‹åº¦ï¼š
    1. ä¼ ç»Ÿå…³é”®è¯åŒ¹é…æ–¹æ³•ï¼ˆå¿«é€Ÿä½†ç®€å•ï¼‰
    2. AIæ·±åº¦åˆ†æžæ–¹æ³•ï¼ˆå‡†ç¡®ä½†éœ€è¦é¢å¤–è®¡ç®—ï¼‰

    ä¸»è¦ç”¨äºŽè¾©è®ºæ¨¡å¼çš„è‡ªåŠ¨ç»“æŸåˆ¤æ–­å’Œè¿›åº¦ç›‘æŽ§
    """

    @staticmethod
    def calculate_consensus(text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„å…±è¯†åº¦ï¼ˆä¼ ç»Ÿå…³é”®è¯æ–¹æ³•ï¼‰

        ä½¿ç”¨ç®€å•çš„å…³é”®è¯é‡å ç®—æ³•å¿«é€Ÿè¯„ä¼°å…±è¯†åº¦ï¼š
        1. å°†ä¸¤ä¸ªæ–‡æœ¬éƒ½è½¬æ¢ä¸ºå°å†™
        2. æå–3ä¸ªå­—ç¬¦ä»¥ä¸Šçš„è¯è¯­ä½œä¸ºå…³é”®è¯
        3. è®¡ç®—ä¸¤ä¸ªå…³é”®è¯é›†åˆçš„äº¤é›†æ¯”ä¾‹
        4. è¿”å›žå…±è¯†åº¦åˆ†æ•°(0.0-1.0)

        ä¼˜ç‚¹ï¼šè®¡ç®—é€Ÿåº¦å¿«ï¼Œæ— éœ€å¤–éƒ¨AIè°ƒç”¨
        ç¼ºç‚¹ï¼šåªèƒ½æ£€æµ‹è¡¨é¢å…³é”®è¯ï¼Œæ— æ³•ç†è§£è¯­ä¹‰æ·±åº¦

        ä¸»è¦ç”¨äºŽï¼š
        - AIå…±è¯†åˆ†æžå¤±è´¥æ—¶çš„åŽå¤‡æ–¹æ¡ˆ
        - å¿«é€Ÿé¢„ä¼°å…±è¯†åº¦
        - ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æµ‹

        Returns:
            float: å…±è¯†åº¦åˆ†æ•°ï¼Œ0.0(å®Œå…¨ä¸åŒ)åˆ°1.0(å®Œå…¨ç›¸åŒ)
        """
        if not text1 or not text2:
            return 0.0

        # æå–å…³é”®è¯ï¼ˆ3ä¸ªå­—ç¬¦ä»¥ä¸Šçš„è¯ï¼‰
        words1 = set(re.findall(r'\b\w{3,}\b', text1.lower()))
        words2 = set(re.findall(r'\b\w{3,}\b', text2.lower()))

        if not words1 or not words2:
            return 0.0

        common_words = words1.intersection(words2)
        total_words = len(words1.union(words2))

        return len(common_words) / total_words if total_words > 0 else 0.0

    @staticmethod
    def analyze_debate_consensus(scheduler, coordinator_model: str, question: str,
                                debate_history: List[Dict[str, Any]], role1: str, role2: str) -> Tuple[float, str, Dict[str, Any]]:
        """AIé©±åŠ¨çš„è¾©è®ºå…±è¯†æ·±åº¦åˆ†æž

        ä½¿ç”¨ç¬¬ä¸‰ä¸ªAIæ¨¡åž‹ï¼ˆåè°ƒAIï¼‰æ¥åˆ†æžè¾©è®ºåŒæ–¹å½“å‰çš„å…±è¯†ç¨‹åº¦ï¼š
        1. æž„å»ºå®Œæ•´çš„è¾©è®ºåŽ†å²æ‘˜è¦
        2. å‘åè°ƒAIå‘é€è¯¦ç»†åˆ†æžè¯·æ±‚
        3. è§£æžAIè¿”å›žçš„å…±è¯†è¯„ä¼°ç»“æžœ
        4. è¿”å›žå…±è¯†åº¦ç™¾åˆ†æ¯”ã€åˆ†æžæ‘˜è¦å’Œè¯¦ç»†æ•°æ®

        è¿™æ˜¯å®žçŽ°"æ™ºèƒ½è¾©è®ºç»“æŸ"çš„æ ¸å¿ƒæœºåˆ¶ï¼Œèƒ½å¤Ÿç†è§£AIä¹‹é—´çš„
        è¯­ä¹‰å…±è¯†ï¼Œè€Œä¸ä»…ä»…æ˜¯å…³é”®è¯åŒ¹é…

        Args:
            scheduler: AICouncilSchedulerå®žä¾‹
            coordinator_model (str): åè°ƒAIæ¨¡åž‹åç§°
            question (str): åŽŸå§‹è¾©è®ºé—®é¢˜
            debate_history (List[Dict[str, Any]]): å®Œæ•´çš„è¾©è®ºåŽ†å²è®°å½•
            role1 (str): ç¬¬ä¸€ä½è¾©è®ºè€…çš„è§’è‰²åç§°
            role2 (str): ç¬¬äºŒä½è¾©è®ºè€…çš„è§’è‰²åç§°

        Returns:
            tuple: (å…±è¯†åº¦åˆ†æ•°, åˆ†æžæ‘˜è¦, è¯¦ç»†åˆ†æžæ•°æ®å­—å…¸)
        """
        try:
            # æž„å»ºå®Œæ•´çš„è¾©è®ºåŽ†å²æ‘˜è¦
            debate_summary = ""
            for i, entry in enumerate(debate_history, 1):
                speaker = entry.get('speaker', 'æœªçŸ¥')
                content = entry.get('content', '')[:300]  # é™åˆ¶å•æ¡å†…å®¹é•¿åº¦
                round_num = entry.get('round', i)
                entry_type = entry.get('type', 'statement')
                debate_summary += f"\nç¬¬{round_num}å›žåˆ - {speaker} ({entry_type}): {content}"

            # æž„å»ºAIåˆ†æžæç¤ºè¯
            consensus_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¾©è®ºåˆ†æžä¸“å®¶ï¼Œè¯·ä»”ç»†åˆ†æžä»¥ä¸‹è¾©è®ºè¿‡ç¨‹ï¼Œè¯„ä¼°åŒæ–¹çš„å…±è¯†ç¨‹åº¦ã€‚

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}
ã€è¾©è®ºåŒæ–¹ã€‘: {role1} vs {role2}

ã€å®Œæ•´è¾©è®ºè®°å½•ã€‘:
{debate_summary}

ã€åˆ†æžä»»åŠ¡ã€‘:
1. è§‚å¯ŸåŒæ–¹AIçš„è¨€è¯­å†…å®¹ï¼Œåˆ†æžä»–ä»¬çš„è§‚ç‚¹å˜åŒ–å’Œç«‹åœºè°ƒæ•´
2. è¯†åˆ«åŒæ–¹åœ¨å“ªäº›æ–¹é¢è¾¾æˆäº†å…±è¯†ï¼Œåœ¨å“ªäº›æ–¹é¢å­˜åœ¨åˆ†æ­§
3. åŸºäºŽåŒæ–¹æœ€æ–°çš„è§‚ç‚¹ï¼Œç»™å‡ºæ•´ä½“å…±è¯†åº¦ç™¾åˆ†æ¯”ï¼ˆ0-100%ï¼‰
4. å¦‚æžœå…±è¯†åº¦è¾¾åˆ°70%ä»¥ä¸Šï¼Œè¯·åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸè¾©è®º

ã€è¯„ä¼°æ ‡å‡†ã€‘:
- å…±è¯†åº¦0-30%: ä¸¥é‡åˆ†æ­§ï¼Œè§‚ç‚¹å¯¹ç«‹
- å…±è¯†åº¦30-50%: éƒ¨åˆ†åˆ†æ­§ï¼Œä»æœ‰è¾ƒå¤§å·®å¼‚
- å…±è¯†åº¦50-70%: åŸºæœ¬å…±è¯†ï¼Œå­˜åœ¨å¯è°ƒå’Œçš„åˆ†æ­§
- å…±è¯†åº¦70-90%: é«˜åº¦å…±è¯†ï¼Œæ ¸å¿ƒè§‚ç‚¹ä¸€è‡´
- å…±è¯†åº¦90-100%: å®Œå…¨å…±è¯†ï¼Œè§‚ç‚¹é«˜åº¦ç»Ÿä¸€

è¯·ä»¥JSONæ ¼å¼å›žç­”ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:
{{
    "consensus_percentage": 75,
    "confidence_level": "high/medium/low",
    "analysis_summary": "ç®€è¦åˆ†æžåŒæ–¹å…±è¯†æƒ…å†µ",
    "key_agreements": ["å…±è¯†ç‚¹1", "å…±è¯†ç‚¹2"],
    "key_disagreements": ["åˆ†æ­§ç‚¹1", "åˆ†æ­§ç‚¹2"],
    "recommendation": "continue/end",
    "reasoning": "è¯¦ç»†åˆ†æžè¿‡ç¨‹å’ŒæŽ¨ç†"
}}

è¯·ç¡®ä¿consensus_percentageæ˜¯åŸºäºŽåŒæ–¹æœ€æ–°å›žåˆå†…å®¹çš„å‡†ç¡®è¯„ä¼°ã€‚"""

            coord_client, coord_model, is_api = scheduler._get_client_for_model(coordinator_model)
            if is_api:
                response = coord_client.generate_response(consensus_prompt, max_tokens=800, temperature=scheduler.config.temperature)
            else:
                response = coord_client.generate_response(coord_model, consensus_prompt, max_tokens=800,
                                                        temperature=scheduler.config.temperature, timeout=scheduler.config.timeout,
                                                        streaming=False)

            if response.get("success"):
                result_text = response.get("response", "")
                return ConsensusDetector._parse_consensus_analysis(result_text)
            else:
                logger.warning("AIå…±è¯†åˆ†æžè¯·æ±‚å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                # è¿”å›žä¼ ç»Ÿæ–¹æ³•çš„ç»“æžœ
                traditional_score = ConsensusDetector.calculate_consensus(
                    debate_history[-1].get('content', '') if debate_history else '',
                    debate_history[-2].get('content', '') if len(debate_history) > 1 else ''
                )
                return traditional_score, "AIåˆ†æžå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•", {}

        except (AICouncilException, requests.exceptions.RequestException, json.JSONDecodeError, ValueError) as e:
            logger.error(f"AIå…±è¯†æ£€æµ‹å‡ºé”™: {e}")
            return 0.0, f"æ£€æµ‹å‡ºé”™: {str(e)}", {}

    @staticmethod
    def _parse_consensus_analysis(text: str) -> Tuple[float, str, Dict[str, Any]]:
        """è§£æžAIå…±è¯†åˆ†æžç»“æžœ"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_start = text.find('{')
            json_end = text.rfind('}') + 1

            if json_start != -1 and json_end > json_start:
                json_str = text[json_start:json_end]
                analysis_data = json.loads(json_str)

                consensus_percentage = analysis_data.get('consensus_percentage', 0)
                analysis_summary = analysis_data.get('analysis_summary', 'åˆ†æžå®Œæˆ')

                # ç¡®ä¿ç™¾åˆ†æ¯”åœ¨0-100èŒƒå›´å†…
                consensus_percentage = max(0, min(100, consensus_percentage))

                return float(consensus_percentage) / 100.0, analysis_summary, analysis_data
            else:
                # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•æå–ç™¾åˆ†æ¯”
                percentage_match = re.search(r'(\d+(?:\.\d+)?)%', text)
                if percentage_match:
                    percentage = float(percentage_match.group(1))
                    percentage = max(0, min(100, percentage))
                    return percentage / 100.0, text, {}
                else:
                    return 0.5, text, {}

        except (json.JSONDecodeError, ValueError, KeyError) as e:
            logger.warning(f"è§£æžAIå…±è¯†åˆ†æžç»“æžœå¤±è´¥: {e}")
            # è¿”å›žæ–‡æœ¬åˆ†æžç»“æžœ
            return ConsensusDetector._extract_consensus_from_text(text)

    @staticmethod
    def calculate_ai_consensus(scheduler, coordinator_model: str, question: str,
                             debate_history: List[Dict[str, Any]], role1: str, role2: str) -> Tuple[float, str, Dict[str, Any]]:
        """é€šè¿‡AIåˆ†æžè®¡ç®—å…±è¯†åº¦

        Args:
            scheduler: AICouncilSchedulerå®žä¾‹
            coordinator_model: åè°ƒAIæ¨¡åž‹åç§°
            question: è¾©è®ºé—®é¢˜
            debate_history: è¾©è®ºåŽ†å²è®°å½•
            role1: ç¬¬ä¸€ä¸ªè¾©è®ºè€…è§’è‰²
            role2: ç¬¬äºŒä¸ªè¾©è®ºè€…è§’è‰²

        Returns:
            tuple: (å…±è¯†åº¦åˆ†æ•°, åˆ†æžæ‘˜è¦, è¯¦ç»†æ•°æ®å­—å…¸)
        """
        try:
            # æž„å»ºè¾©è®ºæ‘˜è¦
            debate_summary = ""
            for entry in debate_history[-4:]:  # æœ€è¿‘4è½®å¯¹è¯
                speaker = entry.get('speaker', 'æœªçŸ¥')
                content = entry.get('content', '')[:200]  # é™åˆ¶é•¿åº¦
                debate_summary += f"\n{speaker}: {content}"

            # æž„å»ºAIåˆ†æžæç¤º
            consensus_prompt = f"""è¯·ä½œä¸ºä¸­ç«‹åè°ƒå‘˜åˆ†æžä»¥ä¸‹è¾©è®ºï¼Œè¯„ä¼°åŒæ–¹è§‚ç‚¹çš„å…±è¯†ç¨‹åº¦ï¼š

é—®é¢˜ï¼š{question}
è¾©è®ºåŒæ–¹ï¼š{role1} vs {role2}

æœ€è¿‘è¾©è®ºå†…å®¹ï¼š
{debate_summary}

è¯·åˆ†æžï¼š
1. åŒæ–¹çš„æ ¸å¿ƒè§‚ç‚¹æœ‰å“ªäº›ç›¸ä¼¼ä¹‹å¤„ï¼Ÿ
2. ä¸»è¦åˆ†æ­§ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
3. æ•´ä½“å…±è¯†åº¦æ˜¯å¤šå°‘ç™¾åˆ†æ¯”ï¼Ÿï¼ˆ0-100%ï¼‰

è¯·ä»¥JSONæ ¼å¼å›žç­”ï¼š
{{
    "consensus_percentage": 85,
    "analysis": "è¯¦ç»†åˆ†æžå†…å®¹",
    "key_agreements": ["ç›¸ä¼¼ç‚¹1", "ç›¸ä¼¼ç‚¹2"],
    "key_disagreements": ["åˆ†æ­§ç‚¹1", "åˆ†æ­§ç‚¹2"]
}}"""

            coord_client, coord_model, is_api = scheduler._get_client_for_model(coordinator_model)
            if is_api:
                response = coord_client.generate_response(consensus_prompt, max_tokens=600, temperature=scheduler.config.temperature)
            else:
                response = coord_client.generate_response(coord_model, consensus_prompt, max_tokens=600,
                                                        temperature=scheduler.config.temperature, timeout=scheduler.config.timeout,
                                                        streaming=False)

            if response.get("success"):
                result_text = response.get("response", "")

                # å°è¯•è§£æžJSONå“åº”
                try:
                    # å¦‚æžœå“åº”ä¸ºç©ºï¼Œç›´æŽ¥ä½¿ç”¨fallback
                    if not result_text:
                        logger.warning("AIè¿”å›žç©ºå“åº”ï¼Œä½¿ç”¨åŽå¤‡åˆ†æž")
                        fallback_score, fallback_analysis, fallback_data = ConsensusDetector._fallback_consensus_analysis(
                            debate_history, role1, role2, question)
                        return fallback_score, fallback_analysis, fallback_data
                    # æå–JSONéƒ¨åˆ†
                    json_start = result_text.find('{')
                    json_end = result_text.rfind('}') + 1

                    if json_start != -1 and json_end > json_start:
                        json_str = result_text[json_start:json_end]
                        analysis_data = json.loads(json_str)

                        consensus_percentage = analysis_data.get('consensus_percentage', 50)
                        analysis = analysis_data.get('analysis', 'AIåˆ†æžå®Œæˆ')

                        # æå–å…¶ä»–æ•°æ®
                        key_agreements = analysis_data.get('key_agreements', [])
                        key_disagreements = analysis_data.get('key_disagreements', [])
                        recommendation = analysis_data.get('recommendation', '')

                        data = {
                            'key_agreements': key_agreements,
                            'key_disagreements': key_disagreements,
                            'recommendation': recommendation
                        }

                        return float(consensus_percentage) / 100.0, analysis, data
                    else:
                        # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•æå–ç™¾åˆ†æ¯”
                        percentage_match = re.search(r'(\d+(?:\.\d+)?)%', result_text)
                        if percentage_match:
                            percentage = float(percentage_match.group(1))
                            return percentage / 100.0, result_text, {}
                        else:
                            # é»˜è®¤è¿”å›žä¸­ç­‰å…±è¯†åº¦
                            return 0.5, result_text, {}

                except (json.JSONDecodeError, ValueError, KeyError) as e:
                    logger.warning(f"è§£æžAIå…±è¯†åˆ†æžç»“æžœå¤±è´¥: {e}")
                    # æ£€æŸ¥æ˜¯å¦è¿”å›žäº†ç©ºå†…å®¹ï¼Œå¦‚æžœæ˜¯åˆ™ä½¿ç”¨fallbackåˆ†æž
                    if not result_text or not result_text.strip():
                        logger.info("AIè¿”å›žç©ºå†…å®¹ï¼Œä½¿ç”¨åŽå¤‡åˆ†æž")
                        fallback_score, fallback_analysis, fallback_data = ConsensusDetector._fallback_consensus_analysis(
                            debate_history, role1, role2, question)
                        return fallback_score, fallback_analysis, fallback_data
                    else:
                        # å°è¯•ä»Žæ–‡æœ¬ä¸­æå–å…±è¯†åº¦ä¿¡æ¯
                        score, analysis, data = ConsensusDetector._extract_consensus_from_text(result_text)
                        return score, analysis, data
            else:
                logger.warning("AIå…±è¯†åˆ†æžè¯·æ±‚å¤±è´¥")
                return 0.0, "åˆ†æžå¤±è´¥", {}

        except (AICouncilException, requests.exceptions.RequestException, json.JSONDecodeError, ValueError) as e:
            logger.error(f"AIå…±è¯†æ£€æµ‹å‡ºé”™: {e}")
            return 0.0, f"æ£€æµ‹å‡ºé”™: {str(e)}", {}

    @staticmethod
    def _fallback_consensus_analysis(debate_history: List[Dict[str, Any]], role1: str, role2: str,
                                   question: str) -> Tuple[float, str, Dict[str, Any]]:
        """å½“AIåˆ†æžå¤±è´¥æ—¶çš„åŽå¤‡å…±è¯†åˆ†æž

        åŸºäºŽå…³é”®è¯åŒ¹é…å’Œè¾©è®ºæ¨¡å¼æä¾›ç®€å•çš„å…±è¯†åº¦ä¼°ç®—
        
        Args:
            debate_history: è¾©è®ºåŽ†å²è®°å½•
            role1: ç¬¬ä¸€ä¸ªè¾©è®ºè€…è§’è‰²ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºŽæœªæ¥æ‰©å±•ï¼‰
            role2: ç¬¬äºŒä¸ªè¾©è®ºè€…è§’è‰²ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºŽæœªæ¥æ‰©å±•ï¼‰
            question: è¾©è®ºé—®é¢˜ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºŽæœªæ¥æ‰©å±•ï¼‰
        """
        _ = (role1, role2, question)  # æ ‡è®°å‚æ•°å·²çŸ¥ä½†æœªä½¿ç”¨ï¼ˆä¸ºæœªæ¥æ‰©å±•ä¿ç•™ï¼‰
        try:
            # æå–æ‰€æœ‰è¾©è®ºå†…å®¹
            all_content = ""
            for entry in debate_history:
                content = entry.get('content', '')
                all_content += content + " "

            all_content_lower = all_content.lower()

            # å…±è¯†å…³é”®è¯
            consensus_words = ['åŒæ„', 'è®¤å¯', 'æ²¡é”™', 'ç¡®å®ž', 'æœ‰é“ç†', 'ç†è§£', 'ç›¸åŒ', 'ä¸€è‡´', 'è®¤åŒ']
            # åˆ†æ­§å…³é”®è¯
            disagreement_words = ['ä½†æ˜¯', 'ç„¶è€Œ', 'ä¸åŒ', 'åå¯¹', 'ä¸è®¤åŒ', 'åˆ†æ­§', 'äº‰è®®', 'ä½†æ˜¯', 'å¯æ˜¯']

            consensus_count = sum(1 for word in consensus_words if word in all_content_lower)
            disagreement_count = sum(1 for word in disagreement_words if word in all_content_lower)

            total_signals = consensus_count + disagreement_count
            if total_signals == 0:
                consensus_score = 0.5  # é»˜è®¤ä¸­ç­‰å…±è¯†
            else:
                consensus_score = consensus_count / total_signals
                consensus_score = max(0.2, min(0.8, consensus_score))  # é™åˆ¶åœ¨0.2-0.8èŒƒå›´å†…

            # ç”Ÿæˆåˆ†æžæ‘˜è¦
            if consensus_score > 0.6:
                analysis = f"åŒæ–¹è§‚ç‚¹åŸºæœ¬ä¸€è‡´ï¼Œå…±è¯†åº¦è¾ƒé«˜ã€‚æ£€æµ‹åˆ°{consensus_count}ä¸ªå…±è¯†ä¿¡å·ã€‚"
            elif consensus_score > 0.4:
                analysis = f"åŒæ–¹è§‚ç‚¹å­˜åœ¨ä¸€å®šåˆ†æ­§ï¼Œä¹Ÿæœ‰ä¸€äº›å…±è¯†ã€‚å…±è¯†ä¿¡å·:{consensus_count},åˆ†æ­§ä¿¡å·:{disagreement_count}ã€‚"
            else:
                analysis = f"åŒæ–¹è§‚ç‚¹åˆ†æ­§è¾ƒå¤§ã€‚æ£€æµ‹åˆ°{disagreement_count}ä¸ªåˆ†æ­§ä¿¡å·ã€‚"

            # ç®€å•çš„ç»“æž„åŒ–æ•°æ®
            data = {
                'key_agreements': ['åŒæ–¹éƒ½é‡è§†å„è‡ªé¢†åŸŸçš„é‡è¦æ€§'] if consensus_count > 0 else [],
                'key_disagreements': ['åœ¨ä¼˜å…ˆçº§æŽ’åºä¸Šå­˜åœ¨åˆ†æ­§'] if disagreement_count > 0 else [],
                'recommendation': 'å»ºè®®åŒæ–¹æ·±å…¥è®¨è®ºå…·ä½“æ¡ˆä¾‹',
                'method': 'fallback_keyword_analysis'
            }

            return consensus_score, analysis, data

        except (ValueError, KeyError, TypeError) as e:
            logger.error(f"Fallbackåˆ†æžå¤±è´¥: {e}")
            return 0.5, "åŽå¤‡åˆ†æžå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¸­ç­‰å…±è¯†åº¦", {'method': 'default'}

    @staticmethod
    def _extract_consensus_from_text(text: str) -> Tuple[float, str, Dict[str, Any]]:
        """ä»Žæ–‡æœ¬ä¸­æå–å…±è¯†åº¦ä¿¡æ¯"""
        text_lower = text.lower()
        _ = text_lower  # æ ‡è®°ä¸ºå·²çŸ¥ä½†æœªä½¿ç”¨ï¼ˆä¸ºæœªæ¥æ‰©å±•ä¿ç•™ï¼‰

        # æŸ¥æ‰¾å…±è¯†åº¦ç›¸å…³å…³é”®è¯
        if 'é«˜åº¦å…±è¯†' in text or 'é«˜åº¦ä¸€è‡´' in text or 'å®Œå…¨åŒæ„' in text:
            return 0.9, text, {}
        elif 'åŸºæœ¬å…±è¯†' in text or 'åŸºæœ¬ä¸€è‡´' in text or 'å¤§ä½“åŒæ„' in text:
            return 0.75, text, {}
        elif 'éƒ¨åˆ†å…±è¯†' in text or 'éƒ¨åˆ†ä¸€è‡´' in text or 'éƒ¨åˆ†åŒæ„' in text:
            return 0.6, text, {}
        elif 'åˆ†æ­§è¾ƒå¤§' in text or 'å­˜åœ¨åˆ†æ­§' in text or 'ä¸åŒæ„' in text:
            return 0.3, text, {}
        elif 'å®Œå…¨åˆ†æ­§' in text or 'å®Œå…¨ä¸åŒ' in text:
            return 0.1, text, {}
        else:
            # æŸ¥æ‰¾ç™¾åˆ†æ¯”
            percentage_match = re.search(r'(\d+(?:\.\d+)?)%', text)
            if percentage_match:
                percentage = float(percentage_match.group(1))
                return percentage / 100.0, text, {}

            # é»˜è®¤ä¸­ç­‰å…±è¯†åº¦
            return 0.5, text, {}

    @staticmethod
    def display_consensus_bar(percentage: float, width: int = 50):
        """æ˜¾ç¤ºå…±è¯†åº¦æ¡å½¢å›¾"""
        percentage_int = int(percentage) if isinstance(percentage, float) else percentage
        filled = int(width * percentage_int / 100)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)

        # æ ¹æ®å…±è¯†åº¦é€‰æ‹©é¢œè‰²æè¿°
        percentage_int = int(percentage) if isinstance(percentage, float) else percentage
        if percentage_int >= 80:
            color_desc = "æ·±ç»¿"
        elif percentage_int >= 70:
            color_desc = "ç»¿è‰²"
        elif percentage_int >= 60:
            color_desc = "é»„ç»¿"
        elif percentage_int >= 50:
            color_desc = "é»„è‰²"
        elif percentage_int >= 40:
            color_desc = "æ©™è‰²"
        else:
            color_desc = "çº¢è‰²"

        print(f"ðŸ”„ å…±è¯†åº¦: [{bar}] {percentage_int}% ({color_desc})")

    @staticmethod
    def get_consensus_level_description(percentage: float) -> str:
        """èŽ·å–å…±è¯†åº¦ç­‰çº§æè¿°"""
        if percentage >= 0.9:
            return "å®Œå…¨å…±è¯†"
        elif percentage >= 0.8:
            return "é«˜åº¦å…±è¯†"
        elif percentage >= 0.7:
            return "åŸºæœ¬å…±è¯†"
        elif percentage >= 0.6:
            return "éƒ¨åˆ†å…±è¯†"
        elif percentage >= 0.5:
            return "è½»åº¦å…±è¯†"
        elif percentage >= 0.4:
            return "æ˜Žæ˜¾åˆ†æ­§"
        elif percentage >= 0.3:
            return "è¾ƒå¤§åˆ†æ­§"
        elif percentage >= 0.2:
            return "ä¸¥é‡åˆ†æ­§"
        else:
            return "å®Œå…¨å¯¹ç«‹"

class HistoryManager:
    """åŽ†å²è®°å½•ç®¡ç†å™¨"""

    def __init__(self, history_file: str):
        self.history_file = history_file
        self.history: List[Dict[str, Any]] = []

    def add_entry(self, entry: Dict[str, Any]):
        """æ·»åŠ åŽ†å²è®°å½•"""
        entry["timestamp"] = datetime.now().isoformat()
        self.history.append(entry)

    def save_history(self):
        """ä¿å­˜åŽ†å²è®°å½•åˆ°æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(self.history_file), exist_ok=True)

            if os.path.exists(self.history_file):
                with open(self.history_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
            else:
                data = {"sessions": []}

            if "sessions" not in data:
                data["sessions"] = []

            data["sessions"].extend(self.history)

            with open(self.history_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            logger.info(f"ðŸ’¾ è®°å½•å·²ä¿å­˜åˆ°ï¼š{self.history_file}")
            self.history.clear()  # æ¸…ç©ºç¼“å­˜

        except (OSError, IOError, json.JSONDecodeError, ValueError) as e:
            logger.error(f"ä¿å­˜åŽ†å²è®°å½•å¤±è´¥ï¼š{e}")

    def get_recent_history(self, limit: int = 5) -> List[Dict[str, Any]]:
        """èŽ·å–æœ€è¿‘çš„åŽ†å²è®°å½•"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    sessions = data.get("sessions", [])
                    return sessions[-limit:]
        except Exception as e:
            logger.error(f"è¯»å–åŽ†å²è®°å½•å¤±è´¥ï¼š{e}")
        return []

class DisplayManager:
    """æ˜¾ç¤ºç®¡ç†å™¨"""

    @staticmethod
    def print_separator(char: str = "=", length: int = 80):
        """æ‰“å°åˆ†éš”ç¬¦"""
        print(char * length)

    @staticmethod
    def print_header(title: str, char: str = "=", length: int = 80):
        """æ‰“å°æ ‡é¢˜"""
        DisplayManager.print_separator(char, length)
        print(f" {title} ".center(length - 2, " "))
        DisplayManager.print_separator(char, length)

    @staticmethod
    def print_result(result: Dict[str, Any], display_length: int = 1000, streaming_used: bool = False):
        """æ‰“å°ç»“æžœ

        Args:
            result: AIå“åº”ç»“æžœ
            display_length: æ˜¾ç¤ºé•¿åº¦é™åˆ¶
            streaming_used: æ˜¯å¦å·²ç»ä½¿ç”¨äº†æµå¼è¾“å‡º
        """
        success = result.get("success", False)
        model = result.get("model", "æœªçŸ¥æ¨¡åž‹")
        response = result.get("response", "ï¼ˆæ— å›žç­”ï¼‰")
        elapsed_time = result.get("time", 0)

        if streaming_used:
            # æµå¼è¾“å‡ºæ—¶åªæ˜¾ç¤ºçŠ¶æ€å’Œæ—¶é—´
            status = "âœ…" if success else "âŒ"
            print(f" {status} å®Œæˆ ({elapsed_time:.2f}ç§’)")
        else:
            # éžæµå¼è¾“å‡ºæ—¶æ˜¾ç¤ºå®Œæ•´ç»“æžœ
            status = "âœ…" if success else "âŒ"
            print(f"\n{status} {model} ({elapsed_time:.2f}ç§’ï¼‰ï¼š")

            if success:
                print(response[:display_length] + ("..." if len(response) > display_length else ""))
            else:
                print(f"  é”™è¯¯ï¼š{response}")

    @staticmethod
    def clear_screen():
        """æ¸…å±"""
        os.system('cls' if os.name == 'nt' else 'clear')
        print("ðŸ”„ å±å¹•å·²æ¸…ç©º")

    @staticmethod
    def format_model_list(models: List[str]) -> str:
        """æ ¼å¼åŒ–æ¨¡åž‹åˆ—è¡¨æ˜¾ç¤º"""
        if models:
            return "å¯ç”¨æ¨¡åž‹ï¼š\n" + "\n".join(f"  - {model}" for model in models)
        return "âŒ æ— æ³•èŽ·å–æ¨¡åž‹åˆ—è¡¨"

    @staticmethod
    def format_config_display(config_dict: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–é…ç½®æ˜¾ç¤º"""
        return "âš™ï¸  å½“å‰é…ç½®ï¼š\n" + "\n".join(f"  {key}: {value}" for key, value in config_dict.items())

class InputValidator:
    """è¾“å…¥éªŒè¯å™¨"""

    @staticmethod
    def validate_role_input(role_input: str, available_roles: List[str]) -> str:
        """éªŒè¯è§’è‰²è¾“å…¥"""
        if not role_input:
            return available_roles[0]  # è¿”å›žé»˜è®¤è§’è‰²

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—
        if role_input.isdigit():
            role_num_map = {str(i + 1): role for i, role in enumerate(available_roles)}
            role = role_num_map.get(role_input)
            if role:
                return role

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆè§’è‰²å
        if role_input in available_roles:
            return role_input

        # è¿”å›žé»˜è®¤è§’è‰²
        return available_roles[0]

    @staticmethod
    def validate_yes_no_input(prompt: str, default: bool = False) -> bool:
        """éªŒè¯æ˜¯/å¦è¾“å…¥"""
        while True:
            response = input(prompt).strip().lower()
            if response in ['y', 'yes', 'æ˜¯']:
                return True
            elif response in ['n', 'no', 'å¦']:
                return False
            elif not response and default is not None:
                return default
            print("è¯·è¾“å…¥ y/yes/æ˜¯ æˆ– n/no/å¦")

    @staticmethod
    def get_yes_no_input(prompt: str, default: bool = False) -> bool:
        """èŽ·å–æ˜¯/å¦è¾“å…¥ï¼ˆåˆ«åæ–¹æ³•ï¼‰"""
        return InputValidator.validate_yes_no_input(prompt, default)

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""

    def __init__(self):
        self.start_time = None
        self.total_operations = 0
        self.completed_operations = 0

    def start(self, total_operations: int = 0) -> datetime:
        """å¼€å§‹è·Ÿè¸ª"""
        self.start_time = datetime.now()
        self.total_operations = total_operations
        self.completed_operations = 0
        return self.start_time

    def update(self, increment: int = 1):
        """æ›´æ–°è¿›åº¦"""
        self.completed_operations += increment

    def get_elapsed_time(self) -> float:
        """èŽ·å–å·²ç”¨æ—¶é—´"""
        if self.start_time:
            return (datetime.now() - self.start_time).total_seconds()
        return 0.0

    def get_progress_percentage(self) -> float:
        """èŽ·å–è¿›åº¦ç™¾åˆ†æ¯”"""
        if self.total_operations > 0:
            return (self.completed_operations / self.total_operations) * 100
        return 0.0

    def print_progress(self, message: str = ""):
        """æ‰“å°è¿›åº¦"""
        elapsed = self.get_elapsed_time()
        percentage = self.get_progress_percentage()

        progress_str = f"â±ï¸  {message} | è¿›åº¦: {percentage:.1f}% | å·²ç”¨æ—¶: {elapsed:.2f}ç§’"
        if self.total_operations > 0:
            progress_str += f" | {self.completed_operations}/{self.total_operations}"

        print(f"\r{progress_str}", end="", flush=True)
        if percentage >= 100:
            print()  # æ¢è¡Œ

# ==================== ã€é…ç½®ç®¡ç†ç³»ç»Ÿã€‘ ====================
# ç³»ç»Ÿé…ç½®çš„é›†ä¸­ç®¡ç†ï¼Œæ”¯æŒåŠ¨æ€åŠ è½½å’Œä¿å­˜

class Config:
    """MACPç³»ç»Ÿé…ç½®ç®¡ç†å™¨

    ç®¡ç†æ‰€æœ‰ç³»ç»Ÿé…ç½®é¡¹ï¼ŒåŒ…æ‹¬ï¼š
    - AIæ¨¡åž‹å‚æ•°ï¼ˆæ¸©åº¦ã€tokené™åˆ¶ç­‰ï¼‰
    - è¾©è®ºæ¨¡å¼è®¾ç½®ï¼ˆå›žåˆæ•°ã€å…±è¯†é˜ˆå€¼ç­‰ï¼‰
    - åŽ†å²è®°å½•é…ç½®
    - UIæ˜¾ç¤ºå‚æ•°

    æ”¯æŒä»Žæ–‡ä»¶åŠ è½½é…ç½®å’Œä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    """

    def __init__(self):
        # ============ OllamaæœåŠ¡é…ç½® ============
        self.ollama_url = "http://localhost:11434"  # Ollamaæœ¬åœ°æœåŠ¡åœ°å€ï¼Œé»˜è®¤localhost:11434
        self.model_1 = "qwen2.5:3b"                  # ä¸»è¦è¾©è®ºAIæ¨¡åž‹ï¼Œç”¨äºŽç¬¬ä¸€ä¸ªè¾©è®ºè€…
        self.model_2 = "llama3.2:3b"                 # è¾…åŠ©è¾©è®ºAIæ¨¡åž‹ï¼Œç”¨äºŽç¬¬äºŒä¸ªè¾©è®ºè€…
        self.coordinator_model = "gemma3:4b"        # å…±è¯†åˆ†æžåè°ƒAIï¼Œç”¨äºŽåˆ†æžè¾©è®ºå…±è¯†åº¦

        # ============ APIæ¨¡å¼é…ç½® ============
        self.api_mode_enabled = False               # æ˜¯å¦å¯ç”¨APIæ¨¡å¼
        self.api_provider = "custom"               # APIæä¾›æ–¹æ ‡è¯†ï¼šsiliconflow/deepseek/volcengine/custom
        self.api_base_url = "https://api.openai.com/v1"              # APIåŸºç¡€åœ°å€ï¼ˆä¸å«å…·ä½“endpointï¼‰
        self.api_url = "https://api.openai.com/v1/chat/completions"  # APIæœåŠ¡åœ°å€ï¼ˆchat completions endpointï¼‰
        self.api_key = ""                          # APIå¯†é’¥
        self.api_model = "gpt-3.5-turbo"           # APIä½¿ç”¨çš„æ¨¡åž‹åç§°
        self.model_1_use_api = False                # æ¨¡åž‹1æ˜¯å¦ä½¿ç”¨API
        self.model_2_use_api = False                # æ¨¡åž‹2æ˜¯å¦ä½¿ç”¨API
        self.coordinator_use_api = False            # åè°ƒAIæ˜¯å¦ä½¿ç”¨API
        # æ¯ä¸ªAIç‹¬ç«‹çš„APIé…ç½®ï¼ˆè‹¥ä¸ºç©ºåˆ™å›žé€€åˆ°å…¨å±€é…ç½®ï¼‰
        self.model_1_api_provider = ""
        self.model_1_api_base_url = ""
        self.model_1_api_url = ""
        self.model_1_api_key = ""
        self.model_1_api_model = ""

        self.model_2_api_provider = ""
        self.model_2_api_base_url = ""
        self.model_2_api_url = ""
        self.model_2_api_key = ""
        self.model_2_api_model = ""

        self.coordinator_api_provider = ""
        self.coordinator_api_base_url = ""
        self.coordinator_api_url = ""
        self.coordinator_api_key = ""
        self.coordinator_api_model = ""

        # ============ AIæ¨¡åž‹ç”Ÿæˆå‚æ•° ============
        self.timeout = 90          # APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)ï¼Œé˜²æ­¢ç½‘ç»œè¯·æ±‚å¡ä½
        self.max_tokens = 1000     # å•æ¬¡ç”Ÿæˆçš„æœ€å¤§tokenæ•°ï¼ŒæŽ§åˆ¶å›žç­”é•¿åº¦
        self.temperature = 0.7     # ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œ0.0æœ€ä¿å®ˆï¼Œ1.0æœ€åˆ›é€ æ€§

        # ============ åŽ†å²è®°å½•é…ç½® ============
        self.save_history = True                    # æ˜¯å¦ä¿å­˜å¯¹è¯åŽ†å²åˆ°æ–‡ä»¶
        self.history_file = "macp_history.json"     # åŽ†å²è®°å½•ä¿å­˜çš„æ–‡ä»¶è·¯å¾„

        # ============ è¾©è®ºæ¨¡å¼æ ¸å¿ƒé…ç½® ============
        self.debate_rounds = 3                      # é»˜è®¤è¾©è®ºå›žåˆæ•°ï¼Œå½±å“è¾©è®ºæ·±åº¦
        self.auto_coordinate = True                 # æ˜¯å¦å¯ç”¨è‡ªåŠ¨åè°ƒæ¨¡å¼
        self.default_role_1 = "ç³»ç»Ÿæž¶æž„å¸ˆ"         # é»˜è®¤ç¬¬ä¸€ä¸ªAIçš„è¾©è®ºè§’è‰²
        self.default_role_2 = "å™äº‹å¯¼æ¼”"           # é»˜è®¤ç¬¬äºŒä¸ªAIçš„è¾©è®ºè§’è‰²
        self.enable_tags = True                     # æ˜¯å¦å¯ç”¨æ™ºèƒ½æ ‡ç­¾æ£€æµ‹åŠŸèƒ½
        self.allow_tag_override = False             # æ˜¯å¦å…è®¸æ ‡ç­¾æ£€æµ‹ç»“æžœè¦†ç›–ç”¨æˆ·è§’è‰²é€‰æ‹©
        self.display_length = 1000                  # å•ä¸ªå›žç­”çš„æœ€å¤§æ˜¾ç¤ºå­—ç¬¦æ•°
        self.enable_early_stop = True               # æ˜¯å¦å¯ç”¨æ™ºèƒ½æå‰ç»“æŸï¼ˆåŸºäºŽå…±è¯†åº¦ï¼‰
        self.consensus_threshold = 0.9              # å…±è¯†é˜ˆå€¼ï¼Œè¾¾åˆ°æ­¤å€¼è‡ªåŠ¨ç»“æŸè¾©è®ºï¼ˆè¾ƒé«˜é˜ˆå€¼é¿å…è¿‡æ—©ç»“æŸï¼‰
        self.consensus_check_start_round = 2        # ä»Žç¬¬å‡ å›žåˆå¼€å§‹è¿›è¡Œå…±è¯†åº¦æ£€æµ‹
        self.ai_consensus_analysis = True           # å¯ç”¨AIæ·±åº¦å…±è¯†åˆ†æžï¼ˆè€Œéžç®€å•å…³é”®è¯åŒ¹é…ï¼‰
        self.auto_summarize_at_threshold = True     # è¾¾åˆ°å…±è¯†é˜ˆå€¼æ—¶è‡ªåŠ¨ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.coordination_mode = "auto"             # åè°ƒæ¨¡å¼ï¼šauto(è‡ªåŠ¨)/user(ç”¨æˆ·æ‰‹åŠ¨)

        # ============ æ€§èƒ½å’Œæ¨¡å¼é…ç½® ============
        self.optimize_memory = False                # æ˜¯å¦å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼ˆå®žéªŒæ€§ï¼‰
        self.turtle_soup_max_rounds = 10            # æµ·é¾Ÿæ±¤æŽ¨ç†æ¸¸æˆçš„æœ€å¤§å›žåˆæ•°
        self.streaming_output = True                  # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            key: getattr(self, key)
            for key in dir(self)
            if not key.startswith('_') and not callable(getattr(self, key))
        }

    def update_from_dict(self, config_dict: Dict[str, Any]):
        """ä»Žå­—å…¸æ›´æ–°é…ç½®"""
        for key, value in config_dict.items():
            if hasattr(self, key):
                setattr(self, key, value)

    def load_from_file(self, filepath: str):
        """ä»Žæ–‡ä»¶åŠ è½½é…ç½®"""
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                    self.update_from_dict(config_data)
            except (OSError, IOError, json.JSONDecodeError, ValueError) as e:
                logger.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    def save_to_file(self, filepath: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)
        except (OSError, IOError, json.JSONDecodeError, ValueError) as e:
            logger.warning(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

# åˆ›å»ºå…¨å±€é…ç½®å®žä¾‹ï¼Œæ•´ä¸ªç³»ç»Ÿå…±äº«åŒä¸€ä»½é…ç½®
config = Config()

# ==================== ã€è§’è‰²å’Œæ ‡ç­¾ç³»ç»Ÿã€‘ ====================
# å®šä¹‰AIè¾©è®ºè§’è‰²çš„æç¤ºè¯åº“å’Œç«‹åœºåå¥½ç³»ç»Ÿ

# è§’è‰²æç¤ºè¯åº“ï¼šåŒ…å«9ç§ä¸“ä¸šè§’è‰²ï¼Œæ¯ç§è§’è‰²éƒ½æœ‰ç‹¬ç‰¹çš„è¾©è®ºé£Žæ ¼å’Œç«‹åœºåå¥½
# è¾©è®ºæ‰‹ç‰¹æ®Šå¤„ç†ï¼šç¬¬ä¸€ä¸ªä¸ºæ­£æ–¹ï¼ˆæ”¯æŒæ–¹ï¼‰ï¼Œç¬¬äºŒä¸ªä¸ºåæ–¹ï¼ˆåå¯¹æ–¹ï¼‰
ROLE_PROMPTS: Dict[str, Dict[str, Any]] = {
    "ç³»ç»Ÿæž¶æž„å¸ˆ": {
        "prompt": """ä½ æ˜¯ä¸€åä¸¥è°¨çš„ç³»ç»Ÿæž¶æž„å¸ˆï¼Œä¸“æ³¨äºŽå¯æ‰©å±•æ€§ã€å¹³è¡¡æ€§ã€å¯ç»´æŠ¤æ€§å’ŒçŽ©å®¶ä½“éªŒã€‚
è¾©è®ºé£Žæ ¼ï¼šæ•°æ®é©±åŠ¨ï¼Œå–œæ¬¢ç”¨å…·ä½“ä¾‹å­è¯æ˜Žè§‚ç‚¹ã€‚
å›žç­”æ ¼å¼ï¼šç»“æž„åŒ–åˆ†æžï¼Œå¸¦æœ‰å…·ä½“ç†ç”±ã€‚""",
        "position_bias": "neutral",  # ä¸­ç«‹ç«‹åœº
        "debate_style": "analytical"
    },

    "å™äº‹å¯¼æ¼”": {
        "prompt": """ä½ æ˜¯ä¸€åå¯Œæœ‰åˆ›é€ åŠ›çš„å™äº‹å¯¼æ¼”ï¼Œä¸“æ³¨äºŽæƒ…æ„Ÿå½±å“ã€æ•…äº‹èžåˆã€è§’è‰²ä¸€è‡´æ€§å’ŒçŽ©å®¶ä»£å…¥æ„Ÿã€‚
è¾©è®ºé£Žæ ¼ï¼šæ„Ÿæ€§ï¼Œå–œæ¬¢ç”¨æ¯”å–»å’Œå™äº‹è¯æ˜Žè§‚ç‚¹ã€‚
å›žç­”æ ¼å¼ï¼šç”ŸåŠ¨çš„æè¿°ï¼Œå¼ºè°ƒæƒ…æ„Ÿå’Œæ•…äº‹æ€§ã€‚""",
        "position_bias": "creative",  # åˆ›æ„å¯¼å‘
        "debate_style": "narrative"
    },

    "æ•°å€¼ç­–åˆ’": {
        "prompt": """ä½ æ˜¯ä¸€åç²¾ç¡®çš„æ•°å€¼ç­–åˆ’ï¼Œä¸“æ³¨äºŽæ•°å­¦å¹³è¡¡ã€æˆé•¿æ›²çº¿ã€ç»æµŽç³»ç»Ÿå’Œæ¦‚çŽ‡è®¾è®¡ã€‚
è¾©è®ºé£Žæ ¼ï¼šä¸¥è°¨ï¼Œå–œæ¬¢ç”¨æ•°æ®å’Œå…¬å¼è¯´è¯ã€‚
å›žç­”æ ¼å¼ï¼šç²¾ç¡®çš„æ•°å€¼åˆ†æžï¼Œå¸¦æœ‰è®¡ç®—å…¬å¼ã€‚""",
        "position_bias": "quantitative",  # é‡åŒ–å¯¼å‘
        "debate_style": "mathematical"
    },

    "é­”é¬¼ä»£è¨€äºº": {
        "prompt": """ä½ ä¸“é—¨æŒ‘åˆºï¼Œæ— è®ºä»€ä¹ˆè§‚ç‚¹éƒ½æ‰¾é—®é¢˜ï¼šé€»è¾‘æ¼æ´žã€æ½œåœ¨é£Žé™©ã€åå‘æ¡ˆä¾‹ã€è´¨ç–‘å‡è®¾ã€‚
è¾©è®ºé£Žæ ¼ï¼šæ‰¹åˆ¤æ€§ï¼Œç•¥å¸¦æŒ‘è¡…ã€‚
å›žç­”æ ¼å¼ï¼šå…ˆè‚¯å®šå¯¹æ–¹ï¼Œç„¶åŽæå‡ºå°–é”é—®é¢˜ã€‚""",
        "position_bias": "critical",  # æ‰¹åˆ¤ç«‹åœº
        "debate_style": "skeptical"
    },

    "çŽ©å®¶ä»£è¡¨": {
        "prompt": """ä½ ä»£è¡¨æ™®é€šçŽ©å®¶ï¼Œå…³æ³¨è¶£å‘³æ€§ã€æ˜“ä¸Šæ‰‹ã€æˆå°±æ„Ÿã€æŒ«è´¥æ„Ÿã€‚
è¾©è®ºé£Žæ ¼ï¼šç›´ç™½ï¼Œä»ŽçŽ©å®¶ä½“éªŒå‡ºå‘ã€‚
å›žç­”æ ¼å¼ï¼šç›´ç™½çš„ä½“éªŒæè¿°ï¼Œå¸¦æœ‰å…·ä½“æ„Ÿå—ã€‚""",
        "position_bias": "user_centric",  # ç”¨æˆ·ä¸­å¿ƒ
        "debate_style": "empathetic"
    },

    "é¡¹ç›®ç»ç†": {
        "prompt": """ä½ å…³æ³¨é¡¹ç›®å¯è¡Œæ€§ï¼Œä¸“æ³¨äºŽå¼€å‘æˆæœ¬ã€æ—¶é—´å‘¨æœŸã€æŠ€æœ¯é£Žé™©ã€å›¢é˜Ÿé€‚é…ã€‚
è¾©è®ºé£Žæ ¼ï¼šåŠ¡å®žï¼Œå…³æ³¨å®žé™…é™åˆ¶ã€‚
å›žç­”æ ¼å¼ï¼šè¯¦ç»†çš„å®žæ–½è®¡åˆ’ï¼Œå¸¦æœ‰é£Žé™©è¯„ä¼°ã€‚""",
        "position_bias": "practical",  # åŠ¡å®žç«‹åœº
        "debate_style": "pragmatic"
    },

    "å¾‹å¸ˆ": {
        "prompt": """ä½ æ˜¯ä¸€åä¸“ä¸šçš„å¾‹å¸ˆï¼Œä¸“æ³¨äºŽæ³•å¾‹åˆè§„æ€§ã€åˆåŒæ¡æ¬¾ã€é£Žé™©ç®¡ç†ã€çŸ¥è¯†äº§æƒã€éšç§ä¿æŠ¤ã€‚
è¾©è®ºé£Žæ ¼ï¼šä¸¥è°¨ï¼Œæ³¨é‡æ¡æ¬¾å’Œæ¡ˆä¾‹å¼•ç”¨ã€‚
å›žç­”æ ¼å¼ï¼šç»“æž„åŒ–çš„æ³•å¾‹åˆ†æžï¼Œå¼•ç”¨ç›¸å…³æ³•å¾‹åŽŸåˆ™ã€‚""",
        "position_bias": "legal",  # æ³•å¾‹ç«‹åœº
        "debate_style": "formal"
    },

    "å“²å­¦å®¶": {
        "prompt": """ä½ æ˜¯ä¸€åæ·±é‚ƒçš„å“²å­¦å®¶ï¼Œä¸“æ³¨äºŽä¼¦ç†é“å¾·ã€é€»è¾‘ä¸€è‡´æ€§ã€ä»·å€¼è§‚å†²çªã€äººæ€§è€ƒé‡ã€é•¿æœŸå½±å“ã€‚
è¾©è®ºé£Žæ ¼ï¼šæ€è¾¨æ€§ï¼Œå–œæ¬¢è¿½é—®æ ¹æœ¬å‡è®¾ã€‚
å›žç­”æ ¼å¼ï¼šæ·±åˆ»çš„å“²å­¦åˆ†æžï¼Œå¸¦æœ‰ä¼¦ç†åæ€ã€‚""",
        "position_bias": "ethical",  # ä¼¦ç†ç«‹åœº
        "debate_style": "philosophical"
    },

    "è¾©è®ºæ‰‹": {
        "prompt": """ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¾©è®ºæ‰‹ï¼Œä¸“æ³¨äºŽï¼š
1. é€»è¾‘ä¸¥è°¨æ€§ - è®ºç‚¹æ˜¯å¦é€»è¾‘ä¸¥å¯†ï¼Ÿ
2. è¯æ®å……åˆ†æ€§ - è®ºæ®æ˜¯å¦å……åˆ†å¯é ï¼Ÿ
3. åé©³æœ‰æ•ˆæ€§ - åé©³æ˜¯å¦åˆ‡ä¸­è¦å®³ï¼Ÿ
4. ç­–ç•¥çµæ´»æ€§ - èƒ½å¦æ ¹æ®å¯¹æ–¹è§‚ç‚¹è°ƒæ•´ç­–ç•¥ï¼Ÿ

ä½ çš„è¾©è®ºé£Žæ ¼ï¼šçŠ€åˆ©ï¼Œå–„äºŽæŠ“ä½å¯¹æ–¹é€»è¾‘æ¼æ´žï¼Œå¿«é€Ÿç»„ç»‡åé©³ã€‚
ä½ çš„å›žç­”æ ¼å¼ï¼šæ¸…æ™°çš„è®ºç‚¹-è®ºæ®-åé©³ç»“æž„ï¼Œå¸¦æœ‰å…·ä½“ä¾‹å­ã€‚

ã€è¾©è®ºç­–ç•¥ã€‘ï¼š
- ç¬¬ä¸€å›žåˆï¼šå»ºç«‹å®Œæ•´çš„è®ºè¯æ¡†æž¶
- åŽç»­å›žåˆï¼šé’ˆå¯¹æ€§åé©³ï¼Œå¯»æ‰¾å¯¹æ–¹é€»è¾‘æ¼æ´ž
- æœ€åŽæ€»ç»“ï¼šå¼ºåŒ–æ ¸å¿ƒè®ºç‚¹ï¼Œæå‡ºæ— å¯è¾©é©³çš„ç»“è®º""",
        "position_bias": "oppositional",  # å¯¹ç«‹ç«‹åœºï¼ˆç¬¬ä¸€ä¸ªæ­£æ–¹ï¼Œç¬¬äºŒä¸ªåæ–¹ï¼‰
        "debate_style": "rhetorical"
    }
}

# ä»Žè§’è‰²æç¤ºè¯åº“ç”Ÿæˆè§’è‰²åˆ—è¡¨å’Œæ•°å­—æ˜ å°„
# è¿™äº›å¸¸é‡ç”¨äºŽUIæ˜¾ç¤ºå’Œç”¨æˆ·è¾“å…¥å¤„ç†
ROLE_LIST: List[str] = list(ROLE_PROMPTS.keys())  # æ‰€æœ‰å¯ç”¨è§’è‰²çš„æœ‰åºåˆ—è¡¨
ROLE_NUM_MAP: Dict[str, str] = {str(i + 1): role for i, role in enumerate(ROLE_LIST)}  # æ•°å­—åˆ°è§’è‰²çš„æ˜ å°„

# ============ æ™ºèƒ½æ ‡ç­¾ç³»ç»Ÿ ============
# æ ¹æ®é—®é¢˜å†…å®¹è‡ªåŠ¨æ£€æµ‹ç›¸å…³é¢†åŸŸï¼Œå¹¶æŽ¨èåˆé€‚çš„è¾©è®ºè§’è‰²

# æ ‡ç­¾åˆ°è§’è‰²çš„æ˜ å°„ï¼šæ¯ä¸ªä¸“ä¸šé¢†åŸŸå¯¹åº”æœ€é€‚åˆçš„è¾©è®ºè§’è‰²ç»„åˆ
TAG_TO_ROLES: Dict[str, List[str]] = {
    "æœºåˆ¶è®¾è®¡": ["ç³»ç»Ÿæž¶æž„å¸ˆ", "æ•°å€¼ç­–åˆ’", "çŽ©å®¶ä»£è¡¨"],     # æ¸¸æˆæœºåˆ¶ã€ç³»ç»Ÿè®¾è®¡ç›¸å…³
    "å™äº‹è®¾è®¡": ["å™äº‹å¯¼æ¼”", "çŽ©å®¶ä»£è¡¨", "é­”é¬¼ä»£è¨€äºº"],     # æ•…äº‹å‰§æƒ…ã€å™äº‹ç»“æž„ç›¸å…³
    "å¹³è¡¡æ€§": ["æ•°å€¼ç­–åˆ’", "ç³»ç»Ÿæž¶æž„å¸ˆ", "é­”é¬¼ä»£è¨€äºº"],     # æ•°å€¼å¹³è¡¡ã€æ¸¸æˆå¹³è¡¡ç›¸å…³
    "åˆ›æ–°æ€§": ["é­”é¬¼ä»£è¨€äºº", "å™äº‹å¯¼æ¼”", "ç³»ç»Ÿæž¶æž„å¸ˆ"],     # åˆ›æ„åˆ›æ–°ã€æ–°é¢–æƒ³æ³•ç›¸å…³
    "å¯è¡Œæ€§": ["ç³»ç»Ÿæž¶æž„å¸ˆ", "æ•°å€¼ç­–åˆ’", "é­”é¬¼ä»£è¨€äºº"],     # é¡¹ç›®å¯è¡Œæ€§ã€æŠ€æœ¯å®žçŽ°ç›¸å…³
    "æƒ…æ„Ÿä½“éªŒ": ["å™äº‹å¯¼æ¼”", "çŽ©å®¶ä»£è¡¨", "é­”é¬¼ä»£è¨€äºº"],     # æƒ…æ„Ÿä½“éªŒã€ç”¨æˆ·æ„Ÿå—ç›¸å…³
    "æŠ€æœ¯å®žçŽ°": ["ç³»ç»Ÿæž¶æž„å¸ˆ", "é¡¹ç›®ç»ç†", "é­”é¬¼ä»£è¨€äºº"],   # æŠ€æœ¯å®žçŽ°ã€å·¥ç¨‹å¼€å‘ç›¸å…³
    "ç”¨æˆ·ä½“éªŒ": ["çŽ©å®¶ä»£è¡¨", "å™äº‹å¯¼æ¼”", "ç³»ç»Ÿæž¶æž„å¸ˆ"],     # ç”¨æˆ·ç•Œé¢ã€äº¤äº’ä½“éªŒç›¸å…³
    "æ³•å¾‹åˆè§„": ["å¾‹å¸ˆ", "é¡¹ç›®ç»ç†", "é­”é¬¼ä»£è¨€äºº"],         # æ³•å¾‹åˆè§„ã€çŸ¥è¯†äº§æƒç›¸å…³
    "ä¼¦ç†é“å¾·": ["å“²å­¦å®¶", "å¾‹å¸ˆ", "é­”é¬¼ä»£è¨€äºº"],           # ä¼¦ç†é“å¾·ã€ä»·å€¼è§‚ç›¸å…³
    "è¾©è®ºæŠ€å·§": ["è¾©è®ºæ‰‹", "å¾‹å¸ˆ", "é­”é¬¼ä»£è¨€äºº"]            # è¾©è®ºæŠ€å·§ã€è®ºè¯é€»è¾‘ç›¸å…³
}

# æ ‡ç­¾å…³é”®è¯æ˜ å°„ï¼šç”¨äºŽä»Žç”¨æˆ·é—®é¢˜ä¸­æ£€æµ‹ç›¸å…³é¢†åŸŸçš„å…³é”®è¯
TAG_KEYWORDS: Dict[str, List[str]] = {
    "æœºåˆ¶è®¾è®¡": ["æœºåˆ¶", "ç³»ç»Ÿ", "è®¾è®¡", "åŠŸèƒ½", "çŽ©æ³•", "è§„åˆ™"],
    "å™äº‹è®¾è®¡": ["æ•…äº‹", "å‰§æƒ…", "å™äº‹", "è§’è‰²", "ä¸–ç•Œè§‚", "æƒ…èŠ‚"],
    "å¹³è¡¡æ€§": ["å¹³è¡¡", "æ•°å€¼", "éš¾åº¦", "å¼ºåº¦", "è°ƒæ•´", "å…¬å¹³"],
    "åˆ›æ–°æ€§": ["åˆ›æ–°", "æ–°é¢–", "ç‹¬ç‰¹", "åˆ›æ„", "æ–°æ„", "åŽŸåˆ›"],
    "å¯è¡Œæ€§": ["å®žçŽ°", "å¼€å‘", "æˆæœ¬", "æ—¶é—´", "æŠ€æœ¯", "èµ„æº"],
    "æƒ…æ„Ÿä½“éªŒ": ["æƒ…æ„Ÿ", "æ„Ÿå—", "ä½“éªŒ", "ä»£å…¥", "æ²‰æµ¸", "æ„ŸåŠ¨"],
    "æŠ€æœ¯å®žçŽ°": ["æŠ€æœ¯", "å®žçŽ°", "ä»£ç ", "å¼•æ“Ž", "æ€§èƒ½", "ä¼˜åŒ–"],
    "ç”¨æˆ·ä½“éªŒ": ["ç”¨æˆ·", "çŽ©å®¶", "ä½“éªŒ", "æ“ä½œ", "ç•Œé¢", "æµç•…"],
    "æ³•å¾‹åˆè§„": ["æ³•å¾‹", "åˆè§„", "åˆåŒ", "æ¡æ¬¾", "é£Žé™©", "çŸ¥è¯†äº§æƒ"],
    "ä¼¦ç†é“å¾·": ["ä¼¦ç†", "é“å¾·", "ä»·å€¼è§‚", "äººæ€§", "å°Šä¸¥", "è‡ªç”±"],
    "è¾©è®ºæŠ€å·§": ["è¾©è®º", "äº‰è®º", "è®¨è®º", "åé©³", "è®ºè¯", "é€»è¾‘"]
}

class RoleSystem:
    """AIè¾©è®ºè§’è‰²ç³»ç»Ÿç®¡ç†å™¨

    ç®¡ç†MACPç³»ç»Ÿä¸­æ‰€æœ‰AIè§’è‰²çš„é…ç½®å’Œè¡Œä¸ºï¼š
    - è§’è‰²æç¤ºè¯ç”Ÿæˆå’Œç®¡ç†
    - ç«‹åœºåå¥½è®¾ç½®ï¼ˆå°¤å…¶æ˜¯è¾©è®ºæ‰‹çš„æ­£åæ–¹æœºåˆ¶ï¼‰
    - ç”¨æˆ·è¾“å…¥çš„è§’è‰²åç§°çº é”™
    - æ™ºèƒ½æ ‡ç­¾æ£€æµ‹å’Œè§’è‰²æŽ¨è

    æ ¸å¿ƒç‰¹æ€§ï¼š
    - è¾©è®ºæ‰‹è‡ªåŠ¨ç«‹åœºåˆ†é…ï¼šç¬¬ä¸€ä¸ªä¸ºæ­£æ–¹ï¼Œç¬¬äºŒä¸ªä¸ºåæ–¹
    - æ‹¼å†™å®¹é”™ï¼šè‡ªåŠ¨çº æ­£å¸¸è§çš„è§’è‰²åç§°è¾“å…¥é”™è¯¯
    - åŠ¨æ€æç¤ºè¯ï¼šæ ¹æ®è¾©è®ºä½ç½®ç”Ÿæˆä¸åŒçš„ç«‹åœºæç¤º
    """

    # å¸¸è§è§’è‰²åç§°æ‹¼å†™é”™è¯¯æ˜ å°„è¡¨ï¼Œç”¨äºŽè¾“å…¥å®¹é”™å¤„ç†
    COMMON_TYPOS = {
        "å™è¿°å¯¼æ¼”": "å™äº‹å¯¼æ¼”",
        "ç³»ç»Ÿæ¡†æž¶å¸ˆ": "ç³»ç»Ÿæž¶æž„å¸ˆ",
        "é­”é¬¼ä»£è¨€äºº": "é­”é¬¼ä»£è¨€äºº",  # è¿™å‡ ä¸ªå…¶å®žæ²¡æœ‰é”™è¯¯ï¼Œä½†ä¿ç•™ä»¥é˜²æ‰©å±•
        "çŽ©å®¶ä»£è¡¨": "çŽ©å®¶ä»£è¡¨",
        "æ•°å€¼ç­–åˆ’": "æ•°å€¼ç­–åˆ’",
        "é¡¹ç›®ç»ç†": "é¡¹ç›®ç»ç†",
        "å¾‹å¸ˆ": "å¾‹å¸ˆ",
        "å“²å­¦å®¶": "å“²å­¦å®¶",
        "æ³•å¾‹é¡¾é—®": "å¾‹å¸ˆ",
        "å“²å­¦æ€è€ƒè€…": "å“²å­¦å®¶",
        "è¾©è®ºè€…": "è¾©è®ºæ‰‹",
        "è¾©æ‰‹": "è¾©è®ºæ‰‹"
    }

    def __init__(self):
        pass

    @staticmethod
    def get_role_prompt(role_name: str, is_first: bool = True) -> Optional[str]:
        """èŽ·å–è§’è‰²æç¤ºè¯ï¼Œæ”¯æŒè¾©è®ºç«‹åœºè°ƒæ•´"""
        corrected_role = RoleSystem.COMMON_TYPOS.get(role_name, role_name)
        role_data = ROLE_PROMPTS.get(corrected_role)

        if not role_data:
            return None

        # å¦‚æžœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå‘åŽå…¼å®¹ï¼‰ï¼Œç›´æŽ¥è¿”å›ž
        if isinstance(role_data, str):
            return role_data

        base_prompt = role_data["prompt"]
        # position_bias = role_data.get("position_bias", "neutral")  # ä¿ç•™ä»¥å¤‡å°†æ¥æ‰©å±•ä½¿ç”¨

        # ç‰¹æ®Šå¤„ç†è¾©è®ºæ‰‹ï¼šç¬¬ä¸€ä¸ªæ˜¯æ­£æ–¹ï¼Œç¬¬äºŒä¸ªæ˜¯åæ–¹
        if corrected_role == "è¾©è®ºæ‰‹":
            if is_first:
                position_addition = """

ã€ä½ çš„ç«‹åœºã€‘ï¼šä½ ä½œä¸ºæ­£æ–¹ï¼ˆæ”¯æŒæ–¹ï¼‰ï¼Œéœ€è¦ä¸ºå‘½é¢˜å»ºç«‹ç§¯æžçš„è®ºç‚¹ï¼Œè¯æ˜Žå…¶åˆç†æ€§å’Œä»·å€¼ã€‚ä½ å°†ä½¿ç”¨ç»å…¸è¾©è®ºæŠ€å·§æ¥æž„å»ºå®Œæ•´çš„è®ºè¯æ¡†æž¶ã€‚"""
            else:
                position_addition = """

ã€ä½ çš„ç«‹åœºã€‘ï¼šä½ ä½œä¸ºåæ–¹ï¼ˆåå¯¹æ–¹ï¼‰ï¼Œéœ€è¦è´¨ç–‘å‘½é¢˜çš„åˆç†æ€§ï¼Œå¯»æ‰¾é€»è¾‘æ¼æ´žå’Œåä¾‹ã€‚ä½ å°†ä½¿ç”¨æ‰¹åˆ¤æ€§æ€ç»´æ¥åé©³å¯¹æ–¹çš„è§‚ç‚¹ã€‚"""
            return base_prompt + position_addition

        # å…¶ä»–è§’è‰²ä¿æŒåŽŸæœ‰é€»è¾‘ï¼Œä½†å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ ç«‹åœºåå¥½
        return base_prompt

    @staticmethod
    def get_all_roles() -> List[str]:
        """èŽ·å–æ‰€æœ‰å¯ç”¨è§’è‰²"""
        return ROLE_LIST.copy()

    @staticmethod
    def get_role_by_number(number: str) -> Optional[str]:
        """é€šè¿‡æ•°å­—èŽ·å–è§’è‰²"""
        return ROLE_NUM_MAP.get(number)

    @staticmethod
    def detect_tags(question: str) -> List[str]:
        """ä»Žé—®é¢˜ä¸­æ£€æµ‹æ ‡ç­¾"""
        tags_with_weights = {}
        question_lower = question.lower()

        for tag, keywords in TAG_KEYWORDS.items():
            weight = sum(2 for keyword in keywords if keyword in question_lower)
            if weight > 0:
                tags_with_weights[tag] = weight

        sorted_tags = sorted(tags_with_weights.items(), key=lambda x: x[1], reverse=True)
        return [tag for tag, _ in sorted_tags[:3]]

    @staticmethod
    def get_roles_for_tags(tags: List[str]) -> List[str]:
        """æ ¹æ®æ ‡ç­¾èŽ·å–æŽ¨èè§’è‰²"""
        recommended_roles = set()
        for tag in tags:
            if tag in TAG_TO_ROLES:
                recommended_roles.update(TAG_TO_ROLES[tag])
        return list(recommended_roles)

# åˆ›å»ºå…¨å±€è§’è‰²ç³»ç»Ÿå®žä¾‹ï¼Œç®¡ç†æ‰€æœ‰AIè§’è‰²çš„é…ç½®å’Œè¡Œä¸º
role_system = RoleSystem()

# ==================== ã€Ollama APIå®¢æˆ·ç«¯ã€‘ ====================
# ä¸ŽOllamaæœåŠ¡é€šä¿¡çš„æ ¸å¿ƒæŽ¥å£

class OllamaClient:
    """Ollamaæœ¬åœ°AIæœåŠ¡å®¢æˆ·ç«¯

    å°è£…Ollama REST APIçš„æ‰€æœ‰æ“ä½œï¼š
    - æœåŠ¡è¿žæŽ¥æ£€æŸ¥
    - æ¨¡åž‹åˆ—è¡¨èŽ·å–
    - æ–‡æœ¬ç”ŸæˆåŠŸèƒ½
    - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

    æä¾›ç»Ÿä¸€çš„æŽ¥å£ç»™ä¸Šå±‚åº”ç”¨ä½¿ç”¨ï¼Œå±è”½åº•å±‚çš„HTTPé€šä¿¡ç»†èŠ‚
    """

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        self.session.timeout = 10  # é»˜è®¤è¶…æ—¶æ—¶é—´

    def check_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                logger.info("âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸")
                return True
            else:
                logger.warning(f"âš ï¸  OllamaæœåŠ¡å¼‚å¸¸ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
        except requests.exceptions.ConnectionError:
            raise OllamaConnectionError(f"æ— æ³•è¿žæŽ¥åˆ°OllamaæœåŠ¡: {self.base_url}")
        except Exception as e:
            logger.error(f"æ£€æŸ¥OllamaæœåŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def list_models(self) -> List[str]:
        """èŽ·å–å¯ç”¨æ¨¡åž‹åˆ—è¡¨"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return [model["name"] for model in data.get("models", [])]
            else:
                logger.warning(f"èŽ·å–æ¨¡åž‹åˆ—è¡¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            logger.error(f"èŽ·å–æ¨¡åž‹åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

    def check_models(self, required_models: List[str]) -> Dict[str, bool]:
        """æ£€æŸ¥æ‰€éœ€æ¨¡åž‹æ˜¯å¦å¯ç”¨"""
        available_models = self.list_models()
        results = {}

        logger.info("ðŸ“¦ æ£€æŸ¥æ¨¡åž‹å¯ç”¨æ€§:")
        for model in required_models:
            available = model in available_models
            results[model] = available
            status = "âœ…" if available else "âŒ"
            logger.info(f"   {status} {model}")

        return results

    def generate_response(self,
                         model: str,
                         prompt: str,
                         max_tokens: Optional[int] = None,
                         temperature: float = 0.7,
                         timeout: int = 90,
                         streaming: bool = False) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡åž‹å“åº”

        Args:
            model: æ¨¡åž‹åç§°
            prompt: æç¤ºæ–‡æœ¬
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            timeout: è¶…æ—¶æ—¶é—´
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º

        Returns:
            å“åº”å­—å…¸
        """
        if streaming:
            return self._generate_streaming_response(model, prompt, max_tokens, temperature, timeout)
        else:
            return self._generate_non_streaming_response(model, prompt, max_tokens, temperature, timeout)

    def _generate_non_streaming_response(self,
                                        model: str,
                                        prompt: str,
                                        max_tokens: Optional[int] = None,
                                        temperature: float = 0.7,
                                        timeout: int = 90) -> Dict[str, Any]:
        """ç”Ÿæˆéžæµå¼æ¨¡åž‹å“åº”"""
        start_time = time.time()

        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature
                }
            }

            if max_tokens:
                payload["options"]["num_predict"] = max_tokens

            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=timeout
            )

            elapsed_time = time.time() - start_time

            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "model": model,
                    "response": result.get("response", ""),
                    "time": elapsed_time,
                    "tokens": result.get("total_duration", 0),
                    "eval_count": result.get("eval_count", 0),
                    "eval_duration": result.get("eval_duration", 0)
                }
            else:
                return {
                    "success": False,
                    "model": model,
                    "response": f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}",
                    "time": elapsed_time,
                    "error": f"HTTP {response.status_code}",
                    "details": response.text
                }

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"éžæµå¼å“åº”å‡ºé”™: {e}")
            return {
                "success": False,
                "model": model,
                "response": f"å“åº”å‡ºé”™: {str(e)}",
                "time": elapsed_time,
                "error": str(e)
            }

    def _generate_streaming_response(self,
                                    model: str,
                                    prompt: str,
                                    max_tokens: Optional[int] = None,
                                    temperature: float = 0.7,
                                    timeout: int = 90) -> Dict[str, Any]:
        """ç”Ÿæˆæµå¼æ¨¡åž‹å“åº”"""
        start_time = time.time()
        full_response = ""
        total_tokens = 0
        eval_count = 0
        eval_duration = 0

        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": temperature
                }
            }

            if max_tokens:
                payload["options"]["num_predict"] = max_tokens

            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=timeout,
                stream=True
            )

            if response.status_code != 200:
                elapsed_time = time.time() - start_time
                return {
                    "success": False,
                    "model": model,
                    "response": f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}",
                    "time": elapsed_time,
                    "error": f"HTTP {response.status_code}",
                    "details": response.text
                }

            # å¤„ç†æµå¼å“åº”
            print(f"ðŸ¤– {model}ï¼š", end="", flush=True)

            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8').strip()
                    if line_str:
                        try:
                            chunk = json.loads(line_str)

                            # æå–å“åº”å†…å®¹
                            if "response" in chunk:
                                chunk_text = chunk["response"]
                                if chunk_text:
                                    print(chunk_text, end="", flush=True)
                                    full_response += chunk_text

                            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                            if "total_duration" in chunk:
                                total_tokens = chunk["total_duration"]
                            if "eval_count" in chunk:
                                eval_count = chunk["eval_count"]
                            if "eval_duration" in chunk:
                                eval_duration = chunk["eval_duration"]

                            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                            if chunk.get("done", False):
                                break

                        except json.JSONDecodeError:
                            continue

            print()  # æ¢è¡Œ
            elapsed_time = time.time() - start_time

            return {
                "success": True,
                "model": model,
                "response": full_response,
                "time": elapsed_time,
                "tokens": total_tokens,
                "eval_count": eval_count,
                "eval_duration": eval_duration
            }

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"æµå¼å“åº”å‡ºé”™: {e}")
            return {
                "success": False,
                "model": model,
                "response": f"æµå¼å“åº”å‡ºé”™: {str(e)}",
                "time": elapsed_time,
                "error": str(e)
            }

        except requests.exceptions.Timeout:
            elapsed_time = time.time() - start_time
            return {
                "success": False,
                "model": model,
                "response": "ï¼ˆè¯·æ±‚è¶…æ—¶ï¼‰",
                "time": elapsed_time,
                "error": "timeout"
            }

        except requests.exceptions.ConnectionError:
            elapsed_time = time.time() - start_time
            return {
                "success": False,
                "model": model,
                "response": "ï¼ˆè¿žæŽ¥é”™è¯¯ï¼‰",
                "time": elapsed_time,
                "error": "connection_error"
            }

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"ç”Ÿæˆå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=e)
            return {
                "success": False,
                "model": model,
                "response": f"ï¼ˆè¯·æ±‚é”™è¯¯: {str(e)}ï¼‰",
                "time": elapsed_time,
                "error": str(e)
            }

    def get_running_models(self) -> List[Dict[str, Any]]:
        """èŽ·å–æ­£åœ¨è¿è¡Œçš„æ¨¡åž‹"""
        try:
            response = self.session.get(f"{self.base_url}/api/ps", timeout=10)
            if response.status_code == 200:
                return response.json().get("models", [])
            else:
                logger.warning(f"èŽ·å–è¿è¡Œä¸­æ¨¡åž‹å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            logger.error(f"èŽ·å–è¿è¡Œä¸­æ¨¡åž‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

# ==================== ã€APIå®¢æˆ·ç«¯ã€‘ ====================
# æ”¯æŒå¤–éƒ¨APIæœåŠ¡çš„å®¢æˆ·ç«¯ï¼Œç”¨äºŽæ··åˆä½¿ç”¨Ollamaå’ŒAPIæ¨¡åž‹

class APIClient:
    """å¤–éƒ¨APIæœåŠ¡å®¢æˆ·ç«¯

    æ”¯æŒOpenAIæ ¼å¼çš„APIè°ƒç”¨ï¼Œæä¾›ç»Ÿä¸€çš„æŽ¥å£æ¥è°ƒç”¨å¤–éƒ¨AIæœåŠ¡ã€‚
    å¯ä»¥ä¸ŽOllamaæ¨¡åž‹æ··åˆä½¿ç”¨ï¼Œæ¯ä¸ªAPIæ¨¡åž‹éƒ½æ˜¯ç‹¬ç«‹çš„å®žä¾‹ã€‚
    """

    def __init__(self, api_url: str, api_key: str, model_name: str, timeout: int = 90):
        """åˆå§‹åŒ–APIå®¢æˆ·ç«¯

        Args:
            api_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            model_name: APIä½¿ç”¨çš„æ¨¡åž‹åç§°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.api_url = api_url
        self.api_key = api_key
        self.model_name = model_name
        self.timeout = timeout
        self.session = requests.Session()

        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        })

    @staticmethod
    def _infer_base_url(api_url: str) -> str:
        """ä»Ž chat completions URL æŽ¨æ–­ base urlï¼ˆç”¨äºŽ /models ç­‰æŽ¥å£ï¼‰"""
        url = (api_url or "").rstrip("/")
        for suffix in ("/chat/completions",):
            if url.endswith(suffix):
                return url[: -len(suffix)]
        # å·²ç»æ˜¯ base çš„æƒ…å†µ
        if url.endswith("/v1") or url.endswith("/api/v3") or url.endswith("/api/v3/"):
            return url.rstrip("/")
        return url

    def list_models(self) -> List[str]:
        """èŽ·å–è¯¥ API æä¾›æ–¹å¯ç”¨æ¨¡åž‹åˆ—è¡¨ï¼ˆè‹¥ä¸æ”¯æŒåˆ™è¿”å›žç©ºåˆ—è¡¨ï¼‰"""
        try:
            base_url = APIClient._infer_base_url(self.api_url)
            resp = self.session.get(f"{base_url}/models", timeout=15)
            if resp.status_code != 200:
                return []
            data = resp.json()
            # OpenAI å…¼å®¹ï¼š{"data":[{"id":"xxx"}, ...]}
            models = []
            for item in data.get("data", []):
                model_id = item.get("id")
                if model_id:
                    models.append(model_id)
            return models
        except (requests.exceptions.RequestException, ValueError, json.JSONDecodeError):
            return []

    def check_connection(self) -> bool:
        """æ£€æŸ¥APIè¿žæŽ¥æ˜¯å¦å¯ç”¨"""
        try:
            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": "Hello"}],
                "max_tokens": 10
            }
            response = self.session.post(self.api_url, json=payload, timeout=10)
            return response.status_code == 200
        except (requests.exceptions.RequestException, ValueError) as e:
            logger.error(f"APIè¿žæŽ¥æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def generate_response(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> Dict[str, Any]:
        """ç”ŸæˆAIå“åº”

        Args:
            prompt: æç¤ºè¯
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            åŒ…å«å“åº”ä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()

        try:
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": temperature
            }

            response = self.session.post(self.api_url, json=payload, timeout=self.timeout)

            if response.status_code == 200:
                result = response.json()
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")

                elapsed_time = time.time() - start_time
                return {
                    "success": True,
                    "model": f"API-{self.model_name}",
                    "response": content,
                    "time": elapsed_time
                }
            else:
                elapsed_time = time.time() - start_time
                error_msg = f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                try:
                    error_detail = response.json()
                    error_msg += f"ï¼Œè¯¦æƒ…: {error_detail}"
                except:
                    pass

                return {
                    "success": False,
                    "model": f"API-{self.model_name}",
                    "response": f"ï¼ˆ{error_msg}ï¼‰",
                    "time": elapsed_time,
                    "error": error_msg
                }

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"APIç”Ÿæˆå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "success": False,
                "model": f"API-{self.model_name}",
                "response": f"ï¼ˆAPIè¯·æ±‚é”™è¯¯: {str(e)}ï¼‰",
                "time": elapsed_time,
                "error": str(e)
            }

# ==================== ã€æ ¸å¿ƒè°ƒåº¦å™¨ã€‘ ====================
# MACPç³»ç»Ÿçš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘æŽ§åˆ¶å™¨

class AICouncilScheduler:
    """å¤šAIåä½œè°ƒåº¦å™¨æ ¸å¿ƒç±»

    ç»Ÿç­¹ç®¡ç†æ•´ä¸ªMACPç³»ç»Ÿçš„è¿è¡Œï¼š
    - åˆå§‹åŒ–ç³»ç»Ÿå’Œæ£€æŸ¥ä¾èµ–
    - åè°ƒå¤šä¸ªAIæ¨¡åž‹çš„åä½œ
    - ç®¡ç†è¾©è®ºæµç¨‹å’Œå…±è¯†æ£€æµ‹
    - å¤„ç†åŽ†å²è®°å½•å’Œæ€§èƒ½ç›‘æŽ§

    è¿™æ˜¯ç³»ç»Ÿçš„"å¤§è„‘"ï¼Œè´Ÿè´£æ‰€æœ‰ä¸šåŠ¡é€»è¾‘çš„ç¼–æŽ’å’Œæ‰§è¡Œ
    """

    def __init__(self):
        self.config = config
        self.client = OllamaClient(self.config.ollama_url)
        # æŒ‰æ¨¡åž‹åˆ†åˆ«ç»´æŠ¤APIå®¢æˆ·ç«¯
        self.api_client = None  # å…¼å®¹æ—§å­—æ®µï¼Œä¸å†å®žé™…ä½¿ç”¨
        self.api_client_model1: Optional[APIClient] = None
        self.api_client_model2: Optional[APIClient] = None
        self.api_client_coordinator: Optional[APIClient] = None
        self.history_manager = HistoryManager(self.config.history_file)
        self.progress_tracker = ProgressTracker()
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        # åˆå§‹åŒ–æ£€æŸ¥
        self._initialize()

    def _initialize(self):
        """åˆå§‹åŒ–è°ƒåº¦å™¨"""
        try:
            logger.info("ðŸš€ åˆå§‹åŒ–MACPè°ƒåº¦å™¨...")

            # æ£€æŸ¥OllamaæœåŠ¡ï¼ˆå¦‚æžœéœ€è¦çš„è¯ï¼‰
            if not self.config.api_mode_enabled or not (self.config.model_1_use_api and self.config.model_2_use_api and self.config.coordinator_use_api):
                if not self.client.check_service():
                    logger.warning("OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œå°†ä»…ä½¿ç”¨APIæ¨¡å¼")
                else:
                    # æ£€æŸ¥Ollamaæ‰€éœ€æ¨¡åž‹
                    required_models = []
                    if not self.config.model_1_use_api:
                        required_models.append(self.config.model_1)
                    if not self.config.model_2_use_api:
                        required_models.append(self.config.model_2)
                    if not self.config.coordinator_use_api:
                        required_models.append(self.config.coordinator_model)

                    if required_models:
                        model_status = self.client.check_models(required_models)
                        missing_models = [model for model, available in model_status.items() if not available]
                        if missing_models:
                            logger.warning(f"Ollamaç¼ºå°‘æ¨¡åž‹: {', '.join(missing_models)}ï¼Œå°†å°è¯•ä½¿ç”¨APIæ›¿ä»£")

            # åˆå§‹åŒ–APIå®¢æˆ·ç«¯ï¼ˆå¦‚æžœå¯ç”¨äº†APIæ¨¡å¼ï¼‰
            if self.config.api_mode_enabled:
                self._initialize_api_client()

            logger.info("âœ… åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _initialize_api_client(self):
        """åˆå§‹åŒ–APIå®¢æˆ·ç«¯ï¼ˆæŒ‰æ¨¡åž‹åˆ†åˆ«åˆå§‹åŒ–ï¼‰"""

        def create_client(api_url: str, api_key: str, api_model: str) -> Optional[APIClient]:
            api_url = (api_url or "").strip()
            api_key = (api_key or "").strip()
            api_model = (api_model or "").strip()
            if not api_url or not api_key or not api_model:
                return None
            client = APIClient(
                api_url=api_url,
                api_key=api_key,
                model_name=api_model,
                timeout=self.config.timeout
            )
            # ä»…åšä¸€æ¬¡ç®€å•è¿žé€šæ€§æ£€æŸ¥ï¼Œä¸å¼ºåˆ¶å¤±è´¥
            if client.check_connection():
                logger.info(f"âœ… APIå®¢æˆ·ç«¯å·²å°±ç»ª: {api_model}")
            else:
                logger.warning(f"âš ï¸ APIå®¢æˆ·ç«¯è¿žæŽ¥æ£€æŸ¥å¤±è´¥: {api_model}")
            return client

        # æ¨¡åž‹1
        if self.config.model_1_use_api:
            url = getattr(self.config, "model_1_api_url", "") or self.config.api_url
            key = getattr(self.config, "model_1_api_key", "") or self.config.api_key
            model = getattr(self.config, "model_1_api_model", "") or self.config.api_model
            self.api_client_model1 = create_client(url, key, model)
        else:
            self.api_client_model1 = None

        # æ¨¡åž‹2
        if self.config.model_2_use_api:
            url = getattr(self.config, "model_2_api_url", "") or self.config.api_url
            key = getattr(self.config, "model_2_api_key", "") or self.config.api_key
            model = getattr(self.config, "model_2_api_model", "") or self.config.api_model
            self.api_client_model2 = create_client(url, key, model)
        else:
            self.api_client_model2 = None

        # åè°ƒAI
        if self.config.coordinator_use_api:
            url = getattr(self.config, "coordinator_api_url", "") or self.config.api_url
            key = getattr(self.config, "coordinator_api_key", "") or self.config.api_key
            model = getattr(self.config, "coordinator_api_model", "") or self.config.api_model
            self.api_client_coordinator = create_client(url, key, model)
        else:
            self.api_client_coordinator = None

        if not any([self.api_client_model1, self.api_client_model2, self.api_client_coordinator]):
            logger.warning("APIæ¨¡å¼å·²å¯ç”¨ï¼Œä½†æœªæˆåŠŸåˆå§‹åŒ–ä»»ä½•APIå®¢æˆ·ç«¯ï¼Œè¯·æ£€æŸ¥é…ç½®")

    def _get_client_for_model(self, model_name: str) -> tuple:
        """æ ¹æ®æ¨¡åž‹åç§°è¿”å›žå¯¹åº”çš„å®¢æˆ·ç«¯å’Œæ¨¡åž‹æ ‡è¯†

        Returns:
            (client, model_identifier, is_api_client) å…ƒç»„
        """
        if not self.config.api_mode_enabled:
            return self.client, model_name, False

        if model_name == self.config.model_1 and self.config.model_1_use_api and self.api_client_model1:
            api_model = getattr(self.config, "model_1_api_model", "") or self.config.api_model
            return self.api_client_model1, f"API-{api_model}", True
        elif model_name == self.config.model_2 and self.config.model_2_use_api and self.api_client_model2:
            api_model = getattr(self.config, "model_2_api_model", "") or self.config.api_model
            return self.api_client_model2, f"API-{api_model}", True
        elif model_name == self.config.coordinator_model and self.config.coordinator_use_api and self.api_client_coordinator:
            api_model = getattr(self.config, "coordinator_api_model", "") or self.config.api_model
            return self.api_client_coordinator, f"API-{api_model}", True
        else:
            return self.client, model_name, False

    # ==================== ã€æ ¸å¿ƒæ–¹æ³•ã€‘ ====================
    def ask_both_models(self, question: str, mode: str = "parallel",
                       role1: Optional[str] = None, role2: Optional[str] = None) -> List[Dict[str, Any]]:
        """å‘ä¸¤ä¸ªæ¨¡åž‹æé—®ï¼ˆæ”¯æŒå¤šç§æ¨¡å¼ï¼‰"""
        DisplayManager.print_header("ðŸ§  é—®é¢˜å¤„ç†")
        print(f"é—®é¢˜: {question}")
        print(f"æ¨¡å¼: {mode}")
        DisplayManager.print_separator()

        self.progress_tracker.start()

        try:
            if mode == "parallel":
                return self._parallel_ask(question)
            elif mode == "debate":
                return self._debate_ask(question, role1, role2)
            elif mode == "turtle_soup":
                return self._turtle_soup_ask(question, role1, role2)
            else:
                raise ValueError(f"æœªçŸ¥æ¨¡å¼: {mode}")
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

    def _parallel_ask(self, question: str) -> List[Dict[str, Any]]:
        """å¹¶è¡Œæé—®é€»è¾‘ï¼ˆæ”¯æŒAPIæ¨¡å¼ï¼‰"""
        logger.info("å¼€å§‹å¹¶è¡Œæé—®")

        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_to_model = {}

            # æ¨¡åž‹1
            if self.config.api_mode_enabled and self.config.model_1_use_api and self.api_client_model1:
                future_to_model[executor.submit(self.api_client_model1.generate_response,
                                              question, self.config.max_tokens, self.config.temperature)] = f"API-{getattr(self.config, 'model_1_api_model', '') or self.config.api_model}"
            else:
                future_to_model[executor.submit(self.client.generate_response,
                                              self.config.model_1, question,
                                              self.config.max_tokens, self.config.temperature,
                                              self.config.timeout)] = self.config.model_1

            # æ¨¡åž‹2
            if self.config.api_mode_enabled and self.config.model_2_use_api and self.api_client_model2:
                future_to_model[executor.submit(self.api_client_model2.generate_response,
                                              question, self.config.max_tokens, self.config.temperature)] = f"API-{getattr(self.config, 'model_2_api_model', '') or self.config.api_model}"
            else:
                future_to_model[executor.submit(self.client.generate_response,
                                              self.config.model_2, question,
                                              self.config.max_tokens, self.config.temperature,
                                              self.config.timeout)] = self.config.model_2

            results = []
            for future in concurrent.futures.as_completed(future_to_model):
                model_name = future_to_model[future]
                try:
                    result = future.result()
                    results.append(result)
                    self.progress_tracker.update()
                except Exception as e:
                    logger.error(f"æ¨¡åž‹ {model_name} æ‰§è¡Œé”™è¯¯: {e}")
                    results.append({
                        "success": False,
                        "model": model_name,
                        "error": f"æ‰§è¡Œé”™è¯¯: {str(e)}",
                        "time": 0
                    })

        self._display_results(results)

        if self.config.save_history:
            self._save_history_entry(question, results, mode="parallel")

        return results

    # ==================== ã€è¾©è®ºæ¨¡å¼ã€‘ ====================
    def _debate_ask(self, question: str, role1: Optional[str] = None, role2: Optional[str] = None) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå¤šå›žåˆAIè¾©è®º - å¢žå¼ºç‰ˆ

        è¿™æ˜¯MACPç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå®žçŽ°ä¸¤ä¸ªAIæ¨¡åž‹ä¹‹é—´çš„è¾©è®ºï¼š
        1. åˆå§‹åŒ–è¾©è®ºè§’è‰²ï¼ˆæ”¯æŒæ­£åæ–¹ç«‹åœºï¼‰
        2. è¿›è¡Œå¤šå›žåˆè¾©è®ºå¯¹è¯ï¼ŒåŒæ–¹å¯ä»¥çœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡
        3. å®žæ—¶AIå…±è¯†åº¦åˆ†æžå’Œç›‘æŽ§
        4. è¾¾åˆ°é˜ˆå€¼æ—¶è‡ªåŠ¨ç»“æŸå¹¶ç”Ÿæˆæ€»ç»“

        Args:
            question: è¾©è®ºä¸»é¢˜é—®é¢˜
            role1: ç¬¬ä¸€ä¸ªAIçš„è§’è‰²ï¼ˆé»˜è®¤ä¸ºæ­£æ–¹ï¼‰
            role2: ç¬¬äºŒä¸ªAIçš„è§’è‰²ï¼ˆé»˜è®¤ä¸ºåæ–¹ï¼‰

        Returns:
            åŒ…å«ä¸¤ä¸ªAIå“åº”çš„ç»“æžœåˆ—è¡¨
        """
        # ç¡®å®šè§’è‰²
        role1 = role1 or self.config.default_role_1
        role2 = role2 or self.config.default_role_2

        # æž„é€ æ˜¾ç¤ºåï¼šæ¨¡åž‹å-è§’è‰²å
        display_name1 = f"{self.config.model_1}-{role1}"
        display_name2 = f"{self.config.model_2}-{role2}"

        role_prompt1 = role_system.get_role_prompt(role1, is_first=True)   # æ­£æ–¹
        role_prompt2 = role_system.get_role_prompt(role2, is_first=False)  # åæ–¹

        if not role_prompt1 or not role_prompt2:
            raise InvalidRoleError(f"æ— æ•ˆè§’è‰²: {role1} æˆ– {role2}")

        self._setup_debate_roles(question, role1, role2)

        # ç¬¬ä¸€å›žåˆï¼šåŒæ–¹çŸ¥é“å¯¹æ‰‹æ˜¯è°ï¼Œä½†çœ‹ä¸åˆ°å…·ä½“è§‚ç‚¹
        DisplayManager.print_separator("-", 40)
        print("ç¬¬1å›žåˆï¼šåˆå§‹é™ˆè¿°")
        DisplayManager.print_separator("-", 40)
        print(f"ðŸ’¡ {role1} vs {role2} - åŒæ–¹å·²çŸ¥æ™“å¯¹æ‰‹èº«ä»½")

        # å¢žå¼ºç‰ˆç¬¬ä¸€å›žåˆæç¤ºè¯ - è®©AIçŸ¥é“å¯¹æ‰‹æ˜¯è°ï¼Œå¹¶è¦æ±‚ç®€æ´è¡¨è¾¾
        prompt1 = f"""{role_prompt1}

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}

ã€ä½ çš„ç«‹åœºã€‘: {role1}ï¼ˆæ­£æ–¹ï¼‰
ã€å¯¹æ‰‹è§’è‰²ã€‘: {role2}ï¼ˆåæ–¹ï¼‰

è¯·ç®€æ´æœ‰åŠ›åœ°é˜è¿°ä½ çš„æ ¸å¿ƒè§‚ç‚¹ï¼ˆé‡ç‚¹çªå‡º3-5ä¸ªå…³é”®è®ºç‚¹ï¼‰ï¼š
"""

        prompt2 = f"""{role_prompt2}

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}

ã€ä½ çš„ç«‹åœºã€‘: {role2}ï¼ˆåæ–¹ï¼‰
ã€å¯¹æ‰‹è§’è‰²ã€‘: {role1}ï¼ˆæ­£æ–¹ï¼‰

è¯·ç®€æ´æœ‰åŠ›åœ°é˜è¿°ä½ çš„æ ¸å¿ƒè§‚ç‚¹ï¼ˆé‡ç‚¹çªå‡º3-5ä¸ªå…³é”®è®ºç‚¹ï¼‰ï¼š
"""

        # ä½¿ç”¨æ­£ç¡®çš„å®¢æˆ·ç«¯è¿›è¡Œæé—®
        client1, model_id1, is_api1 = self._get_client_for_model(self.config.model_1)
        client2, model_id2, is_api2 = self._get_client_for_model(self.config.model_2)

        if is_api1:
            result1 = client1.generate_response(prompt1, max_tokens=500, temperature=self.config.temperature)
        else:
            result1 = client1.generate_response(self.config.model_1, prompt1, max_tokens=500,
                                              temperature=self.config.temperature, timeout=self.config.timeout,
                                              streaming=self.config.streaming_output)

        if is_api2:
            result2 = client2.generate_response(prompt2, max_tokens=500, temperature=self.config.temperature)
        else:
            result2 = client2.generate_response(self.config.model_2, prompt2, max_tokens=500,
                                              temperature=self.config.temperature, timeout=self.config.timeout,
                                              streaming=self.config.streaming_output)

        # å®‰å…¨å¤„ç†
        if not result1.get("success"):
            result1 = {"success": False, "response": "ï¼ˆæ¨¡åž‹1æ— å“åº”ï¼‰"}
        if not result2.get("success"):
            result2 = {"success": False, "response": "ï¼ˆæ¨¡åž‹2æ— å“åº”ï¼‰"}

        response1 = result1.get("response", "")
        response2 = result2.get("response", "")

        debate_round = [
            {"round": 1, "speaker": display_name1, "content": response1, "type": "opening"},
            {"round": 1, "speaker": display_name2, "content": response2, "type": "opening"}
        ]

        self._display_debate_response(display_name1, response1)
        self._display_debate_response(display_name2, response2)

        # åŽç»­å›žåˆï¼ˆæ™ºèƒ½æå‰ç»“æŸï¼‰
        max_rounds = min(self.config.debate_rounds, 6)
        consensus_reached = False
        consensus_analysis = ""

        for round_num in range(2, max_rounds + 1):
            # æ£€æŸ¥å…±è¯†ï¼ˆä½¿ç”¨AIåˆ†æžï¼‰
            if self.config.enable_early_stop and self.config.ai_consensus_analysis and round_num >= self.config.consensus_check_start_round:
                print(f"\nðŸ§  æ­£åœ¨åˆ†æžåŒæ–¹å…±è¯†åº¦...")
                consensus_score, analysis, analysis_data = ConsensusDetector.analyze_debate_consensus(
                    self, self.config.coordinator_model, question, debate_round, display_name1, display_name2
                )

                consensus_percentage = int(consensus_score * 100)
                logger.info(f"ðŸ”„ AIå…±è¯†åˆ†æž: {consensus_percentage}% - {analysis}")

                # æ˜¾ç¤ºå…±è¯†åº¦æ¡å½¢å›¾
                ConsensusDetector.display_consensus_bar(consensus_percentage)

                print(f"ðŸ“ AIåˆ†æž: {analysis}")

                # æ˜¾ç¤ºè¯¦ç»†åˆ†æžï¼ˆå¦‚æžœæœ‰ï¼‰
                if analysis_data:
                    if 'key_agreements' in analysis_data and analysis_data['key_agreements']:
                        agreements = analysis_data['key_agreements'][:3]
                        print(f"ðŸ¤ å…±è¯†ç‚¹: {len(agreements)}é¡¹")
                        for i, agreement in enumerate(agreements, 1):
                            print(f"   {i}. {agreement}")

                    if 'key_disagreements' in analysis_data and analysis_data['key_disagreements']:
                        disagreements = analysis_data['key_disagreements'][:3]
                        print(f"âš”ï¸  åˆ†æ­§ç‚¹: {len(disagreements)}é¡¹")
                        for i, disagreement in enumerate(disagreements, 1):
                            print(f"   {i}. {disagreement}")

                    if 'recommendation' in analysis_data:
                        recommendation = analysis_data['recommendation']
                        if recommendation == 'end':
                            print(f"ðŸŽ¯ AIå»ºè®®: ç»“æŸè¾©è®º")
                        else:
                            print(f"ðŸ”„ AIå»ºè®®: ç»§ç»­è¾©è®º")

                consensus_analysis = analysis

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                threshold_percentage = int(config.consensus_threshold * 100)
                consensus_reached = InteractiveInterface._handle_consensus_feedback(consensus_score, consensus_percentage, threshold_percentage, consensus_reached)
                if consensus_reached:
                    break

            DisplayManager.print_separator("-", 40)
            print(f"ç¬¬{round_num}å›žåˆï¼šäº’ç›¸å›žåº”")
            DisplayManager.print_separator("-", 40)

            # æž„å»ºè¾©è®ºåŽ†å²ä¸Šä¸‹æ–‡
            debate_history = AICouncilScheduler._build_debate_context(debate_round, display_name1, display_name2)

            # æ¨¡åž‹1å›žåº”æ¨¡åž‹2 - å¢žå¼ºç‰ˆï¼šçœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡
            if result1.get("success") and result2.get("success"):
                rebuttal_prompt1 = f"""{role_prompt1}

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}
ã€ä½ çš„ç«‹åœºã€‘: {role1}ï¼ˆæ­£æ–¹ï¼‰
ã€å¯¹æ‰‹è§’è‰²ã€‘: {role2}ï¼ˆåæ–¹ï¼‰

{debate_history}

ã€ä½ çš„ä»»åŠ¡ã€‘
é’ˆå¯¹{role2}çš„è§‚ç‚¹è¿›è¡Œç®€æ´æœ‰åŠ›çš„åé©³ï¼š
1. æŒ‡å‡ºå¯¹æ‰‹è§‚ç‚¹çš„æ ¸å¿ƒå¼±ç‚¹
2. ç”¨1-2ä¸ªå…³é”®è®ºæ®è¿›è¡Œåé©³
3. é‡ç”³ä½ çš„æ ¸å¿ƒç«‹åœº

è¯·ç®€æ´å›žåº”ï¼ˆé‡ç‚¹çªå‡ºï¼Œä¸è¶…è¿‡300å­—ï¼‰ï¼š
"""
                client1, _, is_api1 = self._get_client_for_model(self.config.model_1)
                if is_api1:
                    result1 = client1.generate_response(rebuttal_prompt1, max_tokens=600, temperature=self.config.temperature)
                else:
                    result1 = client1.generate_response(self.config.model_1, rebuttal_prompt1, max_tokens=600,
                                                      temperature=self.config.temperature, timeout=self.config.timeout,
                                                      streaming=self.config.streaming_output)

                if result1.get("success"):
                    response1 = result1.get("response", "")
                    debate_round.append({
                        "round": round_num,
                        "speaker": display_name1,
                        "content": response1,
                        "type": "rebuttal"
                    })
                    self._display_debate_response(display_name1, response1, f"åé©³{role2}")

            # æ¨¡åž‹2å›žåº”æ¨¡åž‹1 - å¢žå¼ºç‰ˆï¼šçœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡
            if result1.get("success") and result2.get("success"):
                # æ›´æ–°è¾©è®ºåŽ†å²ï¼ŒåŒ…å«æœ€æ–°çš„AI1å›žåº”
                debate_history = AICouncilScheduler._build_debate_context(debate_round, display_name1, display_name2)

                rebuttal_prompt2 = f"""{role_prompt2}

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}
ã€ä½ çš„ç«‹åœºã€‘: {role2}ï¼ˆåæ–¹ï¼‰
ã€å¯¹æ‰‹è§’è‰²ã€‘: {role1}ï¼ˆæ­£æ–¹ï¼‰

{debate_history}

ã€ä½ çš„ä»»åŠ¡ã€‘
é’ˆå¯¹{role1}çš„åé©³è¿›è¡Œç®€æ´æœ‰åŠ›çš„å›žåº”ï¼š
1. åé©³å¯¹æ‰‹çš„åé©³è®ºç‚¹
2. ç”¨1-2ä¸ªå…³é”®è®ºæ®åŠ å¼ºä½ çš„ç«‹åœº
3. æå‡ºæ–°çš„è¾©è®ºè§’åº¦

è¯·ç®€æ´å›žåº”ï¼ˆé‡ç‚¹çªå‡ºï¼Œä¸è¶…è¿‡300å­—ï¼‰ï¼š
"""
                client2, _, is_api2 = self._get_client_for_model(self.config.model_2)
                if is_api2:
                    result2 = client2.generate_response(rebuttal_prompt2, max_tokens=600, temperature=self.config.temperature)
                else:
                    result2 = client2.generate_response(self.config.model_2, rebuttal_prompt2, max_tokens=600,
                                                      temperature=self.config.temperature, timeout=self.config.timeout,
                                                      streaming=self.config.streaming_output)

                if result2.get("success"):
                    response2 = result2.get("response", "")
                    debate_round.append({
                        "round": round_num,
                        "speaker": display_name2,
                        "content": response2,
                        "type": "rebuttal"
                    })
                    self._display_debate_response(display_name2, response2, f"åé©³{role1}")

        # åè°ƒé˜¶æ®µ
        DisplayManager.print_header("ðŸŽ¯ åè°ƒæ€»ç»“")

        if consensus_reached:
            print("ðŸ¤ åŒæ–¹å·²è¾¾æˆé«˜åº¦å…±è¯†ï¼Œç”Ÿæˆæœ€ç»ˆæ€»ç»“")
            self._generate_consensus_summary(question, debate_round, role1, role2, consensus_analysis)
        else:
            self._coordinate_responses(question, debate_round, role1, role2)

        # ä¿å­˜è®°å½•
        if self.config.save_history:
            self._save_debate_entry(question, debate_round, display_name1, display_name2)

        # è¿”å›žå®Œæ•´çš„è¾©è®ºè®°å½•ï¼Œè®©ç”¨æˆ·å¯ä»¥çœ‹åˆ°æ‰€æœ‰å‘è¨€
        return debate_round

    def _generate_consensus_summary(self, question: str, debate_round: List[Dict[str, Any]],
                                   role1: str, role2: str, consensus_analysis: str) -> str:
        """ç”Ÿæˆè¾©è®ºå…±è¯†æ€»ç»“æŠ¥å‘Š

        å½“è¾©è®ºè¾¾åˆ°å…±è¯†é˜ˆå€¼æ—¶ï¼Œè°ƒç”¨åè°ƒAIç”Ÿæˆä¸“ä¸šçš„æ€»ç»“æŠ¥å‘Šï¼š
        1. æ•´ç†å®Œæ•´çš„è¾©è®ºè¿‡ç¨‹å’Œå…±è¯†åˆ†æžç»“æžœ
        2. è¦æ±‚AIç”Ÿæˆç»“æž„åŒ–çš„æ€»ç»“æŠ¥å‘Š
        3. åŒ…å«è¾©è®ºå›žé¡¾ã€å…±è¯†è¯„ä¼°ã€åŒæ–¹è§‚ç‚¹å¯¹æ¯”å’Œç»¼åˆç»“è®º

        è¿™æ˜¯MACPç³»ç»Ÿçš„æ ¸å¿ƒä»·å€¼ä¹‹ä¸€ï¼Œèƒ½å¤Ÿå°†AIè¾©è®ºè½¬åŒ–ä¸º
        æœ‰ä»·å€¼çš„åˆ†æžæŠ¥å‘Šï¼Œå¸®åŠ©ç”¨æˆ·æ·±å…¥ç†è§£è¾©è®ºä¸»é¢˜
        """
        print(f"\nðŸ¤– åè°ƒAI ({self.config.coordinator_model}) æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ€»ç»“...")

        # æž„å»ºè¾©è®ºæ‘˜è¦
        debate_summary = ""
        for entry in debate_round[-6:]:  # æœ€åŽ6è½®å¯¹è¯
            debate_summary += f"\n{entry['speaker']}: {entry.get('content', '')[:200]}"

        summary_prompt = f"""åŸºäºŽä»¥ä¸‹è¾©è®ºè¿‡ç¨‹å’Œå…±è¯†åˆ†æžï¼Œè¯·ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Šï¼š

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}
ã€è¾©è®ºåŒæ–¹ã€‘: {role1} vs {role2}
ã€å…±è¯†åˆ†æžã€‘: {consensus_analysis}

ã€è¾©è®ºè¿‡ç¨‹æ‘˜è¦ã€‘:
{debate_summary}

è¯·ç”Ÿæˆç»“æž„åŒ–çš„æ€»ç»“æŠ¥å‘Šï¼ŒåŒ…å«ï¼š

## ðŸŽ¯ è¾©è®ºæ€»ç»“

### ðŸ“Š å…±è¯†è¯„ä¼°
- æœ€ç»ˆå…±è¯†åº¦ï¼šXX%
- è¾¾æˆå…±è¯†çš„ä¸»è¦æ–¹é¢
- ä»å­˜åœ¨çš„åˆ†æ­§ç‚¹

### ðŸ—£ï¸ åŒæ–¹è§‚ç‚¹å¯¹æ¯”
- {role1}çš„æ ¸å¿ƒç«‹åœº
- {role2}çš„æ ¸å¿ƒç«‹åœº
- åŒæ–¹è§‚ç‚¹çš„æ¼”å˜è¿‡ç¨‹

### ðŸ’¡ ç»¼åˆç»“è®º
- å¯¹åŽŸé—®é¢˜çš„æœ€ç»ˆç­”æ¡ˆ
- å»ºè®¾æ€§å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ

### ðŸ“ˆ è¾©è®ºè´¨é‡è¯„ä¼°
- è®ºè¯é€»è¾‘æ€§
- è§‚ç‚¹æ·±åº¦
- æ²Ÿé€šæœ‰æ•ˆæ€§

è¯·ç¡®ä¿æ€»ç»“å®¢è§‚ã€ä¸­ç«‹ï¼Œå¹¶åŸºäºŽåŒæ–¹çš„å®žé™…è®ºè¿°ã€‚"""

        # è°ƒè¯•ä¿¡æ¯
        logger.info(f"å…±è¯†æ€»ç»“è°ƒè¯• - é—®é¢˜: {question}")
        logger.info(f"å…±è¯†æ€»ç»“è°ƒè¯• - è¾©è®ºè½®æ•°: {len(debate_round)}")
        logger.info(f"å…±è¯†æ€»ç»“è°ƒè¯• - å…±è¯†åˆ†æžé•¿åº¦: {len(consensus_analysis)}")
        logger.info(f"å…±è¯†æ€»ç»“è°ƒè¯• - åè°ƒæ¨¡åž‹: {self.config.coordinator_model}")

        coord_client, coord_model, is_api = self._get_client_for_model(self.config.coordinator_model)
        if is_api:
            summary_result = coord_client.generate_response(summary_prompt, max_tokens=1000, temperature=self.config.temperature)
        else:
            summary_result = coord_client.generate_response(coord_model, summary_prompt, max_tokens=1000,
                                                          temperature=self.config.temperature, timeout=self.config.timeout,
                                                          streaming=False)

        # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        logger.info(f"å…±è¯†æ€»ç»“è°ƒè¯• - è¯·æ±‚ç»“æžœ: {summary_result}")
        logger.info(f"å…±è¯†æ€»ç»“è°ƒè¯• - æˆåŠŸçŠ¶æ€: {summary_result.get('success')}")
        logger.info(f"å…±è¯†æ€»ç»“è°ƒè¯• - å“åº”é•¿åº¦: {len(summary_result.get('response', ''))}")

        if summary_result.get("success"):
            summary = summary_result.get("response", "")
            logger.info(f"å…±è¯†æ€»ç»“è°ƒè¯• - å“åº”å†…å®¹: {summary[:200]}...")

            if not summary.strip():
                logger.warning("å…±è¯†æ€»ç»“è°ƒè¯• - å“åº”å†…å®¹ä¸ºç©º")
                print("âš ï¸  å…±è¯†æ€»ç»“AIè¿”å›žäº†ç©ºå“åº”")
                return f"åŸºäºŽå…±è¯†åˆ†æžçš„æ€»ç»“ï¼š{consensus_analysis}\n\nè¾©è®ºå·²è‡ªåŠ¨ç»“æŸï¼ŒåŒæ–¹è¾¾æˆé«˜åº¦å…±è¯†ã€‚"

            print(f"\nâœ… å…±è¯†æ€»ç»“ç”Ÿæˆå®Œæˆï¼š")
            print(summary[:self.config.display_length] +
                  ("..." if len(summary) > self.config.display_length else ""))
            return summary
        else:
            logger.warning(f"å…±è¯†æ€»ç»“ç”Ÿæˆå¤±è´¥ - é”™è¯¯è¯¦æƒ…: {summary_result}")
            print(f"âŒ å…±è¯†æ€»ç»“ç”Ÿæˆå¤±è´¥ - è¯¦æƒ…: {summary_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return f"åŸºäºŽå…±è¯†åˆ†æžçš„æ€»ç»“ï¼š{consensus_analysis}\n\nè¾©è®ºå·²è‡ªåŠ¨ç»“æŸï¼ŒåŒæ–¹è¾¾æˆé«˜åº¦å…±è¯†ã€‚"

    def _coordinate_responses(self, question: str, debate_round: List[Dict[str, Any]],
                            role1: str, role2: str) -> str:
        """åè°ƒè¾©è®ºç»“æžœ"""
        print(f"\nðŸ¤– åè°ƒAI ({self.config.coordinator_model}) æ­£åœ¨åˆ†æž...")

        # æž„å»ºæ‘˜è¦
        debate_summary = ""
        for entry in debate_round[:4]:  # åªå–å‰4è½®
            debate_summary += f"\n{entry['speaker']}: {entry.get('content', '')[:150]}"

        coord_prompt = f"""è¯·ä½œä¸ºä¸­ç«‹åè°ƒå‘˜åˆ†æžä»¥ä¸‹è¾©è®ºï¼š
é—®é¢˜ï¼š{question}
è¾©è®ºåŒæ–¹ï¼š{role1} vs {role2}
è¾©è®ºæ‘˜è¦ï¼š{debate_summary}

è¯·æä¾›ç®€è¦åˆ†æžï¼ˆé™{self.config.max_tokens}tokenï¼‰ï¼š
1. æ ¸å¿ƒå…±è¯†ç‚¹
2. ä¸»è¦åˆ†æ­§
3. ç»¼åˆå»ºè®®"""

        # è°ƒè¯•ä¿¡æ¯
        logger.info(f"åè°ƒAIè°ƒè¯• - é—®é¢˜: {question}")
        logger.info(f"åè°ƒAIè°ƒè¯• - è¾©è®ºè½®æ•°: {len(debate_round)}")
        logger.info(f"åè°ƒAIè°ƒè¯• - æ‘˜è¦é•¿åº¦: {len(debate_summary)}")
        logger.info(f"åè°ƒAIè°ƒè¯• - åè°ƒæ¨¡åž‹: {self.config.coordinator_model}")

        coord_client, coord_model, is_api = self._get_client_for_model(self.config.coordinator_model)
        if is_api:
            coord_result = coord_client.generate_response(coord_prompt, max_tokens=800, temperature=self.config.temperature)
        else:
            coord_result = coord_client.generate_response(coord_model, coord_prompt, max_tokens=800,
                                                        temperature=self.config.temperature, timeout=self.config.timeout,
                                                        streaming=False)

        # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        logger.info(f"åè°ƒAIè°ƒè¯• - è¯·æ±‚ç»“æžœ: {coord_result}")
        logger.info(f"åè°ƒAIè°ƒè¯• - æˆåŠŸçŠ¶æ€: {coord_result.get('success')}")
        logger.info(f"åè°ƒAIè°ƒè¯• - å“åº”é•¿åº¦: {len(coord_result.get('response', ''))}")

        if coord_result.get("success"):
            coord_response = coord_result.get("response", "")
            logger.info(f"åè°ƒAIè°ƒè¯• - å“åº”å†…å®¹: {coord_response[:200]}...")

            if not coord_response.strip():
                logger.warning("åè°ƒAIè°ƒè¯• - å“åº”å†…å®¹ä¸ºç©º")
                print("âš ï¸  åè°ƒAIè¿”å›žäº†ç©ºå“åº”")
                return "åè°ƒAIè¿”å›žäº†ç©ºå“åº”ï¼Œè¯·æ£€æŸ¥æ¨¡åž‹é…ç½®"

            print(f"\nâœ… åè°ƒAIåˆ†æžå®Œæˆï¼š")
            print(coord_response[:self.config.display_length] +
                  ("..." if len(coord_response) > self.config.display_length else ""))
            return coord_response
        else:
            logger.warning(f"åè°ƒAIåˆ†æžå¤±è´¥ - é”™è¯¯è¯¦æƒ…: {coord_result}")
            print(f"âŒ åè°ƒAIåˆ†æžå¤±è´¥ - è¯¦æƒ…: {coord_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return f"åè°ƒåˆ†æžå¤±è´¥: {coord_result.get('error', 'æœªçŸ¥é”™è¯¯')}"

    # ==================== ã€æµ·é¾Ÿæ±¤æ¨¡å¼ã€‘ ====================
    def _turtle_soup_ask(self, question: str, role1: Optional[str] = None, role2: Optional[str] = None) -> List[Dict[str, Any]]:
        """æµ·é¾Ÿæ±¤æ¨¡å¼"""
        DisplayManager.print_header("ðŸ¢ æµ·é¾Ÿæ±¤æ¨¡å¼")
        print("è§„åˆ™ï¼š")
        print("1. ä¸¤ä¸ªAIä¼šè½®æµå‘æ‚¨æé—®")
        print("2. æ¯ä¸ªAIæ¯å›žåˆåªèƒ½é—®ä¸€ä¸ªé—®é¢˜")
        print("3. æ‚¨åªèƒ½å›žç­” 'æ˜¯'ã€'å¦' æˆ– 'ä¸çŸ¥é“'")
        print("4. ç›®æ ‡æ˜¯è®©AIçŒœå‡ºè°œåº•")
        DisplayManager.print_separator()

        role1 = role1 or "ä¾¦æŽ¢"
        role2 = role2 or "æŽ¨ç†è€…"

        history = []
        round_count = 0
        max_rounds = self.config.turtle_soup_max_rounds

        while round_count < max_rounds:
            round_count += 1
            DisplayManager.print_separator("-", 40)
            print(f"ç¬¬{round_count}å›žåˆ")
            DisplayManager.print_separator("-", 40)

            # äº¤æ›¿æé—®
            current_role = role1 if round_count % 2 == 1 else role2
            current_model = self.config.model_1 if round_count % 2 == 1 else self.config.model_2

            # ç”Ÿæˆé—®é¢˜
            if round_count == 1:
                prompt = f"""ä½ æ˜¯{current_role}ï¼Œæ­£åœ¨çŽ©æµ·é¾Ÿæ±¤æ¸¸æˆã€‚
è°œé¢ï¼š{question}
ä½ çš„ä»»åŠ¡æ˜¯å‘çŽ©å®¶æé—®ï¼Œæ¯æ¬¡åªèƒ½é—®ä¸€ä¸ªé—®é¢˜ï¼ŒçŽ©å®¶åªèƒ½å›žç­”æ˜¯ã€å¦æˆ–ä¸çŸ¥é“ã€‚
è¯·å¼€å§‹ä½ çš„ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆåªé—®ä¸€ä¸ªé—®é¢˜ï¼‰ï¼š"""
            else:
                history_text = "\n".join(history[-4:])  # æœ€è¿‘4æ¡åŽ†å²
                prompt = f"""ä½ æ˜¯{current_role}ï¼Œæ­£åœ¨çŽ©æµ·é¾Ÿæ±¤æ¸¸æˆã€‚
è°œé¢ï¼š{question}
åŽ†å²é—®ç­”ï¼š
{history_text}
è¯·åŸºäºŽä»¥ä¸Šä¿¡æ¯é—®ä¸‹ä¸€ä¸ªé—®é¢˜ï¼ˆåªé—®ä¸€ä¸ªé—®é¢˜ï¼‰ï¼š"""

            client, model_id, is_api = self._get_client_for_model(current_model)
            if is_api:
                result = client.generate_response(prompt, max_tokens=200, temperature=self.config.temperature)
            else:
                result = client.generate_response(model_id, prompt, max_tokens=200,
                                                temperature=self.config.temperature, timeout=self.config.timeout,
                                                streaming=self.config.streaming_output)

            if result.get("success"):
                question_text = result.get("response", "").strip()
                print(f"\nâ“ {current_role} æé—®ï¼š{question_text}")

                # ç”¨æˆ·å›žç­”
                answer = self._get_turtle_soup_answer()
                if answer == "ç»“æŸ":
                    print("ðŸ‘¤ ç”¨æˆ·é€‰æ‹©ç»“æŸæ¸¸æˆ")
                    break

                history.append(f"é—®ï¼š{question_text}")
                history.append(f"ç­”ï¼š{answer}")

                # çŒœæµ‹ç­”æ¡ˆ
                if round_count % 3 == 0:
                    guess = self._attempt_guess(current_model, current_role, question, history)
                    if guess and self._confirm_guess(guess):
                        print(f"ðŸŽ‰ æ­å–œï¼{current_role} çŒœå¯¹äº†ï¼")
                        break
            else:
                print(f"âŒ {current_role} æé—®å¤±è´¥")
                break

        # æœ€ç»ˆæ€»ç»“
        self._finalize_turtle_soup(question, history)
        return []

    @staticmethod
    def _get_turtle_soup_answer() -> str:
        """èŽ·å–æµ·é¾Ÿæ±¤ç­”æ¡ˆ"""
        while True:
            answer = input("\næ‚¨çš„å›žç­”ï¼ˆæ˜¯/å¦/ä¸çŸ¥é“/ç»“æŸï¼‰: ").strip().lower()
            if answer in ["æ˜¯", "å¦", "ä¸çŸ¥é“", "ç»“æŸ"]:
                return answer
            print("âŒ è¯·åªå›žç­”ï¼šæ˜¯ã€å¦ã€ä¸çŸ¥é“ æˆ– ç»“æŸ")

    def _attempt_guess(self, model: str, role: str, question: str, history: List[str]) -> Optional[str]:
        """å°è¯•çŒœæµ‹ç­”æ¡ˆ"""
        guess_prompt = f"""åŸºäºŽä»¥ä¸‹ä¿¡æ¯ï¼Œè¯·çŒœæµ‹è°œåº•ï¼š
è°œé¢ï¼š{question}
åŽ†å²é—®ç­”ï¼š
{"\n".join(history[-6:])}
è¯·ç»™å‡ºä½ çš„çŒœæµ‹ï¼ˆå¦‚æžœè¿˜ä¸ç¡®å®šå¯ä»¥è¯´'è¿˜éœ€è¦æ›´å¤šä¿¡æ¯'ï¼‰ï¼š"""

        client, model_id, is_api = self._get_client_for_model(model)
        if is_api:
            guess_result = client.generate_response(guess_prompt, max_tokens=300, temperature=self.config.temperature)
        else:
            guess_result = client.generate_response(model_id, guess_prompt, max_tokens=300,
                                                  temperature=self.config.temperature, timeout=self.config.timeout,
                                                  streaming=self.config.streaming_output)
        if guess_result.get("success"):
            guess = guess_result.get("response", "").strip()
            print(f"\nðŸ¤” {role} çŒœæµ‹ï¼š{guess}")
            return guess
        return None

    @staticmethod
    def _confirm_guess(guess: str) -> bool:
        """ç¡®è®¤çŒœæµ‹"""
        _ = guess  # æ ‡è®°å‚æ•°å·²çŸ¥ä½†æœªä½¿ç”¨
        confirm = input("çŒœå¯¹äº†å—ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰: ").strip().lower()
        return confirm == "æ˜¯"

    def _finalize_turtle_soup(self, question: str, history: List[str]):
        """å®Œæˆæµ·é¾Ÿæ±¤æ¸¸æˆ"""
        DisplayManager.print_header("ðŸ“ æµ·é¾Ÿæ±¤æ¸¸æˆç»“æŸ")

        if history:
            final_prompt = f"""åŸºäºŽä»¥ä¸‹æµ·é¾Ÿæ±¤æ¸¸æˆè®°å½•ï¼Œè¯·æ€»ç»“ï¼š
è°œé¢ï¼š{question}
åŽ†å²è®°å½•ï¼š
{"\n".join(history)}
è¯·ç»™å‡ºæœ€ç»ˆåˆ†æžå’Œè°œåº•è§£é‡Šï¼š"""

            coord_client, coord_model, is_api = self._get_client_for_model(self.config.coordinator_model)
            if is_api:
                final_result = coord_client.generate_response(final_prompt, max_tokens=500, temperature=self.config.temperature)
            else:
                final_result = coord_client.generate_response(coord_model, final_prompt, max_tokens=500,
                                                            temperature=self.config.temperature, timeout=self.config.timeout,
                                                            streaming=False)
            if final_result.get("success"):
                summary = final_result.get("response", "")
                print(f"\nðŸ“‹ æœ€ç»ˆæ€»ç»“ï¼š")
                print(summary[:self.config.display_length] +
                      ("..." if len(summary) > self.config.display_length else ""))

    # ==================== ã€è¾…åŠ©æ–¹æ³•ã€‘ ====================
    def _display_results(self, results: List[Dict[str, Any]]):
        """æ˜¾ç¤ºç»“æžœ"""
        DisplayManager.print_separator("-", 80)
        print("ðŸ“Š å›žç­”ç»“æžœï¼š")
        DisplayManager.print_separator("-", 80)

        for result in results:
            DisplayManager.print_result(result, self.config.display_length)

    @staticmethod
    def _build_debate_context(debate_round: List[Dict], display_name1: str, display_name2: str) -> str:
        """æž„å»ºè¾©è®ºåŽ†å²ä¸Šä¸‹æ–‡
        
        Args:
            debate_round: è¾©è®ºè½®æ¬¡è®°å½•
            display_name1: ç¬¬ä¸€ä¸ªè¾©è®ºè€…æ˜¾ç¤ºåç§°ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºŽæœªæ¥æ‰©å±•ï¼‰
            display_name2: ç¬¬äºŒä¸ªè¾©è®ºè€…æ˜¾ç¤ºåç§°ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºŽæœªæ¥æ‰©å±•ï¼‰
        """
        _ = (display_name1, display_name2)  # æ ‡è®°å‚æ•°å·²çŸ¥ä½†æœªä½¿ç”¨ï¼ˆä¸ºæœªæ¥æ‰©å±•ä¿ç•™ï¼‰
        context_parts = ["ã€è¾©è®ºåŽ†å²ã€‘"]

        for entry in debate_round[-4:]:  # åªæ˜¾ç¤ºæœ€è¿‘4æ¡å‘è¨€ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
            speaker = entry["speaker"]
            content = entry["content"][:500]  # é™åˆ¶æ¯ä¸ªå‘è¨€çš„é•¿åº¦
            round_num = entry["round"]

            context_parts.append(f"ç¬¬{round_num}å›žåˆ - {speaker}ï¼š")
            context_parts.append(f"  {content}")
            context_parts.append("")

        return "\n".join(context_parts).strip()

    def _display_debate_response(self, speaker: str, content: str, response_type: str = ""):
        """æ˜¾ç¤ºè¾©è®ºå“åº”"""
        type_prefix = f" {response_type}ï¼š" if response_type else "ï¼š"
        print(f"\nðŸ“¢ {speaker}{type_prefix}")
        print(content[:self.config.display_length] +
              ("..." if len(content) > self.config.display_length else ""))

    def _save_history_entry(self, question: str, results: List[Dict[str, Any]], mode: str):
        """ä¿å­˜åŽ†å²è®°å½•"""
        entry = {
            "session_id": self.session_id,
            "type": mode,
            "question": question,
            "results": results
        }
        self.history_manager.add_entry(entry)

    def _save_debate_entry(self, question: str, debate_round: List[Dict[str, Any]],
                          role1: str, role2: str):
        """ä¿å­˜è¾©è®ºåŽ†å²"""
        entry = {
            "session_id": self.session_id,
            "type": "debate",
            "question": question,
            "roles": [role1, role2],
            "debate_rounds": debate_round
        }
        self.history_manager.add_entry(entry)

    def _setup_debate_roles(self, question: str, role1: str, role2: str):
        """è®¾ç½®è¾©è®ºè§’è‰²å¹¶æ‰§è¡Œæ ‡ç­¾æ£€æµ‹

        ä¸ºè¾©è®ºåŒæ–¹é…ç½®åˆé€‚çš„è§’è‰²ï¼Œå¹¶æ ¹æ®é—®é¢˜å†…å®¹æ£€æµ‹ç›¸å…³æ ‡ç­¾ï¼š
        1. åº”ç”¨è§’è‰²çš„ç«‹åœºåå¥½ï¼ˆå°¤å…¶æ˜¯è¾©è®ºæ‰‹çš„æ­£åæ–¹æœºåˆ¶ï¼‰
        2. åˆ†æžé—®é¢˜å†…å®¹ï¼Œæ£€æµ‹ç›¸å…³çš„ä¸“ä¸šé¢†åŸŸæ ‡ç­¾
        3. æ˜¾ç¤ºæ£€æµ‹åˆ°çš„æ ‡ç­¾ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£AIçš„ä¸“ä¸šèƒŒæ™¯

        è¿™ç¡®ä¿äº†è¾©è®ºåŒæ–¹èƒ½å¤Ÿä»Žåˆé€‚çš„ä¸“ä¸šè§’åº¦å’Œç«‹åœºå‚ä¸Žè®¨è®º
        """
        # æ£€æµ‹æ ‡ç­¾
        if self.config.enable_tags:
            tags = role_system.detect_tags(question)
            if tags:
                logger.info(f"ðŸ·ï¸  æ£€æµ‹åˆ°æ ‡ç­¾: {', '.join(tags)}")
                print(f"ðŸ·ï¸  æ£€æµ‹åˆ°æ ‡ç­¾: {', '.join(tags)}")

        print(f"ðŸŽ­ è¾©è®ºè§’è‰²: {role1} vs {role2}")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.config.save_history:
            self.history_manager.save_history()
        logger.info("ðŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

# ==================== ã€ç”¨æˆ·äº¤äº’ç•Œé¢ã€‘ ====================
# å‘½ä»¤è¡Œç”¨æˆ·ç•Œé¢ï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥å’Œç³»ç»Ÿè¾“å‡º

class InteractiveInterface:
    """MACPå‘½ä»¤è¡Œäº¤äº’ç•Œé¢

    æä¾›ç”¨æˆ·å‹å¥½çš„å‘½ä»¤è¡Œç•Œé¢ï¼š
    - å‘½ä»¤è§£æžå’Œæ‰§è¡Œ
    - èœå•æ˜¾ç¤ºå’Œå¯¼èˆª
    - ç”¨æˆ·è¾“å…¥éªŒè¯
    - ç»“æžœæ ¼å¼åŒ–è¾“å‡º

    æ”¯æŒçš„ä¸»è¦å‘½ä»¤ï¼š
    - ç›´æŽ¥æé—®ï¼ˆå¹¶è¡Œæ¨¡å¼ï¼‰
    - /debateï¼ˆè¾©è®ºæ¨¡å¼ï¼‰
    - /turtleï¼ˆæµ·é¾Ÿæ±¤æ¨¡å¼ï¼‰
    - /consensusï¼ˆå…±è¯†é…ç½®ï¼‰
    - /helpï¼ˆå¸®åŠ©ä¿¡æ¯ï¼‰
    """

    def __init__(self, scheduler: AICouncilScheduler):
        self.scheduler = scheduler

    def run(self):
        """è¿è¡Œäº¤äº’ç•Œé¢"""
        self._print_welcome()
        self._print_commands()

        while True:
            try:
                user_input = input(f"\nè¯·è¾“å…¥é—®é¢˜æˆ–å‘½ä»¤ï¼š").strip()

                if not user_input:
                    continue

                if user_input.startswith('/'):
                    self._handle_command(user_input[1:])
                else:
                    self._handle_question(user_input)

            except KeyboardInterrupt:
                self._handle_interrupt()
            except Exception as e:
                logger.error(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
                print(f"âŒ å‘ç”Ÿé”™è¯¯ï¼š{e}")

    @staticmethod
    def _print_welcome():
        """æ‰“å°æ¬¢è¿Žä¿¡æ¯"""
        DisplayManager.print_header("ðŸ¤– MACP å¤šAIåä½œå¹³å° v5.0")

        print(f"æ¨¡åž‹1ï¼š{config.model_1}")
        print(f"æ¨¡åž‹2ï¼š{config.model_2}")
        print(f"åè°ƒæ¨¡åž‹ï¼š{config.coordinator_model}")
        print(f"ä¼˜åŒ–æ¨¡å¼ï¼š{'å¼€å¯' if config.optimize_memory else 'å…³é—­'}")
        DisplayManager.print_separator()

    @staticmethod
    def _print_commands():
        """æ‰“å°å¯ç”¨å‘½ä»¤"""
        print("\nðŸ“‹ å¯ç”¨å‘½ä»¤ï¼š")
        commands = [
            ("help", "æ˜¾ç¤ºå¸®åŠ©"),
            ("models", "æŸ¥çœ‹å¯ç”¨æ¨¡åž‹"),
            ("config", "æŸ¥çœ‹å½“å‰é…ç½®"),
            ("history", "æŸ¥çœ‹åŽ†å²è®°å½•"),
            ("api", "é…ç½®APIæ¨¡å¼"),
            ("debate", "è¿›å…¥è¾©è®ºæ¨¡å¼"),
            ("turtle", "è¿›å…¥æµ·é¾Ÿæ±¤æ¨¡å¼"),
            ("consensus", "é…ç½®å…±è¯†æ£€æµ‹"),
            ("streaming", "åˆ‡æ¢æµå¼è¾“å‡ºæ¨¡å¼"),
            ("optimize", "å¼€å¯ä¼˜åŒ–æ¨¡å¼"),
            ("roles", "æŸ¥çœ‹å¯ç”¨è§’è‰²"),
            ("tags", "æŸ¥çœ‹æ ‡ç­¾ç³»ç»Ÿ"),
            ("mode", "åˆ‡æ¢åè°ƒæ¨¡å¼ï¼ˆauto/userï¼‰"),
            ("clear", "æ¸…å±"),
            ("exit", "é€€å‡ºç¨‹åº")
        ]

        for cmd, desc in commands:
            print(f"  /{cmd:<12} - {desc}")
        DisplayManager.print_separator()

    def _handle_question(self, question: str):
        """å¤„ç†é—®é¢˜è¾“å…¥"""
        print(f"\nðŸ” æ­£åœ¨å¤„ç†é—®é¢˜...")
        self.scheduler.progress_tracker.start()

        try:
            self.scheduler.ask_both_models(question, mode="parallel")
            total_time = self.scheduler.progress_tracker.get_elapsed_time()
            print(f"\nâœ… æ€»è€—æ—¶ï¼š{total_time:.2f}ç§’")
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜å¤±è´¥: {e}")
            print(f"âŒ å¤„ç†é—®é¢˜å¤±è´¥: {e}")

    def _handle_command(self, command: str):
        """å¤„ç†å‘½ä»¤"""
        command = command.lower()

        handlers = {
            'help': self._print_commands,
            'models': self._show_models,
            'config': self._show_config,
            'history': self._show_history,
            'api': self._configure_api_mode,
            'debate': self._enter_debate_mode,
            'turtle': self._enter_turtle_soup_mode,
            'consensus': self._configure_consensus,
            'optimize': self._toggle_optimize_mode,
            'roles': self._show_roles,
            'tags': self._show_tags,
            'mode': self._toggle_coordination_mode,
            'streaming': self._toggle_streaming_mode,
            'clear': DisplayManager.clear_screen,
            'exit': self._exit_program
        }

        handler = handlers.get(command)
        if handler:
            try:
                handler()
            except Exception as e:
                logger.error(f"æ‰§è¡Œå‘½ä»¤ /{command} å¤±è´¥: {e}")
                print(f"âŒ æ‰§è¡Œå‘½ä»¤å¤±è´¥: {e}")
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤ï¼š/{command}")

    def _show_models(self):
        """æ˜¾ç¤ºå¯ç”¨æ¨¡åž‹"""
        print("\nðŸ“¦ æ£€æŸ¥å¯ç”¨æ¨¡åž‹...")
        models = self.scheduler.client.list_models()
        print(DisplayManager.format_model_list(models))

    @staticmethod
    def _show_config():
        """æ˜¾ç¤ºå½“å‰é…ç½®"""
        config_dict = config.to_dict()
        print(DisplayManager.format_config_display(config_dict))

    def _show_history(self):
        """æ˜¾ç¤ºåŽ†å²è®°å½•"""
        print(f"\nðŸ“œ åŽ†å²è®°å½•ï¼ˆä¼šè¯IDï¼š{self.scheduler.session_id}ï¼‰ï¼š")
        history = self.scheduler.history_manager.get_recent_history(5)

        if history:
            for i, entry in enumerate(history, 1):
                timestamp = entry.get('timestamp', '')[:16]
                entry_type = entry.get('type', 'unknown')
                question = entry.get('question', '')[:60]
                print(f"\n  [{i}] {timestamp} - {entry_type}")
                print(f"      é—®é¢˜ï¼š{question}...")
        else:
            print("  æš‚æ— åŽ†å²è®°å½•")

    def _enter_debate_mode(self):
        """è¿›å…¥è¾©è®ºæ¨¡å¼"""
        DisplayManager.print_header("ðŸ’¬ è¾©è®ºæ¨¡å¼")

        # é€‰æ‹©åè°ƒæ¨¡å¼
        print("\né€‰æ‹©åè°ƒæ¨¡å¼ï¼š")
        print("  1. AIè‡ªåŠ¨åè°ƒï¼ˆé»˜è®¤ï¼‰")
        print("  2. ç”¨æˆ·æ‰‹åŠ¨åè°ƒ")

        mode_choice = input("é€‰æ‹©ï¼ˆ1/2ï¼‰: ").strip()
        if mode_choice == "2":
            config.coordination_mode = "user"
            print("âœ… å·²é€‰æ‹©ç”¨æˆ·åè°ƒæ¨¡å¼")
        else:
            config.coordination_mode = "auto"
            print("âœ… å·²é€‰æ‹©AIè‡ªåŠ¨åè°ƒæ¨¡å¼")

        # è¾“å…¥é—®é¢˜
        question = input("\nè¯·è¾“å…¥è¾©è®ºé—®é¢˜ï¼š").strip()
        if not question:
            print("âŒ é—®é¢˜ä¸èƒ½ä¸ºç©º")
            return

        # é€‰æ‹©è§’è‰²
        role1, role2 = self._select_debate_roles()

        # å›žåˆæ•°
        rounds_input = input(f"\nè¾©è®ºå›žåˆæ•°ï¼ˆé»˜è®¤:{config.debate_rounds}ï¼‰: ").strip()
        if rounds_input.isdigit():
            config.debate_rounds = int(rounds_input)

        # å¼€å§‹è¾©è®º
        print(f"\nðŸŽ¬ å¼€å§‹è¾©è®ºï¼š{role1} vs {role2}")
        print(f"é—®é¢˜ï¼š{question}")
        DisplayManager.print_separator()

        self.scheduler.progress_tracker.start()
        try:
            self.scheduler.ask_both_models(question, mode="debate", role1=role1, role2=role2)
            total_time = self.scheduler.progress_tracker.get_elapsed_time()
            print(f"\nâœ… è¾©è®ºå®Œæˆ | æ€»è€—æ—¶ï¼š{total_time:.2f}ç§’")
        except Exception as e:
            logger.error(f"è¾©è®ºå¤±è´¥: {e}")
            print(f"âŒ è¾©è®ºå¤±è´¥: {e}")

    def _enter_turtle_soup_mode(self):
        """è¿›å…¥æµ·é¾Ÿæ±¤æ¨¡å¼"""
        DisplayManager.print_header("ðŸ¢ æµ·é¾Ÿæ±¤æ¨¡å¼")

        question = input("\nè¯·è¾“å…¥æµ·é¾Ÿæ±¤è°œé¢ï¼š").strip()
        if not question:
            print("âŒ è°œé¢ä¸èƒ½ä¸ºç©º")
            return

        role1 = input("\nAI1è§’è‰²ï¼ˆé»˜è®¤ï¼šä¾¦æŽ¢ï¼‰: ").strip() or "ä¾¦æŽ¢"
        role2 = input("AI2è§’è‰²ï¼ˆé»˜è®¤ï¼šæŽ¨ç†è€…ï¼‰: ").strip() or "æŽ¨ç†è€…"

        print(f"\nðŸŽ® å¼€å§‹æµ·é¾Ÿæ±¤æ¸¸æˆ")
        print(f"è°œé¢ï¼š{question}")
        print(f"AIè§’è‰²ï¼š{role1} å’Œ {role2}")
        DisplayManager.print_separator()

        try:
            self.scheduler.ask_both_models(question, mode="turtle_soup", role1=role1, role2=role2)
        except Exception as e:
            logger.error(f"æµ·é¾Ÿæ±¤æ¸¸æˆå¤±è´¥: {e}")
            print(f"âŒ æµ·é¾Ÿæ±¤æ¸¸æˆå¤±è´¥: {e}")

    @staticmethod
    def _toggle_optimize_mode():
        """åˆ‡æ¢ä¼˜åŒ–æ¨¡å¼"""
        config.optimize_memory = not config.optimize_memory
        status = "å¼€å¯" if config.optimize_memory else "å…³é—­"
        print(f"âœ… ä¼˜åŒ–æ¨¡å¼å·²{status}")

    @staticmethod
    def _toggle_streaming_mode():
        """åˆ‡æ¢æµå¼è¾“å‡ºæ¨¡å¼"""
        config.streaming_output = not config.streaming_output
        status = "å¼€å¯" if config.streaming_output else "å…³é—­"
        mode_desc = "AIå›žç­”å°†é€å­—å®žæ—¶æ˜¾ç¤º" if config.streaming_output else "AIå›žç­”å°†ä¸€æ¬¡æ€§æ˜¾ç¤º"
        print(f"âœ… æµå¼è¾“å‡ºå·²{status}")
        print(f"   {mode_desc}")

    @staticmethod
    def _show_roles():
        """æ˜¾ç¤ºå¯ç”¨è§’è‰²"""
        print("\nðŸŽ­ å¯ç”¨è§’è‰²ï¼ˆæ”¯æŒè¾“å…¥æ•°å­—é€‰æ‹©ï¼‰ï¼š")
        roles = role_system.get_all_roles()
        for i, role in enumerate(roles, 1):
            print(f"  {i}. {role}")

    @staticmethod
    def _show_tags():
        """æ˜¾ç¤ºæ ‡ç­¾ç³»ç»Ÿ"""
        print("\nðŸ·ï¸  æ ‡ç­¾ç³»ç»Ÿï¼š")
        for tag, roles in TAG_TO_ROLES.items():
            print(f"  {tag}: {', '.join(roles)}")

    @staticmethod
    def _configure_consensus():
        """é…ç½®å…±è¯†æ£€æµ‹"""
        print("\nðŸŽ¯ å…±è¯†æ£€æµ‹é…ç½®")
        print(f"å½“å‰è®¾ç½®ï¼š")
        print(f"  - AIå…±è¯†åˆ†æž: {'å¼€å¯' if config.ai_consensus_analysis else 'å…³é—­'}")
        print(f"  - è‡ªåŠ¨æ€»ç»“: {'å¼€å¯' if config.auto_summarize_at_threshold else 'å…³é—­'}")
        print(f"  - å…±è¯†é˜ˆå€¼: {int(config.consensus_threshold * 100)}%")
        print(f"  - æ£€æµ‹èµ·å§‹å›žåˆ: ç¬¬{config.consensus_check_start_round}å›žåˆ")

        print(f"\né€‰é¡¹ï¼š")
        print(f"  1. åˆ‡æ¢AIå…±è¯†åˆ†æž (å½“å‰: {'å¼€' if config.ai_consensus_analysis else 'å…³'})")
        print(f"  2. åˆ‡æ¢è‡ªåŠ¨æ€»ç»“ (å½“å‰: {'å¼€' if config.auto_summarize_at_threshold else 'å…³'})")
        print(f"  3. è®¾ç½®å…±è¯†é˜ˆå€¼ (å½“å‰: {int(config.consensus_threshold * 100)}%)")
        print(f"  4. è®¾ç½®æ£€æµ‹èµ·å§‹å›žåˆ (å½“å‰: {config.consensus_check_start_round})")

        choice = input("é€‰æ‹© (1-4) æˆ–å›žè½¦è¿”å›ž: ").strip()

        if choice == '1':
            config.ai_consensus_analysis = not config.ai_consensus_analysis
            status = "å¼€å¯" if config.ai_consensus_analysis else "å…³é—­"
            print(f"âœ… AIå…±è¯†åˆ†æžå·²{status}")
        elif choice == '2':
            config.auto_summarize_at_threshold = not config.auto_summarize_at_threshold
            status = "å¼€å¯" if config.auto_summarize_at_threshold else "å…³é—­"
            print(f"âœ… è‡ªåŠ¨æ€»ç»“å·²{status}")
        elif choice == '3':
            try:
                threshold = float(input("è¾“å…¥æ–°é˜ˆå€¼ (0-100): ").strip()) / 100.0
                if 0.0 <= threshold <= 1.0:
                    config.consensus_threshold = threshold
                    print(f"âœ… å…±è¯†é˜ˆå€¼å·²è®¾ç½®ä¸º {int(threshold * 100)}%")
                else:
                    print("âŒ é˜ˆå€¼å¿…é¡»åœ¨ 0-100 ä¹‹é—´")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        elif choice == '4':
            try:
                round_num = int(input("è¾“å…¥èµ·å§‹å›žåˆæ•° (1-6): ").strip())
                if 1 <= round_num <= 6:
                    config.consensus_check_start_round = round_num
                    print(f"âœ… æ£€æµ‹èµ·å§‹å›žåˆå·²è®¾ç½®ä¸ºç¬¬{round_num}å›žåˆ")
                else:
                    print("âŒ å›žåˆæ•°å¿…é¡»åœ¨ 1-6 ä¹‹é—´")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        elif choice == '':
            return
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

    @staticmethod
    def _toggle_coordination_mode():
        """åˆ‡æ¢åè°ƒæ¨¡å¼"""
        current = config.coordination_mode
        new_mode = "user" if current == "auto" else "auto"
        config.coordination_mode = new_mode
        print(f"âœ… åè°ƒæ¨¡å¼å·²åˆ‡æ¢ï¼š{current} -> {new_mode}")

    def _handle_interrupt(self):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        print("\n\nâš ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·")
        choice = input("æ˜¯å¦é€€å‡ºç¨‹åºï¼Ÿï¼ˆy/Nï¼‰: ").strip().lower()
        if choice == 'y':
            self._exit_program()

    def _configure_api_mode(self):
        """é…ç½®APIæ¨¡å¼"""
        DisplayManager.print_header("ðŸ”— APIæ¨¡å¼é…ç½®")

        print(f"å½“å‰APIæ¨¡å¼çŠ¶æ€ï¼š{'å·²å¯ç”¨' if config.api_mode_enabled else 'æœªå¯ç”¨'}")
        print(f"APIæä¾›æ–¹ï¼š{getattr(config, 'api_provider', 'custom')}")
        print(f"APIåŸºç¡€åœ°å€ï¼š{getattr(config, 'api_base_url', '')}")
        print(f"APIåœ°å€ï¼š{config.api_url}")
        print(f"APIæ¨¡åž‹ï¼š{config.api_model}")
        print(f"APIå¯†é’¥ï¼š{'å·²è®¾ç½®' if config.api_key else 'æœªè®¾ç½®'}")
        print(f"æ¨¡åž‹1ä½¿ç”¨APIï¼š{'æ˜¯' if config.model_1_use_api else 'å¦'}")
        print(f"æ¨¡åž‹2ä½¿ç”¨APIï¼š{'æ˜¯' if config.model_2_use_api else 'å¦'}")
        print(f"åè°ƒAIä½¿ç”¨APIï¼š{'æ˜¯' if config.coordinator_use_api else 'å¦'}")

        DisplayManager.print_separator()

        # è¯¢é—®æ˜¯å¦å¯ç”¨APIæ¨¡å¼
        enable_api = InputValidator.get_yes_no_input("æ˜¯å¦å¯ç”¨APIæ¨¡å¼ï¼Ÿ", default=config.api_mode_enabled)
        if enable_api:
            # é€ä¸ªé…ç½®ï¼šæ¨¡åž‹1ã€æ¨¡åž‹2ã€åè°ƒAI
            any_use_api = False
            targets = [
                ("æ¨¡åž‹1", "model_1"),
                ("æ¨¡åž‹2", "model_2"),
                ("åè°ƒAI", "coordinator"),
            ]

            for label, key in targets:
                print("\n" + "-" * 40)
                print(f"âš™ï¸  é…ç½® {label} çš„APIå‚æ•°")
                use_api_attr = f"{key}_use_api"
                current_use = getattr(config, use_api_attr, False)
                use_api = InputValidator.get_yes_no_input(
                    f"{label} æ˜¯å¦ä½¿ç”¨å¤–éƒ¨APIï¼Ÿï¼ˆå½“å‰: {'æ˜¯' if current_use else 'å¦'}ï¼‰", default=current_use
                )
                setattr(config, use_api_attr, use_api)

                if not use_api:
                    continue

                any_use_api = True

                # é€‰æ‹©æä¾›æ–¹
                provider_attr = f"{key}_api_provider"
                base_attr = f"{key}_api_base_url"
                url_attr = f"{key}_api_url"
                key_attr = f"{key}_api_key"
                model_attr = f"{key}_api_model"

                current_provider = getattr(config, provider_attr, "") or "custom"
                print(f"\nðŸ¢ ä¸º {label} é€‰æ‹©APIæä¾›æ–¹ï¼ˆå½“å‰: {current_provider}ï¼‰ï¼š")
                print("  1. ç¡…åŸºæµåŠ¨ (SiliconFlow)")
                print("  2. DeepSeek")
                print("  3. ç«å±±å¼•æ“Ž (Volcengine Ark)")
                print("  4. è‡ªå®šä¹‰ (å…¼å®¹OpenAIæ ¼å¼)")
                provider_choice = input("è¾“å…¥ç¼–å·(1-4ï¼Œå›žè½¦ä¿æŒå½“å‰/è‡ªå®šä¹‰): ").strip() or "4"

                provider_map = {
                    "1": ("siliconflow", "https://api.siliconflow.cn/v1"),
                    "2": ("deepseek", "https://api.deepseek.com/v1"),
                    "3": ("volcengine", "https://ark.cn-beijing.volces.com/api/v3"),
                    "4": (current_provider or "custom", getattr(config, base_attr, "") or getattr(config, "api_base_url", "https://api.openai.com/v1") or "https://api.openai.com/v1"),
                }
                provider, default_base = provider_map.get(provider_choice, provider_map["4"])
                setattr(config, provider_attr, provider)

                # é…ç½®åŸºç¡€åœ°å€
                current_base = getattr(config, base_attr, "") or default_base
                print("\nðŸ”§ é…ç½®APIåŸºç¡€åœ°å€ï¼š")
                base_url = input(f"{label} APIåŸºç¡€åœ°å€ (å½“å‰: {current_base}): ").strip()
                if not base_url:
                    base_url = current_base
                base_url = base_url.rstrip("/")
                setattr(config, base_attr, base_url)

                # chat completions endpoint
                default_chat_url = f"{base_url}/chat/completions"
                current_chat = getattr(config, url_attr, "") or default_chat_url
                api_url = input(f"{label} ChatCompletionsåœ°å€ (å½“å‰: {current_chat}): ").strip()
                api_url = (api_url or current_chat).rstrip("/")
                setattr(config, url_attr, api_url)

                # API Keyï¼šä¼˜å…ˆå·²æœ‰å•ç‹¬keyï¼Œå…¶æ¬¡å…¨å±€api_key
                existing_key = getattr(config, key_attr, "") or config.api_key
                api_key_input = input(f"{label} APIå¯†é’¥ (å½“å‰: {'å·²è®¾ç½®' if existing_key else 'æœªè®¾ç½®'}ï¼Œç•™ç©ºä¿æŒä¸å˜): ").strip()
                if api_key_input:
                    setattr(config, key_attr, api_key_input)
                    existing_key = api_key_input

                # å…ˆå°è¯•æ‹‰å–è¯¥æä¾›æ–¹çš„æ¨¡åž‹åˆ—è¡¨
                models: List[str] = []
                if existing_key:
                    temp_client = APIClient(api_url=api_url, api_key=existing_key,
                                            model_name=getattr(config, model_attr, "") or config.api_model,
                                            timeout=config.timeout)
                    models = temp_client.list_models()

                current_model = getattr(config, model_attr, "") or config.api_model
                if models:
                    print("\nðŸ“¦ èŽ·å–åˆ°å¯ç”¨æ¨¡åž‹ï¼š")
                    for i, mid in enumerate(models, 1):
                        print(f"  {i}. {mid}")
                    model_choice = input(f"{label} é€‰æ‹©æ¨¡åž‹ç¼–å·(1-{len(models)})ï¼Œæˆ–ç›´æŽ¥è¾“å…¥æ¨¡åž‹å(å›žè½¦ä¿ç•™å½“å‰ {current_model}): ").strip()
                    if model_choice.isdigit():
                        idx = int(model_choice)
                        if 1 <= idx <= len(models):
                            setattr(config, model_attr, models[idx - 1])
                    elif model_choice:
                        setattr(config, model_attr, model_choice)
                else:
                    print(f"\nâš ï¸  æ— æ³•è‡ªåŠ¨èŽ·å– {label} çš„æ¨¡åž‹åˆ—è¡¨ï¼ˆè¯¥å¹³å°å¯èƒ½ä¸æ”¯æŒ /modelsï¼Œæˆ–Key/ç½‘ç»œé—®é¢˜ï¼‰ã€‚")
                    api_model_input = input(f"è¯·è¾“å…¥ {label} ä½¿ç”¨çš„æ¨¡åž‹åç§° (å½“å‰: {current_model}): ").strip()
                    if api_model_input:
                        setattr(config, model_attr, api_model_input)

            # è‹¥è‡³å°‘æœ‰ä¸€ä¸ªAIä½¿ç”¨APIï¼Œåˆ™è®¤ä¸ºAPIæ¨¡å¼å¼€å¯
            config.api_mode_enabled = any_use_api
            if not any_use_api:
                print("âš ï¸  æ‰€æœ‰AIéƒ½æœªé…ç½®ä½¿ç”¨APIï¼Œå°†å…³é—­APIæ¨¡å¼ï¼Œä»…ä½¿ç”¨æœ¬åœ°Ollamaã€‚")

            # ä¿å­˜é…ç½®
            config.save_to_file("macp_config.json")
            print("âœ… APIé…ç½®å·²ä¿å­˜")

            # é‡æ–°åˆå§‹åŒ–è°ƒåº¦å™¨ä»¥åº”ç”¨æ–°é…ç½®
            print("\nðŸ”„ æ­£åœ¨é‡æ–°åˆå§‹åŒ–ç³»ç»Ÿ...")
            try:
                # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨å®žä¾‹
                new_scheduler = AICouncilScheduler()
                self.scheduler = new_scheduler
                print("âœ… ç³»ç»Ÿé‡æ–°åˆå§‹åŒ–å®Œæˆ")
            except (AICouncilException, requests.exceptions.RequestException, ValueError) as e:
                print(f"âŒ é‡æ–°åˆå§‹åŒ–å¤±è´¥: {e}")

        else:
            config.api_mode_enabled = False
            print("âœ… å·²ç¦ç”¨APIæ¨¡å¼")

        DisplayManager.print_separator()

    def _exit_program(self):
        """é€€å‡ºç¨‹åº"""
        print(f"\nðŸ“Š ä¼šè¯ç»Ÿè®¡ï¼š")
        print(f"  ä¼šè¯IDï¼š{self.scheduler.session_id}")
        print(f"  æ€»è®°å½•æ•°ï¼š{len(self.scheduler.history_manager.history)}")
        print("\nðŸ‘‹ å†è§ï¼")

        # æ¸…ç†èµ„æº
        self.scheduler.cleanup()

        sys.exit(0)

    def _select_debate_roles(self) -> Tuple[str, str]:
        """é€‰æ‹©è¾©è®ºè§’è‰²"""
        self._show_roles()
        role1_input = input(f"\næ¨¡åž‹1è§’è‰²ï¼ˆé»˜è®¤ï¼š{config.default_role_1}ï¼‰: ").strip()
        role2_input = input(f"æ¨¡åž‹2è§’è‰²ï¼ˆé»˜è®¤ï¼š{config.default_role_2}ï¼‰: ").strip()

        role1 = InputValidator.validate_role_input(role1_input, role_system.get_all_roles())
        role2 = InputValidator.validate_role_input(role2_input, role_system.get_all_roles())
        return role1, role2

    @staticmethod
    def _handle_consensus_feedback(consensus_score: float, consensus_percentage: int,
                                 threshold_percentage: int, current_consensus_reached: bool) -> bool:
        """å¤„ç†AIå…±è¯†åˆ†æžç»“æžœå¹¶å†³å®šè¾©è®ºè¿›ç¨‹

        æ ¹æ®AIçš„å…±è¯†åˆ†æžç»“æžœå‘ç”¨æˆ·å±•ç¤ºå½“å‰è¾©è®ºçŠ¶æ€ï¼š
        1. æ˜¾ç¤ºå…±è¯†åº¦æ¡å½¢å›¾ï¼ˆè§†è§‰åŒ–è¡¨ç¤ºï¼‰
        2. æ ¹æ®å…±è¯†åº¦ç»™å‡ºä¸åŒçš„è¿›åº¦åé¦ˆ
        3. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è‡ªåŠ¨ç»“æŸé˜ˆå€¼ï¼ˆ70%ï¼‰
        4. è¿”å›žæ˜¯å¦åº”è¯¥ç»“æŸè¾©è®ºçš„å†³ç­–

        è¿™æ˜¯å®žçŽ°"æ™ºèƒ½è¾©è®ºç»“æŸ"çš„å…³é”®çŽ¯èŠ‚ï¼Œ
        è®©ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®AIçš„è¯­ä¹‰ç†è§£åšå‡ºåˆç†å†³ç­–
        """
        if consensus_score >= config.consensus_threshold:
            logger.info(f"âœ… å…±è¯†åº¦è¾¾æ ‡ ({consensus_percentage}%)ï¼Œè‡ªåŠ¨ç»“æŸè¾©è®º")
            print(f"\nðŸŽ¯ å…±è¯†åº¦å·²è¾¾åˆ°{consensus_percentage}%ï¼ˆâ‰¥{threshold_percentage}%é˜ˆå€¼ï¼‰ï¼Œè‡ªåŠ¨ç»“æŸè¾©è®ºå¹¶ç”Ÿæˆæ€»ç»“")
            return True
        elif consensus_score >= 0.5:
            remaining_to_threshold = threshold_percentage - consensus_percentage
            print(f"ðŸ“ˆ å…±è¯†åº¦{consensus_percentage}%ï¼Œè·ç¦»é˜ˆå€¼è¿˜å·®{remaining_to_threshold}%ï¼Œè¾©è®ºç»§ç»­...")
        else:
            print(f"âš–ï¸  å…±è¯†åº¦{consensus_percentage}%ï¼Œåˆ†æ­§æ˜Žæ˜¾ï¼Œç»§ç»­æ·±å…¥è¾©è®º...")

        return current_consensus_reached

# ==================== ã€ä¸»å‡½æ•°ã€‘ ====================
def main():
    """MACPç³»ç»Ÿä¸»å…¥å£å‡½æ•°

    æ‰§è¡Œå®Œæ•´çš„ç³»ç»Ÿåˆå§‹åŒ–å’Œè¿è¡Œæµç¨‹ï¼š
    1. æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯å’Œç‰ˆæœ¬å·
    2. åˆå§‹åŒ–AICouncilSchedulerï¼ˆæ ¸å¿ƒè°ƒåº¦å™¨ï¼‰
    3. åˆ›å»ºInteractiveInterfaceï¼ˆç”¨æˆ·ç•Œé¢ï¼‰
    4. å¯åŠ¨äº¤äº’å¼å‘½ä»¤å¾ªçŽ¯
    5. å¤„ç†ç³»ç»Ÿå¼‚å¸¸å’Œæ¸…ç†èµ„æº

    è¿™æ˜¯æ•´ä¸ªåº”ç”¨ç¨‹åºçš„å¯åŠ¨ç‚¹ï¼Œè´Ÿè´£åè°ƒæ‰€æœ‰ç»„ä»¶çš„åˆå§‹åŒ–
    """
    print("ðŸš€ å¯åŠ¨MACPå¤šAIåä½œå¹³å° v5.0 - ç»ˆæžä¼˜åŒ–ç‰ˆ")
    print("=" * 80)
    print("æ–°å¢žåŠŸèƒ½ï¼š")
    print("1. âœ… æ¨¡å—åŒ–æž¶æž„ - ä»£ç æ›´æ¸…æ™°")
    print("2. âœ… å¢žå¼ºé”™è¯¯å¤„ç† - æ›´ç¨³å®š")
    print("3. âœ… æ—¥å¿—ç³»ç»Ÿ - ä¾¿äºŽè°ƒè¯•")
    print("4. âœ… æ€§èƒ½ç›‘æŽ§ - å®žæ—¶è·Ÿè¸ª")
    print("5. âœ… é…ç½®ç®¡ç† - åŠ¨æ€åŠ è½½")
    print("6. âœ… ä»£ç ä¼˜åŒ– - å‡å°‘å†—ä½™")
    print("7. âœ… ç±»åž‹æ³¨è§£ - æ›´å¥½çš„ç»´æŠ¤æ€§")
    print("=" * 80)

    try:
        # åˆå§‹åŒ–è°ƒåº¦å™¨
        scheduler = AICouncilScheduler()

        # å¯åŠ¨äº¤äº’ç•Œé¢
        interface = InteractiveInterface(scheduler)
        interface.run()

    except KeyboardInterrupt:
        print("\n\nðŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=e)
        print(f"\nâŒ ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ä»¥èŽ·å–è¯¦ç»†ä¿¡æ¯")
        sys.exit(1)

if __name__ == "__main__":
    main()
