"""
===============================================================================
å¤šAIåä½œè°ƒåº¦å™¨ v5.0 - ç»ˆæä¼˜åŒ–ç‰ˆ (å•æ–‡ä»¶ç‰ˆæœ¬)
MACP: Multi-Agent Collaboration Platform (å¤šAIåä½œå¹³å°)
===============================================================================

æ ¸å¿ƒåŠŸèƒ½ï¼š
â”œâ”€â”€ ğŸ¤– AIè¾©è®ºç³»ç»Ÿ - æ”¯æŒ9ç§ä¸“ä¸šè§’è‰²ï¼Œå¤šå›åˆæ™ºèƒ½è¾©è®º
â”œâ”€â”€ ğŸ¯ å…±è¯†åº¦æ£€æµ‹ - AIæ·±åº¦åˆ†æï¼Œå®æ—¶ç›‘æ§è¾©è®ºå…±è¯†
â”œâ”€â”€ ğŸ¢ æµ·é¾Ÿæ±¤æ¸¸æˆ - AIæ¨ç†é—®ç­”äº’åŠ¨æ¨¡å¼
â”œâ”€â”€ ğŸ“Š å¹¶è¡Œæé—® - åŒæ—¶å‘å¤šä¸ªAIæ¨¡å‹æé—®
â”œâ”€â”€ ğŸ”„ æ™ºèƒ½ç»“æŸ - å…±è¯†åº¦è¾¾70%è‡ªåŠ¨æ€»ç»“
â””â”€â”€ ğŸ“ å†å²è®°å½• - å®Œæ•´çš„å¯¹è¯å’Œè¾©è®ºä¿å­˜

æŠ€æœ¯ç‰¹æ€§ï¼š
â”œâ”€â”€ ğŸ—ï¸ æ¨¡å—åŒ–æ¶æ„ - å•æ–‡ä»¶æ•´åˆï¼Œæ˜“äºéƒ¨ç½²
â”œâ”€â”€ ğŸ” ç±»å‹å®‰å…¨ - å®Œæ•´çš„ç±»å‹æ³¨è§£
â”œâ”€â”€ ğŸ“‹ é”™è¯¯å¤„ç† - å®Œå–„çš„å¼‚å¸¸ç®¡ç†å’Œæ—¥å¿—
â”œâ”€â”€ âš¡ æ€§èƒ½ä¼˜åŒ– - å¹¶è¡Œå¤„ç†å’Œæ™ºèƒ½ç¼“å­˜
â””â”€â”€ ğŸ¨ ç”¨æˆ·å‹å¥½ - ç›´è§‚çš„å‘½ä»¤è¡Œç•Œé¢

ä½¿ç”¨ç¯å¢ƒï¼š
â”œâ”€â”€ OllamaæœåŠ¡ - æœ¬åœ°AIæ¨¡å‹è¿è¡Œç¯å¢ƒ
â”œâ”€â”€ Python 3.7+ - è¿è¡Œç¯å¢ƒè¦æ±‚
â””â”€â”€ requestsåº“ - ç½‘ç»œè¯·æ±‚ä¾èµ–

ä½œè€…ï¼šåŒ¿åå¼€å‘è€…
åˆ›å»ºæ—¶é—´ï¼š2026å¹´1æœˆ8æ—¥
ç‰ˆæœ¬ï¼šv5.0
===============================================================================
"""

import concurrent.futures
import json
import time
from datetime import datetime
import os
import sys
import re
import logging
import subprocess
import shutil
from typing import Dict, Any, List, Optional, Tuple

# ==================== ã€å…¨å±€æ ‡å¿—ã€‘ ====================
NEED_API_SETUP = False  # æ ‡è®°æ˜¯å¦éœ€è¦åœ¨å¯åŠ¨åé…ç½®API
CURRENT_LANGUAGE = "zh"  # å½“å‰è¯­è¨€: "zh" ä¸­æ–‡, "en" è‹±æ–‡

# ==================== ã€å¤šè¯­è¨€ç³»ç»Ÿã€‘ ====================
LANG_DICT = {
    # ===== é€šç”¨ =====
    "yes": {"zh": "æ˜¯", "en": "Yes"},
    "no": {"zh": "å¦", "en": "No"},
    "confirm": {"zh": "ç¡®è®¤", "en": "Confirm"},
    "cancel": {"zh": "å–æ¶ˆ", "en": "Cancel"},
    "error": {"zh": "é”™è¯¯", "en": "Error"},
    "warning": {"zh": "è­¦å‘Š", "en": "Warning"},
    "success": {"zh": "æˆåŠŸ", "en": "Success"},
    "failed": {"zh": "å¤±è´¥", "en": "Failed"},
    "loading": {"zh": "åŠ è½½ä¸­", "en": "Loading"},
    "please_wait": {"zh": "è¯·ç¨å€™", "en": "Please wait"},
    "input_prompt": {"zh": "è¯·è¾“å…¥é—®é¢˜æˆ–å‘½ä»¤ï¼š", "en": "Enter question or command: "},
    "invalid_choice": {"zh": "æ— æ•ˆé€‰æ‹©", "en": "Invalid choice"},
    "press_enter": {"zh": "æŒ‰å›è½¦é”®ç»§ç»­", "en": "Press Enter to continue"},
    
    # ===== ä¾èµ–æ£€æŸ¥ =====
    "dep_check_title": {"zh": "ğŸ” MACP ä¾èµ–æ£€æŸ¥ç³»ç»Ÿ", "en": "ğŸ” MACP Dependency Check System"},
    "checking_python": {"zh": "ğŸ“Œ æ£€æŸ¥ Python ç‰ˆæœ¬...", "en": "ğŸ“Œ Checking Python version..."},
    "python_ok": {"zh": "æ»¡è¶³è¦æ±‚", "en": "meets requirements"},
    "python_low": {"zh": "ç‰ˆæœ¬è¿‡ä½", "en": "version too low"},
    "install_python": {"zh": "è¯·å®‰è£… Python 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬", "en": "Please install Python 3.7 or higher"},
    "checking_requests": {"zh": "ğŸ“Œ æ£€æŸ¥ requests åº“...", "en": "ğŸ“Œ Checking requests library..."},
    "requests_installed": {"zh": "requests å·²å®‰è£…", "en": "requests installed"},
    "requests_missing": {"zh": "requests æœªå®‰è£…", "en": "requests not installed"},
    "installing_requests": {"zh": "ğŸ”„ æ­£åœ¨è‡ªåŠ¨å®‰è£… requests...", "en": "ğŸ”„ Auto-installing requests..."},
    "requests_install_ok": {"zh": "requests å®‰è£…æˆåŠŸ", "en": "requests installed successfully"},
    "requests_install_fail": {"zh": "requests å®‰è£…å¤±è´¥", "en": "requests installation failed"},
    "checking_ollama": {"zh": "ğŸ“Œ æ£€æŸ¥ Ollama...", "en": "ğŸ“Œ Checking Ollama..."},
    "ollama_installed": {"zh": "Ollama å·²å®‰è£…", "en": "Ollama installed"},
    "ollama_not_found": {"zh": "Ollama æœªå®‰è£…æˆ–æœªæ‰¾åˆ°", "en": "Ollama not installed or not found"},
    "checking_ollama_service": {"zh": "ğŸ“Œ æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€...", "en": "ğŸ“Œ Checking Ollama service status..."},
    "ollama_running": {"zh": "Ollama æœåŠ¡è¿è¡Œä¸­", "en": "Ollama service running"},
    "ollama_not_running": {"zh": "Ollama æœåŠ¡æœªè¿è¡Œ", "en": "Ollama service not running"},
    "starting_ollama": {"zh": "ğŸ”„ å°è¯•å¯åŠ¨ Ollama æœåŠ¡...", "en": "ğŸ”„ Trying to start Ollama service..."},
    "ollama_started": {"zh": "Ollama æœåŠ¡å·²æˆåŠŸå¯åŠ¨", "en": "Ollama service started successfully"},
    "models_installed": {"zh": "å·²å®‰è£…çš„æ¨¡å‹", "en": "Installed models"},
    "no_models": {"zh": "æš‚æ— å·²å®‰è£…çš„æ¨¡å‹", "en": "No models installed"},
    "all_deps_ok": {"zh": "âœ… æ‰€æœ‰ä¾èµ–æ£€æŸ¥é€šè¿‡ï¼", "en": "âœ… All dependencies check passed!"},
    "deps_missing": {"zh": "âš ï¸ éƒ¨åˆ†ä¾èµ–æœªæ»¡è¶³ï¼Œç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œ", "en": "âš ï¸ Some dependencies missing, program may not work properly"},
    
    # ===== æ¨¡å¼é€‰æ‹© =====
    "select_mode": {"zh": "ğŸ¤” è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š", "en": "ğŸ¤” Please select running mode:"},
    "mode_ollama": {"zh": "ğŸ“¥ ä¸‹è½½ Ollama å¹¶å®‰è£…æœ¬åœ°AIæ¨¡å‹ï¼ˆæ¨èæ–°æ‰‹ï¼‰", "en": "ğŸ“¥ Download Ollama and install local AI models (recommended for beginners)"},
    "mode_ollama_desc1": {"zh": "å®Œå…¨æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ç½‘ç»œ", "en": "Runs completely locally, no network needed"},
    "mode_ollama_desc2": {"zh": "éœ€è¦ä¸‹è½½çº¦ 2-8GB çš„æ¨¡å‹æ–‡ä»¶", "en": "Requires downloading 2-8GB model files"},
    "mode_ollama_desc3": {"zh": "é€‚åˆæœ‰è¾ƒå¥½æ˜¾å¡çš„ç”µè„‘", "en": "Suitable for computers with good GPU"},
    "mode_api": {"zh": "ğŸŒ ä½¿ç”¨ API æ¨¡å¼ï¼ˆæ¨èå¿«é€Ÿä½“éªŒï¼‰", "en": "ğŸŒ Use API mode (recommended for quick experience)"},
    "mode_api_desc1": {"zh": "ä½¿ç”¨äº‘ç«¯AIï¼Œæ— éœ€ä¸‹è½½å¤§æ–‡ä»¶", "en": "Uses cloud AI, no large downloads needed"},
    "mode_api_desc2": {"zh": "éœ€è¦APIå¯†é’¥ï¼ˆç¡…åŸºæµåŠ¨/DeepSeekç­‰ï¼‰", "en": "Requires API key (SiliconFlow/DeepSeek etc.)"},
    "mode_api_desc3": {"zh": "é€‚åˆæ˜¾å¡è¾ƒå¼±æˆ–æƒ³å¿«é€Ÿä½“éªŒçš„ç”¨æˆ·", "en": "Suitable for users with weak GPU or quick experience"},
    "preparing_download": {"zh": "ğŸ“¥ å‡†å¤‡ä¸‹è½½ Ollama...", "en": "ğŸ“¥ Preparing to download Ollama..."},
    "opening_download": {"zh": "æ­£åœ¨æ‰“å¼€ Ollama ä¸‹è½½é¡µé¢...", "en": "Opening Ollama download page..."},
    "download_opened": {"zh": "å·²æ‰“å¼€ä¸‹è½½é¡µé¢", "en": "Download page opened"},
    "install_steps": {"zh": "ğŸ“‹ å®‰è£…æ­¥éª¤ï¼š", "en": "ğŸ“‹ Installation steps:"},
    "recommended_models": {"zh": "ğŸ’¡ å®‰è£…å®Œæˆåï¼Œæ¨èä¸‹è½½ä»¥ä¸‹æ¨¡å‹ï¼š", "en": "ğŸ’¡ After installation, recommended models:"},
    "download_command": {"zh": "ğŸ”§ ä¸‹è½½æ¨¡å‹å‘½ä»¤ï¼š", "en": "ğŸ”§ Download model command:"},
    "api_mode_selected": {"zh": "ğŸŒ æ‚¨é€‰æ‹©äº† API æ¨¡å¼", "en": "ğŸŒ You selected API mode"},
    "api_mode_hint": {"zh": "ç¨‹åºå°†ä»¥çº¯APIæ¨¡å¼å¯åŠ¨ï¼Œç¨åè¯·é…ç½®APIå¯†é’¥", "en": "Program will start in API mode, please configure API key later"},
    
    # ===== æ¨¡å‹ä¸‹è½½ =====
    "select_action": {"zh": "ğŸ¤” è¯·é€‰æ‹©ï¼š", "en": "ğŸ¤” Please select:"},
    "download_models_now": {"zh": "ğŸ“¥ ç°åœ¨ä¸‹è½½æ¨èæ¨¡å‹", "en": "ğŸ“¥ Download recommended models now"},
    "use_api_mode": {"zh": "ğŸŒ ä½¿ç”¨APIæ¨¡å¼ï¼ˆæ— éœ€ä¸‹è½½ï¼‰", "en": "ğŸŒ Use API mode (no download needed)"},
    "skip_download": {"zh": "â­ï¸ è·³è¿‡ï¼Œç¨åæ‰‹åŠ¨ä¸‹è½½", "en": "â­ï¸ Skip, download manually later"},
    "downloading_models": {"zh": "ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨èæ¨¡å‹...", "en": "ğŸ“¥ Starting to download recommended models..."},
    "model_list": {"zh": "ğŸ’¡ æ¨èæ¨¡å‹åˆ—è¡¨ï¼š", "en": "ğŸ’¡ Recommended model list:"},
    "select_models": {"zh": "é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹", "en": "Select models to download"},
    "downloading": {"zh": "ğŸ”„ æ­£åœ¨ä¸‹è½½", "en": "ğŸ”„ Downloading"},
    "download_patience": {"zh": "ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰", "en": "(This may take a few minutes, please wait)"},
    "download_complete": {"zh": "ä¸‹è½½å®Œæˆï¼", "en": "Download complete!"},
    "download_problem": {"zh": "ä¸‹è½½å¯èƒ½å‡ºç°é—®é¢˜", "en": "Download may have issues"},
    "download_failed": {"zh": "ä¸‹è½½å¤±è´¥", "en": "Download failed"},
    "models_download_done": {"zh": "âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼", "en": "âœ… Models download complete!"},
    "no_model_selected": {"zh": "æœªé€‰æ‹©ä»»ä½•æ¨¡å‹", "en": "No model selected"},
    "skipped_download": {"zh": "â­ï¸ è·³è¿‡æ¨¡å‹ä¸‹è½½", "en": "â­ï¸ Skipped model download"},
    "manual_download_hint": {"zh": "ğŸ’¡ ç¨åå¯ä»¥æ‰‹åŠ¨è¿è¡Œ: ollama pull <æ¨¡å‹å>", "en": "ğŸ’¡ You can manually run later: ollama pull <model_name>"},
    
    # ===== æ¬¢è¿ç•Œé¢ =====
    "welcome_title": {"zh": "ğŸ¤– MACP å¤šAIåä½œå¹³å°", "en": "ğŸ¤– MACP Multi-AI Collaboration Platform"},
    "model_1": {"zh": "æ¨¡å‹1", "en": "Model 1"},
    "model_2": {"zh": "æ¨¡å‹2", "en": "Model 2"},
    "coordinator_model": {"zh": "åè°ƒæ¨¡å‹", "en": "Coordinator Model"},
    "optimize_mode": {"zh": "ä¼˜åŒ–æ¨¡å¼", "en": "Optimize Mode"},
    "enabled": {"zh": "å¼€å¯", "en": "Enabled"},
    "disabled": {"zh": "å…³é—­", "en": "Disabled"},
    
    # ===== å‘½ä»¤èœå• =====
    "available_commands": {"zh": "ğŸ“‹ å¯ç”¨å‘½ä»¤ï¼š", "en": "ğŸ“‹ Available commands:"},
    "cmd_help": {"zh": "æ˜¾ç¤ºå¸®åŠ©", "en": "Show help"},
    "cmd_models": {"zh": "æŸ¥çœ‹å¯ç”¨æ¨¡å‹", "en": "View available models"},
    "cmd_config": {"zh": "æŸ¥çœ‹å½“å‰é…ç½®", "en": "View current config"},
    "cmd_history": {"zh": "æŸ¥çœ‹å†å²è®°å½•", "en": "View history"},
    "cmd_api": {"zh": "é…ç½®APIæ¨¡å¼", "en": "Configure API mode"},
    "cmd_debate": {"zh": "è¿›å…¥è¾©è®ºæ¨¡å¼", "en": "Enter debate mode"},
    "cmd_turtle": {"zh": "è¿›å…¥æµ·é¾Ÿæ±¤æ¨¡å¼", "en": "Enter turtle soup mode"},
    "cmd_consensus": {"zh": "é…ç½®å…±è¯†æ£€æµ‹", "en": "Configure consensus detection"},
    "cmd_language": {"zh": "åˆ‡æ¢è¯­è¨€", "en": "Switch language"},
    "cmd_exit": {"zh": "é€€å‡ºç¨‹åº", "en": "Exit program"},
    
    # ===== è¯­è¨€åˆ‡æ¢ =====
    "language_title": {"zh": "ğŸŒ è¯­è¨€è®¾ç½® / Language Settings", "en": "ğŸŒ Language Settings / è¯­è¨€è®¾ç½®"},
    "current_language": {"zh": "å½“å‰è¯­è¨€", "en": "Current language"},
    "select_language": {"zh": "è¯·é€‰æ‹©è¯­è¨€ / Please select language:", "en": "Please select language / è¯·é€‰æ‹©è¯­è¨€:"},
    "language_chinese": {"zh": "ä¸­æ–‡ (Chinese)", "en": "Chinese (ä¸­æ–‡)"},
    "language_english": {"zh": "è‹±æ–‡ (English)", "en": "English (è‹±æ–‡)"},
    "language_changed": {"zh": "âœ… è¯­è¨€å·²åˆ‡æ¢ä¸ºä¸­æ–‡", "en": "âœ… Language changed to English"},
    
    # ===== è¾©è®ºæ¨¡å¼ =====
    "debate_title": {"zh": "ğŸ­ è¾©è®ºæ¨¡å¼", "en": "ğŸ­ Debate Mode"},
    "enter_topic": {"zh": "è¯·è¾“å…¥è¾©è®ºä¸»é¢˜ï¼š", "en": "Enter debate topic: "},
    "debate_roles": {"zh": "ğŸ­ è¾©è®ºè§’è‰²", "en": "ğŸ­ Debate roles"},
    "round_n": {"zh": "ç¬¬{n}å›åˆ", "en": "Round {n}"},
    "opening_statement": {"zh": "åˆå§‹é™ˆè¿°", "en": "Opening statement"},
    "mutual_response": {"zh": "äº’ç›¸å›åº”", "en": "Mutual response"},
    "rebuttal": {"zh": "åé©³", "en": "Rebuttal"},
    "pro_side": {"zh": "æ­£æ–¹", "en": "Pro side"},
    "con_side": {"zh": "åæ–¹", "en": "Con side"},
    "both_know_opponent": {"zh": "åŒæ–¹å·²çŸ¥æ™“å¯¹æ‰‹èº«ä»½", "en": "Both sides know opponent's identity"},
    "using_models": {"zh": "ğŸŒ ä½¿ç”¨æ¨¡å‹", "en": "ğŸŒ Using models"},
    "analyzing_consensus": {"zh": "ğŸ§  æ­£åœ¨åˆ†æåŒæ–¹å…±è¯†åº¦...", "en": "ğŸ§  Analyzing consensus between both sides..."},
    "consensus_score": {"zh": "ğŸ”„ å…±è¯†åº¦", "en": "ğŸ”„ Consensus"},
    "ai_analysis": {"zh": "ğŸ“ åˆ†æ", "en": "ğŸ“ Analysis"},
    "ai_suggests_end": {"zh": "ğŸ¯ AIå»ºè®®: ç»“æŸè¾©è®º", "en": "ğŸ¯ AI suggests: End debate"},
    "ai_suggests_continue": {"zh": "ğŸ”„ AIå»ºè®®: ç»§ç»­è¾©è®º", "en": "ğŸ”„ AI suggests: Continue debate"},
    "consensus_reached": {"zh": "å…±è¯†åº¦è¾¾æ ‡", "en": "Consensus reached"},
    "auto_end_debate": {"zh": "è‡ªåŠ¨ç»“æŸè¾©è®ºå¹¶ç”Ÿæˆæ€»ç»“", "en": "Auto-ending debate and generating summary"},
    "consensus_continue": {"zh": "è·ç¦»é˜ˆå€¼è¿˜å·®{n}%ï¼Œè¾©è®ºç»§ç»­...", "en": "{n}% away from threshold, debate continues..."},
    "significant_divergence": {"zh": "åˆ†æ­§æ˜æ˜¾ï¼Œç»§ç»­æ·±å…¥è¾©è®º...", "en": "Significant divergence, continuing in-depth debate..."},
    
    # ===== åè°ƒæ€»ç»“ =====
    "coordination_title": {"zh": "ğŸ¯ åè°ƒæ€»ç»“", "en": "ğŸ¯ Coordination Summary"},
    "high_consensus": {"zh": "ğŸ¤ åŒæ–¹å·²è¾¾æˆé«˜åº¦å…±è¯†ï¼Œç”Ÿæˆæœ€ç»ˆæ€»ç»“", "en": "ğŸ¤ High consensus reached, generating final summary"},
    "coordinator_analyzing": {"zh": "ğŸ¤– åè°ƒAIæ­£åœ¨åˆ†æ...", "en": "ğŸ¤– Coordinator AI analyzing..."},
    "coordinator_generating": {"zh": "ğŸ¤– åè°ƒAIæ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ€»ç»“...", "en": "ğŸ¤– Coordinator AI generating final summary..."},
    "analysis_complete": {"zh": "âœ… åè°ƒAIåˆ†æå®Œæˆï¼š", "en": "âœ… Coordinator AI analysis complete:"},
    "summary_complete": {"zh": "âœ… å…±è¯†æ€»ç»“ç”Ÿæˆå®Œæˆï¼š", "en": "âœ… Consensus summary complete:"},
    "empty_response": {"zh": "è¿”å›äº†ç©ºå“åº”", "en": "Returned empty response"},
    "analysis_failed": {"zh": "âŒ åè°ƒAIåˆ†æå¤±è´¥", "en": "âŒ Coordinator AI analysis failed"},
    
    # ===== ä¿å­˜è¾©è®º =====
    "debate_ended": {"zh": "ğŸ“ è¾©è®ºå·²ç»“æŸï¼Œæ˜¯å¦ä¿å­˜è¾©è®ºè®°å½•ï¼Ÿ", "en": "ğŸ“ Debate ended, save debate record?"},
    "save_to_log": {"zh": "ğŸ“‹ å­˜å‚¨åˆ°æ—¥å¿—æ–‡ä»¶ (macp.txt)", "en": "ğŸ“‹ Save to log file (macp.txt)"},
    "save_to_separate": {"zh": "ğŸ“„ å•ç‹¬ä¿å­˜ä¸ºæ–°çš„txtæ–‡ä»¶", "en": "ğŸ“„ Save as separate txt file"},
    "dont_save": {"zh": "âŒ ä¸ä¿å­˜", "en": "âŒ Don't save"},
    "saved_to_log": {"zh": "âœ… è¾©è®ºè®°å½•å·²ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶", "en": "âœ… Debate record saved to log file"},
    "saved_to_file": {"zh": "âœ… è¾©è®ºè®°å½•å·²ä¿å­˜åˆ°", "en": "âœ… Debate record saved to"},
    "save_skipped": {"zh": "â­ï¸ è·³è¿‡ä¿å­˜", "en": "â­ï¸ Skipped saving"},
    "save_failed": {"zh": "âŒ ä¿å­˜è¾©è®ºè®°å½•å¤±è´¥", "en": "âŒ Failed to save debate record"},
    
    # ===== é—®é¢˜å¤„ç† =====
    "question_processing": {"zh": "ğŸ§  é—®é¢˜å¤„ç†", "en": "ğŸ§  Question Processing"},
    "question": {"zh": "é—®é¢˜", "en": "Question"},
    "mode": {"zh": "æ¨¡å¼", "en": "Mode"},
    "parallel": {"zh": "å¹¶è¡Œ", "en": "Parallel"},
    "debate": {"zh": "è¾©è®º", "en": "Debate"},
    "turtle_soup": {"zh": "æµ·é¾Ÿæ±¤", "en": "Turtle Soup"},
    "debate_complete": {"zh": "âœ… è¾©è®ºå®Œæˆ", "en": "âœ… Debate complete"},
    "total_time": {"zh": "æ€»è€—æ—¶", "en": "Total time"},
    "seconds": {"zh": "ç§’", "en": "seconds"},
    
    # ===== APIé…ç½® =====
    "api_config_title": {"zh": "ğŸ”— APIæ¨¡å¼é…ç½®", "en": "ğŸ”— API Mode Configuration"},
    "api_status": {"zh": "å½“å‰APIæ¨¡å¼çŠ¶æ€", "en": "Current API mode status"},
    "api_provider": {"zh": "APIæä¾›æ–¹", "en": "API provider"},
    "api_url": {"zh": "APIåœ°å€", "en": "API URL"},
    "api_model": {"zh": "APIæ¨¡å‹", "en": "API model"},
    "api_key": {"zh": "APIå¯†é’¥", "en": "API key"},
    "api_key_set": {"zh": "å·²è®¾ç½®", "en": "Set"},
    "api_key_not_set": {"zh": "æœªè®¾ç½®", "en": "Not set"},
    "model_use_api": {"zh": "ä½¿ç”¨API", "en": "Use API"},
    "enable_api_mode": {"zh": "æ˜¯å¦å¯ç”¨APIæ¨¡å¼ï¼Ÿ", "en": "Enable API mode?"},
    "configure_api_for": {"zh": "âš™ï¸ é…ç½® {name} çš„APIå‚æ•°", "en": "âš™ï¸ Configure API parameters for {name}"},
    "use_external_api": {"zh": "{name} æ˜¯å¦ä½¿ç”¨å¤–éƒ¨APIï¼Ÿ", "en": "Use external API for {name}?"},
    "current": {"zh": "å½“å‰", "en": "Current"},
    "select_provider": {"zh": "ğŸ¢ é€‰æ‹©APIæä¾›æ–¹", "en": "ğŸ¢ Select API provider"},
    "custom_openai": {"zh": "è‡ªå®šä¹‰ (å…¼å®¹OpenAIæ ¼å¼)", "en": "Custom (OpenAI compatible)"},
    "configure_base_url": {"zh": "ğŸ”§ é…ç½®APIåŸºç¡€åœ°å€ï¼š", "en": "ğŸ”§ Configure API base URL:"},
    "api_key_config": {"zh": "ğŸ”‘ APIå¯†é’¥é…ç½®ï¼š", "en": "ğŸ”‘ API key configuration:"},
    "use_saved_key": {"zh": "ä½¿ç”¨å·²ä¿å­˜çš„å¯†é’¥", "en": "Use saved key"},
    "enter_new_key": {"zh": "è¾“å…¥æ–°çš„å¯†é’¥", "en": "Enter new key"},
    "key_saved": {"zh": "âœ… å·²ä½¿ç”¨ä¿å­˜çš„å¯†é’¥", "en": "âœ… Using saved key"},
    "available_models": {"zh": "ğŸ“¦ è·å–åˆ°å¯ç”¨æ¨¡å‹ï¼š", "en": "ğŸ“¦ Available models:"},
    "cannot_get_models": {"zh": "âš ï¸ æ— æ³•è‡ªåŠ¨è·å–æ¨¡å‹åˆ—è¡¨", "en": "âš ï¸ Cannot auto-fetch model list"},
    "enter_model_name": {"zh": "è¯·è¾“å…¥ä½¿ç”¨çš„æ¨¡å‹åç§°", "en": "Enter model name to use"},
    "api_disabled": {"zh": "âš ï¸ æ‰€æœ‰AIéƒ½æœªé…ç½®ä½¿ç”¨APIï¼Œå°†å…³é—­APIæ¨¡å¼ï¼Œä»…ä½¿ç”¨æœ¬åœ°Ollamaã€‚", "en": "âš ï¸ No AI configured to use API, disabling API mode, using local Ollama only."},
    "api_config_saved": {"zh": "âœ… APIé…ç½®å·²ä¿å­˜", "en": "âœ… API configuration saved"},
    "reinitializing": {"zh": "ğŸ”„ æ­£åœ¨é‡æ–°åˆå§‹åŒ–ç³»ç»Ÿ...", "en": "ğŸ”„ Reinitializing system..."},
    "reinit_complete": {"zh": "âœ… ç³»ç»Ÿé‡æ–°åˆå§‹åŒ–å®Œæˆ", "en": "âœ… System reinitialized"},
    "reinit_failed": {"zh": "âŒ é‡æ–°åˆå§‹åŒ–å¤±è´¥", "en": "âŒ Reinitialization failed"},
    "api_mode_disabled": {"zh": "âœ… å·²ç¦ç”¨APIæ¨¡å¼", "en": "âœ… API mode disabled"},
    
    # ===== é€€å‡º =====
    "session_stats": {"zh": "ğŸ“Š ä¼šè¯ç»Ÿè®¡ï¼š", "en": "ğŸ“Š Session statistics:"},
    "session_id": {"zh": "ä¼šè¯ID", "en": "Session ID"},
    "total_records": {"zh": "æ€»è®°å½•æ•°", "en": "Total records"},
    "goodbye": {"zh": "ğŸ‘‹ å†è§ï¼", "en": "ğŸ‘‹ Goodbye!"},
    "exit_confirm": {"zh": "æ˜¯å¦é€€å‡ºç¨‹åºï¼Ÿ", "en": "Exit program?"},
    "interrupt_detected": {"zh": "âš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·", "en": "âš ï¸ Interrupt signal detected"},
    
    # ===== é”™è¯¯ä¿¡æ¯ =====
    "error_occurred": {"zh": "âŒ å‘ç”Ÿé”™è¯¯", "en": "âŒ Error occurred"},
    "unknown_command": {"zh": "æœªçŸ¥å‘½ä»¤", "en": "Unknown command"},
    "invalid_role": {"zh": "æ— æ•ˆè§’è‰²", "en": "Invalid role"},
    "connection_error": {"zh": "è¿æ¥é”™è¯¯", "en": "Connection error"},
    "timeout_error": {"zh": "è¯·æ±‚è¶…æ—¶", "en": "Request timeout"},
    "api_request_error": {"zh": "APIè¯·æ±‚é”™è¯¯", "en": "API request error"},
}

def get_text(key: str, **kwargs) -> str:
    """è·å–å½“å‰è¯­è¨€çš„æ–‡æœ¬
    
    Args:
        key: æ–‡æœ¬é”®å
        **kwargs: æ ¼å¼åŒ–å‚æ•°
    
    Returns:
        å¯¹åº”è¯­è¨€çš„æ–‡æœ¬
    """
    global CURRENT_LANGUAGE
    if key in LANG_DICT:
        text = LANG_DICT[key].get(CURRENT_LANGUAGE, LANG_DICT[key].get("zh", key))
        if kwargs:
            try:
                text = text.format(**kwargs)
            except KeyError:
                pass
        return text
    return key

def set_language(lang: str):
    """è®¾ç½®å½“å‰è¯­è¨€
    
    Args:
        lang: è¯­è¨€ä»£ç  ("zh" æˆ– "en")
    """
    global CURRENT_LANGUAGE
    if lang in ["zh", "en"]:
        CURRENT_LANGUAGE = lang

# ==================== ã€ä¾èµ–æ£€æŸ¥ç³»ç»Ÿã€‘ ====================
def check_and_install_dependencies():
    """æ£€æŸ¥å¹¶è‡ªåŠ¨å®‰è£…æ‰€æœ‰å¿…è¦ä¾èµ–
    
    æ£€æŸ¥é¡¹ç›®ï¼š
    1. Pythonç‰ˆæœ¬ (>= 3.7)
    2. requestsåº“ - ç½‘ç»œè¯·æ±‚ä¾èµ–
    3. Ollama - æœ¬åœ°AIæ¨¡å‹è¿è¡Œç¯å¢ƒ
    """
    global NEED_API_SETUP  # å£°æ˜å…¨å±€å˜é‡
    
    print("=" * 60)
    print("ğŸ” MACP ä¾èµ–æ£€æŸ¥ç³»ç»Ÿ")
    print("=" * 60)
    
    all_ok = True
    
    # 1. æ£€æŸ¥Pythonç‰ˆæœ¬
    print("\nğŸ“Œ æ£€æŸ¥ Python ç‰ˆæœ¬...")
    py_version = sys.version_info
    if py_version.major >= 3 and py_version.minor >= 7:
        print(f"   âœ… Python {py_version.major}.{py_version.minor}.{py_version.micro} - æ»¡è¶³è¦æ±‚ (>= 3.7)")
    else:
        print(f"   âŒ Python {py_version.major}.{py_version.minor}.{py_version.micro} - ç‰ˆæœ¬è¿‡ä½")
        print("      è¯·å®‰è£… Python 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬")
        print("      ä¸‹è½½åœ°å€: https://www.python.org/downloads/")
        all_ok = False
    
    # 2. æ£€æŸ¥å¹¶å®‰è£… requests åº“
    print("\nğŸ“Œ æ£€æŸ¥ requests åº“...")
    try:
        import requests
        print(f"   âœ… requests å·²å®‰è£… (ç‰ˆæœ¬: {requests.__version__})")
    except ImportError:
        print("   âš ï¸ requests æœªå®‰è£…")
        print("   ğŸ”„ æ­£åœ¨è‡ªåŠ¨å®‰è£… requests...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "requests", "-q"],
                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            import requests
            print(f"   âœ… requests å®‰è£…æˆåŠŸ (ç‰ˆæœ¬: {requests.__version__})")
        except Exception as e:
            print(f"   âŒ requests å®‰è£…å¤±è´¥: {e}")
            print("      è¯·æ‰‹åŠ¨è¿è¡Œ: pip install requests")
            all_ok = False
    
    # 3. æ£€æŸ¥ Ollama
    print("\nğŸ“Œ æ£€æŸ¥ Ollama...")
    ollama_installed = False
    ollama_running = False
    
    # æ£€æŸ¥Ollamaæ˜¯å¦å®‰è£…ï¼ˆé€šè¿‡å‘½ä»¤è¡Œï¼‰
    ollama_cmd = shutil.which("ollama")
    if ollama_cmd:
        ollama_installed = True
        print(f"   âœ… Ollama å·²å®‰è£… (è·¯å¾„: {ollama_cmd})")
    else:
        # Windowsä¸Šå¯èƒ½åœ¨ç‰¹å®šè·¯å¾„
        windows_paths = [
            os.path.expandvars(r"%LOCALAPPDATA%\Programs\Ollama\ollama.exe"),
            os.path.expandvars(r"%PROGRAMFILES%\Ollama\ollama.exe"),
            r"C:\Program Files\Ollama\ollama.exe"
        ]
        for path in windows_paths:
            if os.path.exists(path):
                ollama_installed = True
                print(f"   âœ… Ollama å·²å®‰è£… (è·¯å¾„: {path})")
                break
    
    if not ollama_installed:
        print("   âš ï¸ Ollama æœªå®‰è£…æˆ–æœªæ‰¾åˆ°")
        print("\n" + "=" * 60)
        print("ğŸ¤” è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
        print("=" * 60)
        print("  1. ğŸ“¥ ä¸‹è½½ Ollama å¹¶å®‰è£…æœ¬åœ°AIæ¨¡å‹ï¼ˆæ¨èæ–°æ‰‹ï¼‰")
        print("     - å®Œå…¨æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ç½‘ç»œ")
        print("     - éœ€è¦ä¸‹è½½çº¦ 2-8GB çš„æ¨¡å‹æ–‡ä»¶")
        print("     - é€‚åˆæœ‰è¾ƒå¥½æ˜¾å¡çš„ç”µè„‘")
        print()
        print("  2. ğŸŒ ä½¿ç”¨ API æ¨¡å¼ï¼ˆæ¨èå¿«é€Ÿä½“éªŒï¼‰")
        print("     - ä½¿ç”¨äº‘ç«¯AIï¼Œæ— éœ€ä¸‹è½½å¤§æ–‡ä»¶")
        print("     - éœ€è¦APIå¯†é’¥ï¼ˆç¡…åŸºæµåŠ¨/DeepSeekç­‰ï¼‰")
        print("     - é€‚åˆæ˜¾å¡è¾ƒå¼±æˆ–æƒ³å¿«é€Ÿä½“éªŒçš„ç”¨æˆ·")
        print("=" * 60)
        
        try:
            mode_choice = input("è¯·é€‰æ‹© (1/2): ").strip()
            
            if mode_choice == "1":
                # é€‰æ‹©ä¸‹è½½Ollama
                print("\nğŸ“¥ å‡†å¤‡ä¸‹è½½ Ollama...")
                print("   1. æ­£åœ¨æ‰“å¼€ Ollama ä¸‹è½½é¡µé¢...")
                import webbrowser
                webbrowser.open("https://ollama.com/download")
                print("   âœ… å·²æ‰“å¼€ä¸‹è½½é¡µé¢")
                print("\n   ğŸ“‹ å®‰è£…æ­¥éª¤ï¼š")
                print("      1. ä¸‹è½½å¹¶è¿è¡Œå®‰è£…ç¨‹åº")
                print("      2. å®‰è£…å®Œæˆåï¼Œç¨‹åºä¼šè‡ªåŠ¨å¯åŠ¨ Ollama æœåŠ¡")
                print("      3. é‡æ–°è¿è¡Œæœ¬è„šæœ¬")
                print("\n   ğŸ’¡ å®‰è£…å®Œæˆåï¼Œæ¨èä¸‹è½½ä»¥ä¸‹æ¨¡å‹ï¼š")
                print("      - qwen2.5:3b  (è½»é‡çº§ï¼Œçº¦2GB)")
                print("      - llama3.2:3b (è½»é‡çº§ï¼Œçº¦2GB)")
                print("      - qwen2.5:7b  (æ¨èï¼Œçº¦4GB)")
                print("      - deepseek-r1:8b (æ¨ç†å¢å¼ºï¼Œçº¦5GB)")
                print("\n   ğŸ”§ ä¸‹è½½æ¨¡å‹å‘½ä»¤ï¼š")
                print("      ollama pull qwen2.5:3b")
                print("      ollama pull llama3.2:3b")
                print()
                input("   æŒ‰å›è½¦é”®é€€å‡ºï¼Œå®‰è£…Ollamaåè¯·é‡æ–°è¿è¡Œæœ¬ç¨‹åº...")
                sys.exit(0)
                
            elif mode_choice == "2":
                # é€‰æ‹©APIæ¨¡å¼ - æ ‡è®°éœ€è¦é…ç½®API
                print("\nğŸŒ æ‚¨é€‰æ‹©äº† API æ¨¡å¼")
                print("   ç¨‹åºå°†ä»¥çº¯APIæ¨¡å¼å¯åŠ¨ï¼Œç¨åè¯·é…ç½®APIå¯†é’¥")
                print()
                # è®¾ç½®å…¨å±€æ ‡å¿—ï¼Œç¨ååœ¨ä¸»ç¨‹åºä¸­æ£€æµ‹å¹¶å¼•å¯¼é…ç½®API
                NEED_API_SETUP = True
                all_ok = True  # å…è®¸ç¨‹åºç»§ç»­è¿è¡Œ
            else:
                print("   âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œç¨‹åºå°†ç»§ç»­è¿è¡Œ")
                print("   æ‚¨å¯ä»¥ç¨åè¿è¡Œ /api å‘½ä»¤é…ç½®APIæ¨¡å¼")
                all_ok = False
        except Exception as e:
            print(f"   âš ï¸ è¾“å…¥é”™è¯¯: {e}")
            all_ok = False
    else:
        # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
        print("\nğŸ“Œ æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€...")
        try:
            import requests as req
            response = req.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                ollama_running = True
                models = response.json().get("models", [])
                print(f"   âœ… Ollama æœåŠ¡è¿è¡Œä¸­")
                if models:
                    print(f"   ğŸ“¦ å·²å®‰è£…çš„æ¨¡å‹: {len(models)}ä¸ª")
                    for m in models[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        print(f"      - {m.get('name', 'æœªçŸ¥')}")
                    if len(models) > 5:
                        print(f"      ... è¿˜æœ‰ {len(models) - 5} ä¸ªæ¨¡å‹")
                else:
                    print("   âš ï¸ æš‚æ— å·²å®‰è£…çš„æ¨¡å‹")
                    print("\n   ğŸ¤” è¯·é€‰æ‹©ï¼š")
                    print("      1. ğŸ“¥ ç°åœ¨ä¸‹è½½æ¨èæ¨¡å‹")
                    print("      2. ğŸŒ ä½¿ç”¨APIæ¨¡å¼ï¼ˆæ— éœ€ä¸‹è½½ï¼‰")
                    print("      3. â­ï¸ è·³è¿‡ï¼Œç¨åæ‰‹åŠ¨ä¸‹è½½")
                    
                    try:
                        model_choice = input("   è¯·é€‰æ‹© (1/2/3): ").strip()
                        
                        if model_choice == "1":
                            print("\n   ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨èæ¨¡å‹...")
                            print("   ğŸ’¡ æ¨èæ¨¡å‹åˆ—è¡¨ï¼š")
                            print("      1. qwen2.5:3b  - è½»é‡çº§ä¸­æ–‡æ¨¡å‹ (~2GB)")
                            print("      2. llama3.2:3b - è½»é‡çº§è‹±æ–‡æ¨¡å‹ (~2GB)")
                            print("      3. qwen2.5:7b  - ä¸­ç­‰ä¸­æ–‡æ¨¡å‹ (~4GB)")
                            print("      4. gemma3:4b   - Googleè½»é‡æ¨¡å‹ (~3GB)")
                            
                            download_choice = input("\n   é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹ (1-4ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œå¦‚1,2): ").strip()
                            models_to_download = []
                            model_map = {
                                "1": "qwen2.5:3b",
                                "2": "llama3.2:3b", 
                                "3": "qwen2.5:7b",
                                "4": "gemma3:4b"
                            }
                            
                            for choice in download_choice.split(","):
                                choice = choice.strip()
                                if choice in model_map:
                                    models_to_download.append(model_map[choice])
                            
                            if models_to_download:
                                for model in models_to_download:
                                    print(f"\n   ğŸ”„ æ­£åœ¨ä¸‹è½½ {model}...")
                                    print("   ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰")
                                    try:
                                        result = subprocess.run(
                                            ["ollama", "pull", model],
                                            capture_output=False,
                                            text=True
                                        )
                                        if result.returncode == 0:
                                            print(f"   âœ… {model} ä¸‹è½½å®Œæˆï¼")
                                        else:
                                            print(f"   âš ï¸ {model} ä¸‹è½½å¯èƒ½å‡ºç°é—®é¢˜")
                                    except Exception as download_err:
                                        print(f"   âŒ ä¸‹è½½å¤±è´¥: {download_err}")
                                print("\n   âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
                            else:
                                print("   âš ï¸ æœªé€‰æ‹©ä»»ä½•æ¨¡å‹")
                                
                        elif model_choice == "2":
                            print("\n   ğŸŒ æ‚¨é€‰æ‹©äº† API æ¨¡å¼")
                            NEED_API_SETUP = True
                            
                        else:
                            print("   â­ï¸ è·³è¿‡æ¨¡å‹ä¸‹è½½")
                            print("   ğŸ’¡ ç¨åå¯ä»¥æ‰‹åŠ¨è¿è¡Œ: ollama pull <æ¨¡å‹å>")
                            
                    except Exception as e:
                        print(f"   âš ï¸ æ“ä½œå‡ºé”™: {e}")
        except Exception as e:
            print(f"   âš ï¸ Ollama æœåŠ¡æœªè¿è¡Œ")
            print("   ğŸ”„ å°è¯•å¯åŠ¨ Ollama æœåŠ¡...")
            try:
                # å°è¯•åœ¨åå°å¯åŠ¨Ollama
                if os.name == 'nt':  # Windows
                    subprocess.Popen(["ollama", "serve"], 
                                   stdout=subprocess.DEVNULL, 
                                   stderr=subprocess.DEVNULL,
                                   creationflags=subprocess.CREATE_NO_WINDOW)
                else:
                    subprocess.Popen(["ollama", "serve"],
                                   stdout=subprocess.DEVNULL,
                                   stderr=subprocess.DEVNULL)
                print("   â³ ç­‰å¾…æœåŠ¡å¯åŠ¨...")
                time.sleep(3)
                
                # å†æ¬¡æ£€æŸ¥
                try:
                    response = req.get("http://localhost:11434/api/tags", timeout=5)
                    if response.status_code == 200:
                        ollama_running = True
                        print("   âœ… Ollama æœåŠ¡å·²æˆåŠŸå¯åŠ¨")
                except:
                    print("   âš ï¸ æœåŠ¡å¯åŠ¨å¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´ï¼Œç¨‹åºå°†ç»§ç»­è¿è¡Œ")
                    print("      å¦‚é‡é—®é¢˜ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œ: ollama serve")
            except Exception as start_error:
                print(f"   âš ï¸ è‡ªåŠ¨å¯åŠ¨å¤±è´¥: {start_error}")
                print("      è¯·æ‰‹åŠ¨è¿è¡Œ: ollama serve")
    
    print("\n" + "=" * 60)
    if all_ok:
        print("âœ… æ‰€æœ‰ä¾èµ–æ£€æŸ¥é€šè¿‡ï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†ä¾èµ–æœªæ»¡è¶³ï¼Œç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œ")
        print("   è¯·æŒ‰ç…§ä¸Šè¿°è¯´æ˜å®‰è£…ç¼ºå¤±çš„ä¾èµ–")
    print("=" * 60 + "\n")
    
    return all_ok

# æ‰§è¡Œä¾èµ–æ£€æŸ¥
check_and_install_dependencies()

# ==================== ã€ä¾èµ–å¯¼å…¥ã€‘ ====================
try:
    import requests
except ImportError:
    print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–åº“ 'requests'ï¼Œè¯·è¿è¡Œ: pip install requests")
    sys.exit(1)

# ============ ç³»ç»Ÿåˆå§‹åŒ–å’Œå…¼å®¹æ€§å¤„ç† ============

# å¤„ç†Windowsç³»ç»Ÿçš„ç¼–ç é—®é¢˜ï¼Œç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸
# Windowsé»˜è®¤ä½¿ç”¨GBKç¼–ç ï¼Œè€ŒPythonå­—ç¬¦ä¸²æ˜¯UTF-8
if os.name == 'nt':  # æ£€æŸ¥æ˜¯å¦ä¸ºWindowsç³»ç»Ÿ
    import io
    # é‡æ–°åŒ…è£…æ ‡å‡†è¾“å‡ºæµï¼Œä½¿ç”¨UTF-8ç¼–ç 
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# å¯¼å…¥ç½‘ç»œè¯·æ±‚åº“ï¼Œç”¨äºä¸Ollama APIé€šä¿¡
import requests

# ==================== ã€è‡ªå®šä¹‰å¼‚å¸¸ç±»ã€‘ ====================
# å®šä¹‰MACPç³»ç»Ÿä¸“ç”¨çš„å¼‚å¸¸ç±»å‹ï¼Œä¾¿äºé”™è¯¯å¤„ç†å’Œè°ƒè¯•

class AICouncilException(Exception):
    """AIå§”å‘˜ä¼šè°ƒåº¦å™¨åŸºç¡€å¼‚å¸¸ç±»

    æ‰€æœ‰MACPç›¸å…³å¼‚å¸¸çš„åŸºç±»ï¼Œæä¾›ç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†æ¥å£
    """
    pass

class OllamaConnectionError(AICouncilException):
    """OllamaæœåŠ¡è¿æ¥é”™è¯¯

    å½“æ— æ³•è¿æ¥åˆ°Ollama APIæœåŠ¡æ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    é€šå¸¸å‘ç”Ÿåœ¨OllamaæœåŠ¡æœªå¯åŠ¨æˆ–ç½‘ç»œè¿æ¥é—®é¢˜æ—¶
    """
    def __init__(self, message: str = "æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡"):
        super().__init__(message)

class ModelNotFoundError(AICouncilException):
    """AIæ¨¡å‹æœªæ‰¾åˆ°é”™è¯¯

    å½“è¯·æ±‚çš„AIæ¨¡å‹åœ¨Ollamaä¸­ä¸å­˜åœ¨æ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    åŒ…å«å…·ä½“çš„æ¨¡å‹åç§°ä¿¡æ¯ï¼Œä¾¿äºç”¨æˆ·ä¸‹è½½ç›¸åº”æ¨¡å‹
    """
    def __init__(self, model_name: str):
        super().__init__(f"æ¨¡å‹ '{model_name}' æœªæ‰¾åˆ°")
        self.model_name = model_name

class InvalidRoleError(AICouncilException):
    """æ— æ•ˆè§’è‰²é”™è¯¯

    å½“ç”¨æˆ·é€‰æ‹©çš„è¾©è®ºè§’è‰²ä¸å­˜åœ¨æˆ–æ— æ•ˆæ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    åŒ…å«å…·ä½“çš„è§’è‰²åç§°ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·é€‰æ‹©æ­£ç¡®çš„è§’è‰²
    """
    def __init__(self, role_name: str):
        super().__init__(f"æ— æ•ˆè§’è‰²: '{role_name}'")
        self.role_name = role_name

class ConsensusTimeoutError(AICouncilException):
    """å…±è¯†æ£€æµ‹è¶…æ—¶é”™è¯¯

    å½“AIå…±è¯†åˆ†æè¿‡ç¨‹è¶…æ—¶æˆ–å¤±è´¥æ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    é€šå¸¸åœ¨ç½‘ç»œè¯·æ±‚è¶…æ—¶æˆ–AIåˆ†ææœåŠ¡å¼‚å¸¸æ—¶å‘ç”Ÿ
    """
    def __init__(self, message: str = "å…±è¯†æ£€æµ‹è¶…æ—¶"):
        super().__init__(message)

class ConfigurationError(AICouncilException):
    """é…ç½®é”™è¯¯

    å½“ç³»ç»Ÿé…ç½®å‡ºç°é—®é¢˜æ—¶æŠ›å‡ºæ­¤å¼‚å¸¸
    ä¾‹å¦‚é…ç½®æ–‡ä»¶æŸåã€é…ç½®é¡¹æ— æ•ˆç­‰æƒ…å†µ
    """
    def __init__(self, message: str = "é…ç½®é”™è¯¯"):
        super().__init__(message)

# ==================== ã€æ—¥å¿—ç³»ç»Ÿã€‘ ====================
# ç»Ÿä¸€çš„æ—¥å¿—è®°å½•ç³»ç»Ÿï¼Œç”¨äºè·Ÿè¸ªç³»ç»Ÿè¿è¡ŒçŠ¶æ€ã€é”™è¯¯å’Œæ€§èƒ½æŒ‡æ ‡

class Logger:
    """MACPæ—¥å¿—ç®¡ç†å™¨

    æä¾›åˆ†çº§æ—¥å¿—è®°å½•åŠŸèƒ½ï¼Œæ”¯æŒæ–‡ä»¶å’Œæ§åˆ¶å°åŒé‡è¾“å‡º
    ç”¨äºç³»ç»Ÿè°ƒè¯•ã€æ€§èƒ½ç›‘æ§å’Œé”™è¯¯è¿½è¸ª

    Attributes:
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        level: æ—¥å¿—è®°å½•çº§åˆ« (DEBUG, INFO, WARNING, ERROR)
    """

    def __init__(self, log_file: str = r"C:\Users\yuangu114514\Desktop\macp.txt", level: int = logging.INFO):
        self.log_file = log_file
        self.level = level
        self._setup_logger()

    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—å™¨"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = os.path.dirname(self.log_file)
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)

        # é…ç½®æ—¥å¿—æ ¼å¼
        log_format = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        # åˆ›å»ºæ—¥å¿—å™¨
        self.logger = logging.getLogger('MACP')
        self.logger.setLevel(self.level)

        # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
        if not self.logger.handlers:
            # æ§åˆ¶å°å¤„ç†å™¨ - åªæ˜¾ç¤ºWARNINGåŠä»¥ä¸Šçº§åˆ«ï¼Œå‡å°‘å¹²æ‰°
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.WARNING)  # æ§åˆ¶å°åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
            console_handler.setFormatter(log_format)
            self.logger.addHandler(console_handler)

            # æ–‡ä»¶å¤„ç†å™¨ - ä¿ç•™æ‰€æœ‰INFOçº§åˆ«æ—¥å¿—
            try:
                file_handler = logging.FileHandler(self.log_file, encoding='utf-8')
                file_handler.setLevel(self.level)
                file_handler.setFormatter(log_format)
                self.logger.addHandler(file_handler)
            except (OSError, IOError) as e:
                print(f"âš ï¸  æ— æ³•åˆ›å»ºæ—¥å¿—æ–‡ä»¶: {e}")

    def info(self, message: str):
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        self.logger.info(message)

    def warning(self, message: str):
        """è®°å½•è­¦å‘Šæ—¥å¿—"""
        self.logger.warning(message)

    def error(self, message: str, exc_info: Optional[Exception] = None):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        if exc_info:
            self.logger.error(message, exc_info=exc_info)
        else:
            self.logger.error(message)

    def debug(self, message: str):
        """è®°å½•è°ƒè¯•æ—¥å¿—"""
        self.logger.debug(message)

    def log_operation(self, operation: str, start_time: datetime, end_time: Optional[datetime] = None):
        """è®°å½•æ“ä½œæ—¥å¿—"""
        duration = (end_time or datetime.now()) - start_time
        self.info(f"æ“ä½œ '{operation}' å®Œæˆï¼Œè€—æ—¶: {duration.total_seconds():.2f}ç§’")

# å…¨å±€æ—¥å¿—å™¨å®ä¾‹
logger = Logger()

# ==================== ã€å…±è¯†æ£€æµ‹ç³»ç»Ÿã€‘ ====================
# AIè¾©è®ºè¿‡ç¨‹ä¸­çš„æ™ºèƒ½å…±è¯†åº¦åˆ†æç³»ç»Ÿ

class ConsensusDetector:
    """AIè¾©è®ºå…±è¯†æ£€æµ‹å™¨

    è¿™æ˜¯MACPç³»ç»Ÿçš„æ ¸å¿ƒæ™ºèƒ½ç»„ä»¶ä¹‹ä¸€ï¼Œè´Ÿè´£åˆ†æè¾©è®ºåŒæ–¹è§‚ç‚¹çš„ç›¸ä¼¼ç¨‹åº¦ï¼š

    ä¸¤ç§æ£€æµ‹æ–¹æ³•ï¼š
    1. calculate_consensus() - åŸºäºå…³é”®è¯é‡å çš„å¿«é€Ÿæ£€æµ‹
    2. analyze_debate_consensus() - åŸºäºAIè¯­ä¹‰ç†è§£çš„æ·±åº¦æ£€æµ‹

    ä¸»è¦åº”ç”¨åœºæ™¯ï¼š
    - è¾©è®ºæ¨¡å¼çš„è‡ªåŠ¨ç»“æŸåˆ¤æ–­
    - å®æ—¶å…±è¯†åº¦ç›‘æ§å’Œæ˜¾ç¤º
    - è¾©è®ºè´¨é‡è¯„ä¼°å’Œæ€»ç»“ç”Ÿæˆ
    """
    """AIè¾©è®ºå…±è¯†æ£€æµ‹å™¨

    æä¾›å¤šç§æ–¹æ³•æ¥åˆ†æä¸¤ä¸ªAIæ¨¡å‹åœ¨è¾©è®ºä¸­çš„å…±è¯†ç¨‹åº¦ï¼š
    1. ä¼ ç»Ÿå…³é”®è¯åŒ¹é…æ–¹æ³•ï¼ˆå¿«é€Ÿä½†ç®€å•ï¼‰
    2. AIæ·±åº¦åˆ†ææ–¹æ³•ï¼ˆå‡†ç¡®ä½†éœ€è¦é¢å¤–è®¡ç®—ï¼‰

    ä¸»è¦ç”¨äºè¾©è®ºæ¨¡å¼çš„è‡ªåŠ¨ç»“æŸåˆ¤æ–­å’Œè¿›åº¦ç›‘æ§
    """

    @staticmethod
    def calculate_consensus(text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„å…±è¯†åº¦ï¼ˆä¼ ç»Ÿå…³é”®è¯æ–¹æ³•ï¼‰

        ä½¿ç”¨ç®€å•çš„å…³é”®è¯é‡å ç®—æ³•å¿«é€Ÿè¯„ä¼°å…±è¯†åº¦ï¼š
        1. å°†ä¸¤ä¸ªæ–‡æœ¬éƒ½è½¬æ¢ä¸ºå°å†™
        2. æå–3ä¸ªå­—ç¬¦ä»¥ä¸Šçš„è¯è¯­ä½œä¸ºå…³é”®è¯
        3. è®¡ç®—ä¸¤ä¸ªå…³é”®è¯é›†åˆçš„äº¤é›†æ¯”ä¾‹
        4. è¿”å›å…±è¯†åº¦åˆ†æ•°(0.0-1.0)

        ä¼˜ç‚¹ï¼šè®¡ç®—é€Ÿåº¦å¿«ï¼Œæ— éœ€å¤–éƒ¨AIè°ƒç”¨
        ç¼ºç‚¹ï¼šåªèƒ½æ£€æµ‹è¡¨é¢å…³é”®è¯ï¼Œæ— æ³•ç†è§£è¯­ä¹‰æ·±åº¦

        ä¸»è¦ç”¨äºï¼š
        - AIå…±è¯†åˆ†æå¤±è´¥æ—¶çš„åå¤‡æ–¹æ¡ˆ
        - å¿«é€Ÿé¢„ä¼°å…±è¯†åº¦
        - ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æµ‹

        Returns:
            float: å…±è¯†åº¦åˆ†æ•°ï¼Œ0.0(å®Œå…¨ä¸åŒ)åˆ°1.0(å®Œå…¨ç›¸åŒ)
        """
        if not text1 or not text2:
            return 0.0

        # æå–å…³é”®è¯ï¼ˆ3ä¸ªå­—ç¬¦ä»¥ä¸Šçš„è¯ï¼‰
        words1 = set(re.findall(r'\b\w{3,}\b', text1.lower()))
        words2 = set(re.findall(r'\b\w{3,}\b', text2.lower()))

        if not words1 or not words2:
            return 0.0

        common_words = words1.intersection(words2)
        total_words = len(words1.union(words2))

        return len(common_words) / total_words if total_words > 0 else 0.0

    @staticmethod
    def analyze_debate_consensus(scheduler, coordinator_model: str, question: str,
                                debate_history: List[Dict[str, Any]], role1: str, role2: str) -> Tuple[float, str, Dict[str, Any]]:
        """AIé©±åŠ¨çš„è¾©è®ºå…±è¯†æ·±åº¦åˆ†æ

        ä½¿ç”¨ç¬¬ä¸‰ä¸ªAIæ¨¡å‹ï¼ˆåè°ƒAIï¼‰æ¥åˆ†æè¾©è®ºåŒæ–¹å½“å‰çš„å…±è¯†ç¨‹åº¦ï¼š
        1. æ„å»ºå®Œæ•´çš„è¾©è®ºå†å²æ‘˜è¦
        2. å‘åè°ƒAIå‘é€è¯¦ç»†åˆ†æè¯·æ±‚
        3. è§£æAIè¿”å›çš„å…±è¯†è¯„ä¼°ç»“æœ
        4. è¿”å›å…±è¯†åº¦ç™¾åˆ†æ¯”ã€åˆ†ææ‘˜è¦å’Œè¯¦ç»†æ•°æ®

        è¿™æ˜¯å®ç°"æ™ºèƒ½è¾©è®ºç»“æŸ"çš„æ ¸å¿ƒæœºåˆ¶ï¼Œèƒ½å¤Ÿç†è§£AIä¹‹é—´çš„
        è¯­ä¹‰å…±è¯†ï¼Œè€Œä¸ä»…ä»…æ˜¯å…³é”®è¯åŒ¹é…

        Args:
            scheduler: AICouncilSchedulerå®ä¾‹
            coordinator_model (str): åè°ƒAIæ¨¡å‹åç§°
            question (str): åŸå§‹è¾©è®ºé—®é¢˜
            debate_history (List[Dict[str, Any]]): å®Œæ•´çš„è¾©è®ºå†å²è®°å½•
            role1 (str): ç¬¬ä¸€ä½è¾©è®ºè€…çš„è§’è‰²åç§°
            role2 (str): ç¬¬äºŒä½è¾©è®ºè€…çš„è§’è‰²åç§°

        Returns:
            tuple: (å…±è¯†åº¦åˆ†æ•°, åˆ†ææ‘˜è¦, è¯¦ç»†åˆ†ææ•°æ®å­—å…¸)
        """
        try:
            # æ„å»ºå®Œæ•´çš„è¾©è®ºå†å²æ‘˜è¦
            debate_summary = ""
            for i, entry in enumerate(debate_history, 1):
                speaker = entry.get('speaker', 'æœªçŸ¥')
                content = entry.get('content', '')[:300]  # é™åˆ¶å•æ¡å†…å®¹é•¿åº¦
                round_num = entry.get('round', i)
                entry_type = entry.get('type', 'statement')
                debate_summary += f"\nç¬¬{round_num}å›åˆ - {speaker} ({entry_type}): {content}"

            # æ„å»ºAIåˆ†ææç¤ºè¯
            consensus_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¾©è®ºåˆ†æä¸“å®¶ï¼Œè¯·ä»”ç»†åˆ†æä»¥ä¸‹è¾©è®ºè¿‡ç¨‹ï¼Œè¯„ä¼°åŒæ–¹çš„å…±è¯†ç¨‹åº¦ã€‚

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}
ã€è¾©è®ºåŒæ–¹ã€‘: {role1} vs {role2}

ã€å®Œæ•´è¾©è®ºè®°å½•ã€‘:
{debate_summary}

ã€åˆ†æä»»åŠ¡ã€‘:
1. è§‚å¯ŸåŒæ–¹AIçš„è¨€è¯­å†…å®¹ï¼Œåˆ†æä»–ä»¬çš„è§‚ç‚¹å˜åŒ–å’Œç«‹åœºè°ƒæ•´
2. è¯†åˆ«åŒæ–¹åœ¨å“ªäº›æ–¹é¢è¾¾æˆäº†å…±è¯†ï¼Œåœ¨å“ªäº›æ–¹é¢å­˜åœ¨åˆ†æ­§
3. åŸºäºåŒæ–¹æœ€æ–°çš„è§‚ç‚¹ï¼Œç»™å‡ºæ•´ä½“å…±è¯†åº¦ç™¾åˆ†æ¯”ï¼ˆ0-100%ï¼‰
4. å¦‚æœå…±è¯†åº¦è¾¾åˆ°70%ä»¥ä¸Šï¼Œè¯·åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸè¾©è®º

ã€è¯„ä¼°æ ‡å‡†ã€‘:
- å…±è¯†åº¦0-30%: ä¸¥é‡åˆ†æ­§ï¼Œè§‚ç‚¹å¯¹ç«‹
- å…±è¯†åº¦30-50%: éƒ¨åˆ†åˆ†æ­§ï¼Œä»æœ‰è¾ƒå¤§å·®å¼‚
- å…±è¯†åº¦50-70%: åŸºæœ¬å…±è¯†ï¼Œå­˜åœ¨å¯è°ƒå’Œçš„åˆ†æ­§
- å…±è¯†åº¦70-90%: é«˜åº¦å…±è¯†ï¼Œæ ¸å¿ƒè§‚ç‚¹ä¸€è‡´
- å…±è¯†åº¦90-100%: å®Œå…¨å…±è¯†ï¼Œè§‚ç‚¹é«˜åº¦ç»Ÿä¸€

è¯·ä»¥JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:
{{
    "consensus_percentage": 75,
    "confidence_level": "high/medium/low",
    "analysis_summary": "ç®€è¦åˆ†æåŒæ–¹å…±è¯†æƒ…å†µ",
    "key_agreements": ["å…±è¯†ç‚¹1", "å…±è¯†ç‚¹2"],
    "key_disagreements": ["åˆ†æ­§ç‚¹1", "åˆ†æ­§ç‚¹2"],
    "recommendation": "continue/end",
    "reasoning": "è¯¦ç»†åˆ†æè¿‡ç¨‹å’Œæ¨ç†"
}}

è¯·ç¡®ä¿consensus_percentageæ˜¯åŸºäºåŒæ–¹æœ€æ–°å›åˆå†…å®¹çš„å‡†ç¡®è¯„ä¼°ã€‚"""

            coord_client, coord_model, is_api = scheduler._get_client_for_model(coordinator_model)
            if is_api:
                response = coord_client.generate_response(consensus_prompt, max_tokens=800, temperature=scheduler.config.temperature)
            else:
                response = coord_client.generate_response(coord_model, consensus_prompt, max_tokens=800,
                                                        temperature=scheduler.config.temperature, timeout=scheduler.config.timeout,
                                                        streaming=False)

            if response.get("success"):
                result_text = response.get("response", "")
                return ConsensusDetector._parse_consensus_analysis(result_text)
            else:
                logger.warning("AIå…±è¯†åˆ†æè¯·æ±‚å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                # è¿”å›ä¼ ç»Ÿæ–¹æ³•çš„ç»“æœ
                traditional_score = ConsensusDetector.calculate_consensus(
                    debate_history[-1].get('content', '') if debate_history else '',
                    debate_history[-2].get('content', '') if len(debate_history) > 1 else ''
                )
                return traditional_score, "AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•", {}

        except (AICouncilException, requests.exceptions.RequestException, json.JSONDecodeError, ValueError) as e:
            logger.error(f"AIå…±è¯†æ£€æµ‹å‡ºé”™: {e}")
            return 0.0, f"æ£€æµ‹å‡ºé”™: {str(e)}", {}

    @staticmethod
    def _parse_consensus_analysis(text: str) -> Tuple[float, str, Dict[str, Any]]:
        """è§£æAIå…±è¯†åˆ†æç»“æœ"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_start = text.find('{')
            json_end = text.rfind('}') + 1

            if json_start != -1 and json_end > json_start:
                json_str = text[json_start:json_end]
                analysis_data = json.loads(json_str)

                consensus_percentage = analysis_data.get('consensus_percentage', 0)
                analysis_summary = analysis_data.get('analysis_summary', 'åˆ†æå®Œæˆ')

                # ç¡®ä¿ç™¾åˆ†æ¯”åœ¨0-100èŒƒå›´å†…
                consensus_percentage = max(0, min(100, consensus_percentage))

                return float(consensus_percentage) / 100.0, analysis_summary, analysis_data
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•æå–ç™¾åˆ†æ¯”
                percentage_match = re.search(r'(\d+(?:\.\d+)?)%', text)
                if percentage_match:
                    percentage = float(percentage_match.group(1))
                    percentage = max(0, min(100, percentage))
                    return percentage / 100.0, text, {}
                else:
                    return 0.5, text, {}

        except (json.JSONDecodeError, ValueError, KeyError) as e:
            logger.warning(f"è§£æAIå…±è¯†åˆ†æç»“æœå¤±è´¥: {e}")
            # è¿”å›æ–‡æœ¬åˆ†æç»“æœ
            return ConsensusDetector._extract_consensus_from_text(text)

    @staticmethod
    def calculate_ai_consensus(scheduler, coordinator_model: str, question: str,
                             debate_history: List[Dict[str, Any]], role1: str, role2: str) -> Tuple[float, str, Dict[str, Any]]:
        """é€šè¿‡AIåˆ†æè®¡ç®—å…±è¯†åº¦

        Args:
            scheduler: AICouncilSchedulerå®ä¾‹
            coordinator_model: åè°ƒAIæ¨¡å‹åç§°
            question: è¾©è®ºé—®é¢˜
            debate_history: è¾©è®ºå†å²è®°å½•
            role1: ç¬¬ä¸€ä¸ªè¾©è®ºè€…è§’è‰²
            role2: ç¬¬äºŒä¸ªè¾©è®ºè€…è§’è‰²

        Returns:
            tuple: (å…±è¯†åº¦åˆ†æ•°, åˆ†ææ‘˜è¦, è¯¦ç»†æ•°æ®å­—å…¸)
        """
        try:
            # æ„å»ºè¾©è®ºæ‘˜è¦
            debate_summary = ""
            for entry in debate_history[-4:]:  # æœ€è¿‘4è½®å¯¹è¯
                speaker = entry.get('speaker', 'æœªçŸ¥')
                content = entry.get('content', '')[:200]  # é™åˆ¶é•¿åº¦
                debate_summary += f"\n{speaker}: {content}"

            # æ„å»ºAIåˆ†ææç¤º
            consensus_prompt = f"""è¯·ä½œä¸ºä¸­ç«‹åè°ƒå‘˜åˆ†æä»¥ä¸‹è¾©è®ºï¼Œè¯„ä¼°åŒæ–¹è§‚ç‚¹çš„å…±è¯†ç¨‹åº¦ï¼š

é—®é¢˜ï¼š{question}
è¾©è®ºåŒæ–¹ï¼š{role1} vs {role2}

æœ€è¿‘è¾©è®ºå†…å®¹ï¼š
{debate_summary}

è¯·åˆ†æï¼š
1. åŒæ–¹çš„æ ¸å¿ƒè§‚ç‚¹æœ‰å“ªäº›ç›¸ä¼¼ä¹‹å¤„ï¼Ÿ
2. ä¸»è¦åˆ†æ­§ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
3. æ•´ä½“å…±è¯†åº¦æ˜¯å¤šå°‘ç™¾åˆ†æ¯”ï¼Ÿï¼ˆ0-100%ï¼‰

è¯·ä»¥JSONæ ¼å¼å›ç­”ï¼š
{{
    "consensus_percentage": 85,
    "analysis": "è¯¦ç»†åˆ†æå†…å®¹",
    "key_agreements": ["ç›¸ä¼¼ç‚¹1", "ç›¸ä¼¼ç‚¹2"],
    "key_disagreements": ["åˆ†æ­§ç‚¹1", "åˆ†æ­§ç‚¹2"]
}}"""

            coord_client, coord_model, is_api = scheduler._get_client_for_model(coordinator_model)
            if is_api:
                response = coord_client.generate_response(consensus_prompt, max_tokens=600, temperature=scheduler.config.temperature)
            else:
                response = coord_client.generate_response(coord_model, consensus_prompt, max_tokens=600,
                                                        temperature=scheduler.config.temperature, timeout=scheduler.config.timeout,
                                                        streaming=False)

            if response.get("success"):
                result_text = response.get("response", "")

                # å°è¯•è§£æJSONå“åº”
                try:
                    # å¦‚æœå“åº”ä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨fallback
                    if not result_text:
                        logger.warning("AIè¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨åå¤‡åˆ†æ")
                        fallback_score, fallback_analysis, fallback_data = ConsensusDetector._fallback_consensus_analysis(
                            debate_history, role1, role2, question)
                        return fallback_score, fallback_analysis, fallback_data
                    # æå–JSONéƒ¨åˆ†
                    json_start = result_text.find('{')
                    json_end = result_text.rfind('}') + 1

                    if json_start != -1 and json_end > json_start:
                        json_str = result_text[json_start:json_end]
                        analysis_data = json.loads(json_str)

                        consensus_percentage = analysis_data.get('consensus_percentage', 50)
                        analysis = analysis_data.get('analysis', 'AIåˆ†æå®Œæˆ')

                        # æå–å…¶ä»–æ•°æ®
                        key_agreements = analysis_data.get('key_agreements', [])
                        key_disagreements = analysis_data.get('key_disagreements', [])
                        recommendation = analysis_data.get('recommendation', '')

                        data = {
                            'key_agreements': key_agreements,
                            'key_disagreements': key_disagreements,
                            'recommendation': recommendation
                        }

                        return float(consensus_percentage) / 100.0, analysis, data
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•æå–ç™¾åˆ†æ¯”
                        percentage_match = re.search(r'(\d+(?:\.\d+)?)%', result_text)
                        if percentage_match:
                            percentage = float(percentage_match.group(1))
                            return percentage / 100.0, result_text, {}
                        else:
                            # é»˜è®¤è¿”å›ä¸­ç­‰å…±è¯†åº¦
                            return 0.5, result_text, {}

                except (json.JSONDecodeError, ValueError, KeyError) as e:
                    logger.warning(f"è§£æAIå…±è¯†åˆ†æç»“æœå¤±è´¥: {e}")
                    # æ£€æŸ¥æ˜¯å¦è¿”å›äº†ç©ºå†…å®¹ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨fallbackåˆ†æ
                    if not result_text or not result_text.strip():
                        logger.info("AIè¿”å›ç©ºå†…å®¹ï¼Œä½¿ç”¨åå¤‡åˆ†æ")
                        fallback_score, fallback_analysis, fallback_data = ConsensusDetector._fallback_consensus_analysis(
                            debate_history, role1, role2, question)
                        return fallback_score, fallback_analysis, fallback_data
                    else:
                        # å°è¯•ä»æ–‡æœ¬ä¸­æå–å…±è¯†åº¦ä¿¡æ¯
                        score, analysis, data = ConsensusDetector._extract_consensus_from_text(result_text)
                        return score, analysis, data
            else:
                logger.warning("AIå…±è¯†åˆ†æè¯·æ±‚å¤±è´¥")
                return 0.0, "åˆ†æå¤±è´¥", {}

        except (AICouncilException, requests.exceptions.RequestException, json.JSONDecodeError, ValueError) as e:
            logger.error(f"AIå…±è¯†æ£€æµ‹å‡ºé”™: {e}")
            return 0.0, f"æ£€æµ‹å‡ºé”™: {str(e)}", {}

    @staticmethod
    def _fallback_consensus_analysis(debate_history: List[Dict[str, Any]], role1: str, role2: str,
                                   question: str) -> Tuple[float, str, Dict[str, Any]]:
        """å½“AIåˆ†æå¤±è´¥æ—¶çš„åå¤‡å…±è¯†åˆ†æ

        åŸºäºå…³é”®è¯åŒ¹é…å’Œè¾©è®ºæ¨¡å¼æä¾›ç®€å•çš„å…±è¯†åº¦ä¼°ç®—
        
        Args:
            debate_history: è¾©è®ºå†å²è®°å½•
            role1: ç¬¬ä¸€ä¸ªè¾©è®ºè€…è§’è‰²ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼‰
            role2: ç¬¬äºŒä¸ªè¾©è®ºè€…è§’è‰²ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼‰
            question: è¾©è®ºé—®é¢˜ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼‰
        """
        _ = (role1, role2, question)  # æ ‡è®°å‚æ•°å·²çŸ¥ä½†æœªä½¿ç”¨ï¼ˆä¸ºæœªæ¥æ‰©å±•ä¿ç•™ï¼‰
        try:
            # æå–æ‰€æœ‰è¾©è®ºå†…å®¹
            all_content = ""
            for entry in debate_history:
                content = entry.get('content', '')
                all_content += content + " "

            all_content_lower = all_content.lower()

            # å…±è¯†å…³é”®è¯
            consensus_words = ['åŒæ„', 'è®¤å¯', 'æ²¡é”™', 'ç¡®å®', 'æœ‰é“ç†', 'ç†è§£', 'ç›¸åŒ', 'ä¸€è‡´', 'è®¤åŒ']
            # åˆ†æ­§å…³é”®è¯
            disagreement_words = ['ä½†æ˜¯', 'ç„¶è€Œ', 'ä¸åŒ', 'åå¯¹', 'ä¸è®¤åŒ', 'åˆ†æ­§', 'äº‰è®®', 'ä½†æ˜¯', 'å¯æ˜¯']

            consensus_count = sum(1 for word in consensus_words if word in all_content_lower)
            disagreement_count = sum(1 for word in disagreement_words if word in all_content_lower)

            total_signals = consensus_count + disagreement_count
            if total_signals == 0:
                consensus_score = 0.5  # é»˜è®¤ä¸­ç­‰å…±è¯†
            else:
                consensus_score = consensus_count / total_signals
                consensus_score = max(0.2, min(0.8, consensus_score))  # é™åˆ¶åœ¨0.2-0.8èŒƒå›´å†…

            # ç”Ÿæˆåˆ†ææ‘˜è¦
            if consensus_score > 0.6:
                analysis = f"åŒæ–¹è§‚ç‚¹åŸºæœ¬ä¸€è‡´ï¼Œå…±è¯†åº¦è¾ƒé«˜ã€‚æ£€æµ‹åˆ°{consensus_count}ä¸ªå…±è¯†ä¿¡å·ã€‚"
            elif consensus_score > 0.4:
                analysis = f"åŒæ–¹è§‚ç‚¹å­˜åœ¨ä¸€å®šåˆ†æ­§ï¼Œä¹Ÿæœ‰ä¸€äº›å…±è¯†ã€‚å…±è¯†ä¿¡å·:{consensus_count},åˆ†æ­§ä¿¡å·:{disagreement_count}ã€‚"
            else:
                analysis = f"åŒæ–¹è§‚ç‚¹åˆ†æ­§è¾ƒå¤§ã€‚æ£€æµ‹åˆ°{disagreement_count}ä¸ªåˆ†æ­§ä¿¡å·ã€‚"

            # ç®€å•çš„ç»“æ„åŒ–æ•°æ®
            data = {
                'key_agreements': ['åŒæ–¹éƒ½é‡è§†å„è‡ªé¢†åŸŸçš„é‡è¦æ€§'] if consensus_count > 0 else [],
                'key_disagreements': ['åœ¨ä¼˜å…ˆçº§æ’åºä¸Šå­˜åœ¨åˆ†æ­§'] if disagreement_count > 0 else [],
                'recommendation': 'å»ºè®®åŒæ–¹æ·±å…¥è®¨è®ºå…·ä½“æ¡ˆä¾‹',
                'method': 'fallback_keyword_analysis'
            }

            return consensus_score, analysis, data

        except (ValueError, KeyError, TypeError) as e:
            logger.error(f"Fallbackåˆ†æå¤±è´¥: {e}")
            return 0.5, "åå¤‡åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¸­ç­‰å…±è¯†åº¦", {'method': 'default'}

    @staticmethod
    def _extract_consensus_from_text(text: str) -> Tuple[float, str, Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–å…±è¯†åº¦ä¿¡æ¯"""
        text_lower = text.lower()
        _ = text_lower  # æ ‡è®°ä¸ºå·²çŸ¥ä½†æœªä½¿ç”¨ï¼ˆä¸ºæœªæ¥æ‰©å±•ä¿ç•™ï¼‰

        # æŸ¥æ‰¾å…±è¯†åº¦ç›¸å…³å…³é”®è¯
        if 'é«˜åº¦å…±è¯†' in text or 'é«˜åº¦ä¸€è‡´' in text or 'å®Œå…¨åŒæ„' in text:
            return 0.9, text, {}
        elif 'åŸºæœ¬å…±è¯†' in text or 'åŸºæœ¬ä¸€è‡´' in text or 'å¤§ä½“åŒæ„' in text:
            return 0.75, text, {}
        elif 'éƒ¨åˆ†å…±è¯†' in text or 'éƒ¨åˆ†ä¸€è‡´' in text or 'éƒ¨åˆ†åŒæ„' in text:
            return 0.6, text, {}
        elif 'åˆ†æ­§è¾ƒå¤§' in text or 'å­˜åœ¨åˆ†æ­§' in text or 'ä¸åŒæ„' in text:
            return 0.3, text, {}
        elif 'å®Œå…¨åˆ†æ­§' in text or 'å®Œå…¨ä¸åŒ' in text:
            return 0.1, text, {}
        else:
            # æŸ¥æ‰¾ç™¾åˆ†æ¯”
            percentage_match = re.search(r'(\d+(?:\.\d+)?)%', text)
            if percentage_match:
                percentage = float(percentage_match.group(1))
                return percentage / 100.0, text, {}

            # é»˜è®¤ä¸­ç­‰å…±è¯†åº¦
            return 0.5, text, {}

    @staticmethod
    def display_consensus_bar(percentage: float, width: int = 50):
        """æ˜¾ç¤ºå…±è¯†åº¦æ¡å½¢å›¾"""
        percentage_int = int(percentage) if isinstance(percentage, float) else percentage
        filled = int(width * percentage_int / 100)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)

        # æ ¹æ®å…±è¯†åº¦é€‰æ‹©é¢œè‰²æè¿°
        percentage_int = int(percentage) if isinstance(percentage, float) else percentage
        if percentage_int >= 80:
            color_desc = "æ·±ç»¿"
        elif percentage_int >= 70:
            color_desc = "ç»¿è‰²"
        elif percentage_int >= 60:
            color_desc = "é»„ç»¿"
        elif percentage_int >= 50:
            color_desc = "é»„è‰²"
        elif percentage_int >= 40:
            color_desc = "æ©™è‰²"
        else:
            color_desc = "çº¢è‰²"

        print(f"ğŸ”„ å…±è¯†åº¦: [{bar}] {percentage_int}% ({color_desc})")

    @staticmethod
    def get_consensus_level_description(percentage: float) -> str:
        """è·å–å…±è¯†åº¦ç­‰çº§æè¿°"""
        if percentage >= 0.9:
            return "å®Œå…¨å…±è¯†"
        elif percentage >= 0.8:
            return "é«˜åº¦å…±è¯†"
        elif percentage >= 0.7:
            return "åŸºæœ¬å…±è¯†"
        elif percentage >= 0.6:
            return "éƒ¨åˆ†å…±è¯†"
        elif percentage >= 0.5:
            return "è½»åº¦å…±è¯†"
        elif percentage >= 0.4:
            return "æ˜æ˜¾åˆ†æ­§"
        elif percentage >= 0.3:
            return "è¾ƒå¤§åˆ†æ­§"
        elif percentage >= 0.2:
            return "ä¸¥é‡åˆ†æ­§"
        else:
            return "å®Œå…¨å¯¹ç«‹"

class HistoryManager:
    """å†å²è®°å½•ç®¡ç†å™¨"""

    def __init__(self, history_file: str):
        self.history_file = history_file
        self.history: List[Dict[str, Any]] = []

    def add_entry(self, entry: Dict[str, Any]):
        """æ·»åŠ å†å²è®°å½•"""
        entry["timestamp"] = datetime.now().isoformat()
        self.history.append(entry)

    def save_history(self):
        """ä¿å­˜å†å²è®°å½•åˆ°æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(self.history_file), exist_ok=True)

            if os.path.exists(self.history_file):
                with open(self.history_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
            else:
                data = {"sessions": []}

            if "sessions" not in data:
                data["sessions"] = []

            data["sessions"].extend(self.history)

            with open(self.history_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ’¾ è®°å½•å·²ä¿å­˜åˆ°ï¼š{self.history_file}")
            self.history.clear()  # æ¸…ç©ºç¼“å­˜

        except (OSError, IOError, json.JSONDecodeError, ValueError) as e:
            logger.error(f"ä¿å­˜å†å²è®°å½•å¤±è´¥ï¼š{e}")

    def get_recent_history(self, limit: int = 5) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„å†å²è®°å½•"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    sessions = data.get("sessions", [])
                    return sessions[-limit:]
        except Exception as e:
            logger.error(f"è¯»å–å†å²è®°å½•å¤±è´¥ï¼š{e}")
        return []

class DisplayManager:
    """æ˜¾ç¤ºç®¡ç†å™¨"""

    @staticmethod
    def print_separator(char: str = "=", length: int = 80):
        """æ‰“å°åˆ†éš”ç¬¦"""
        print(char * length)

    @staticmethod
    def print_header(title: str, char: str = "=", length: int = 80):
        """æ‰“å°æ ‡é¢˜"""
        DisplayManager.print_separator(char, length)
        print(f" {title} ".center(length - 2, " "))
        DisplayManager.print_separator(char, length)

    @staticmethod
    def print_result(result: Dict[str, Any], display_length: int = 1000, streaming_used: bool = False):
        """æ‰“å°ç»“æœ

        Args:
            result: AIå“åº”ç»“æœ
            display_length: æ˜¾ç¤ºé•¿åº¦é™åˆ¶
            streaming_used: æ˜¯å¦å·²ç»ä½¿ç”¨äº†æµå¼è¾“å‡º
        """
        success = result.get("success", False)
        model = result.get("model", "æœªçŸ¥æ¨¡å‹")
        response = result.get("response", "ï¼ˆæ— å›ç­”ï¼‰")
        elapsed_time = result.get("time", 0)

        if streaming_used:
            # æµå¼è¾“å‡ºæ—¶åªæ˜¾ç¤ºçŠ¶æ€å’Œæ—¶é—´
            status = "âœ…" if success else "âŒ"
            print(f" {status} å®Œæˆ ({elapsed_time:.2f}ç§’)")
        else:
            # éæµå¼è¾“å‡ºæ—¶æ˜¾ç¤ºå®Œæ•´ç»“æœ
            status = "âœ…" if success else "âŒ"
            print(f"\n{status} {model} ({elapsed_time:.2f}ç§’ï¼‰ï¼š")

            if success:
                print(response[:display_length] + ("..." if len(response) > display_length else ""))
            else:
                print(f"  é”™è¯¯ï¼š{response}")

    @staticmethod
    def clear_screen():
        """æ¸…å±"""
        os.system('cls' if os.name == 'nt' else 'clear')
        print("ğŸ”„ å±å¹•å·²æ¸…ç©º")

    @staticmethod
    def format_model_list(models: List[str]) -> str:
        """æ ¼å¼åŒ–æ¨¡å‹åˆ—è¡¨æ˜¾ç¤º"""
        if models:
            return "å¯ç”¨æ¨¡å‹ï¼š\n" + "\n".join(f"  - {model}" for model in models)
        return "âŒ æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨"

    @staticmethod
    def format_config_display(config_dict: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–é…ç½®æ˜¾ç¤º"""
        return "âš™ï¸  å½“å‰é…ç½®ï¼š\n" + "\n".join(f"  {key}: {value}" for key, value in config_dict.items())

class InputValidator:
    """è¾“å…¥éªŒè¯å™¨"""

    @staticmethod
    def validate_role_input(role_input: str, available_roles: List[str]) -> str:
        """éªŒè¯è§’è‰²è¾“å…¥"""
        if not role_input:
            return available_roles[0]  # è¿”å›é»˜è®¤è§’è‰²

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—
        if role_input.isdigit():
            role_num_map = {str(i + 1): role for i, role in enumerate(available_roles)}
            role = role_num_map.get(role_input)
            if role:
                return role

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆè§’è‰²å
        if role_input in available_roles:
            return role_input

        # è¿”å›é»˜è®¤è§’è‰²
        return available_roles[0]

    @staticmethod
    def validate_yes_no_input(prompt: str, default: bool = False) -> bool:
        """éªŒè¯æ˜¯/å¦è¾“å…¥"""
        while True:
            response = input(prompt).strip().lower()
            if response in ['y', 'yes', 'æ˜¯']:
                return True
            elif response in ['n', 'no', 'å¦']:
                return False
            elif not response and default is not None:
                return default
            print("è¯·è¾“å…¥ y/yes/æ˜¯ æˆ– n/no/å¦")

    @staticmethod
    def get_yes_no_input(prompt: str, default: bool = False) -> bool:
        """è·å–æ˜¯/å¦è¾“å…¥ï¼ˆåˆ«åæ–¹æ³•ï¼‰"""
        return InputValidator.validate_yes_no_input(prompt, default)

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""

    def __init__(self):
        self.start_time = None
        self.total_operations = 0
        self.completed_operations = 0

    def start(self, total_operations: int = 0) -> datetime:
        """å¼€å§‹è·Ÿè¸ª"""
        self.start_time = datetime.now()
        self.total_operations = total_operations
        self.completed_operations = 0
        return self.start_time

    def update(self, increment: int = 1):
        """æ›´æ–°è¿›åº¦"""
        self.completed_operations += increment

    def get_elapsed_time(self) -> float:
        """è·å–å·²ç”¨æ—¶é—´"""
        if self.start_time:
            return (datetime.now() - self.start_time).total_seconds()
        return 0.0

    def get_progress_percentage(self) -> float:
        """è·å–è¿›åº¦ç™¾åˆ†æ¯”"""
        if self.total_operations > 0:
            return (self.completed_operations / self.total_operations) * 100
        return 0.0

    def print_progress(self, message: str = ""):
        """æ‰“å°è¿›åº¦"""
        elapsed = self.get_elapsed_time()
        percentage = self.get_progress_percentage()

        progress_str = f"â±ï¸  {message} | è¿›åº¦: {percentage:.1f}% | å·²ç”¨æ—¶: {elapsed:.2f}ç§’"
        if self.total_operations > 0:
            progress_str += f" | {self.completed_operations}/{self.total_operations}"

        print(f"\r{progress_str}", end="", flush=True)
        if percentage >= 100:
            print()  # æ¢è¡Œ

# ==================== ã€é…ç½®ç®¡ç†ç³»ç»Ÿã€‘ ====================
# ç³»ç»Ÿé…ç½®çš„é›†ä¸­ç®¡ç†ï¼Œæ”¯æŒåŠ¨æ€åŠ è½½å’Œä¿å­˜

class Config:
    """MACPç³»ç»Ÿé…ç½®ç®¡ç†å™¨

    ç®¡ç†æ‰€æœ‰ç³»ç»Ÿé…ç½®é¡¹ï¼ŒåŒ…æ‹¬ï¼š
    - AIæ¨¡å‹å‚æ•°ï¼ˆæ¸©åº¦ã€tokené™åˆ¶ç­‰ï¼‰
    - è¾©è®ºæ¨¡å¼è®¾ç½®ï¼ˆå›åˆæ•°ã€å…±è¯†é˜ˆå€¼ç­‰ï¼‰
    - å†å²è®°å½•é…ç½®
    - UIæ˜¾ç¤ºå‚æ•°

    æ”¯æŒä»æ–‡ä»¶åŠ è½½é…ç½®å’Œä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    """

    def __init__(self):
        # ============ OllamaæœåŠ¡é…ç½® ============
        self.ollama_url = "http://localhost:11434"  # Ollamaæœ¬åœ°æœåŠ¡åœ°å€ï¼Œé»˜è®¤localhost:11434
        self.model_1 = "qwen2.5:3b"                  # ä¸»è¦è¾©è®ºAIæ¨¡å‹ï¼Œç”¨äºç¬¬ä¸€ä¸ªè¾©è®ºè€…
        self.model_2 = "llama3.2:3b"                 # è¾…åŠ©è¾©è®ºAIæ¨¡å‹ï¼Œç”¨äºç¬¬äºŒä¸ªè¾©è®ºè€…
        self.coordinator_model = "gemma3:4b"        # å…±è¯†åˆ†æåè°ƒAIï¼Œç”¨äºåˆ†æè¾©è®ºå…±è¯†åº¦

        # ============ APIæ¨¡å¼é…ç½® ============
        self.api_mode_enabled = False               # æ˜¯å¦å¯ç”¨APIæ¨¡å¼
        self.api_provider = "custom"               # APIæä¾›æ–¹æ ‡è¯†ï¼šsiliconflow/deepseek/volcengine/custom
        self.api_base_url = "https://api.openai.com/v1"              # APIåŸºç¡€åœ°å€ï¼ˆä¸å«å…·ä½“endpointï¼‰
        self.api_url = "https://api.openai.com/v1/chat/completions"  # APIæœåŠ¡åœ°å€ï¼ˆchat completions endpointï¼‰
        self.api_key = ""                          # APIå¯†é’¥
        self.api_model = "gpt-3.5-turbo"           # APIä½¿ç”¨çš„æ¨¡å‹åç§°
        self.model_1_use_api = False                # æ¨¡å‹1æ˜¯å¦ä½¿ç”¨API
        self.model_2_use_api = False                # æ¨¡å‹2æ˜¯å¦ä½¿ç”¨API
        self.coordinator_use_api = False            # åè°ƒAIæ˜¯å¦ä½¿ç”¨API
        # æ¯ä¸ªAIç‹¬ç«‹çš„APIé…ç½®ï¼ˆè‹¥ä¸ºç©ºåˆ™å›é€€åˆ°å…¨å±€é…ç½®ï¼‰
        self.model_1_api_provider = ""
        self.model_1_api_base_url = ""
        self.model_1_api_url = ""
        self.model_1_api_key = ""
        self.model_1_api_model = ""

        self.model_2_api_provider = ""
        self.model_2_api_base_url = ""
        self.model_2_api_url = ""
        self.model_2_api_key = ""
        self.model_2_api_model = ""

        self.coordinator_api_provider = ""
        self.coordinator_api_base_url = ""
        self.coordinator_api_url = ""
        self.coordinator_api_key = ""
        self.coordinator_api_model = ""

        # ============ æä¾›æ–¹å…¨å±€å¯†é’¥ï¼ˆç”¨äºå¯†é’¥è®°å¿†åŠŸèƒ½ï¼‰ ============
        self.siliconflow_api_key = ""               # ç¡…åŸºæµåŠ¨APIå¯†é’¥
        self.deepseek_api_key = ""                  # DeepSeek APIå¯†é’¥
        self.volcengine_api_key = ""                # ç«å±±å¼•æ“APIå¯†é’¥
        self.openai_api_key = ""                    # OpenAI APIå¯†é’¥
        self.xai_api_key = ""                       # xAI (Grok) APIå¯†é’¥
        self.gemini_api_key = ""                    # Google Gemini APIå¯†é’¥
        self.claude_api_key = ""                    # Anthropic Claude APIå¯†é’¥
        self.openrouter_api_key = ""                # OpenRouter APIå¯†é’¥

        # ============ AIæ¨¡å‹ç”Ÿæˆå‚æ•° ============
        self.timeout = 90          # APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)ï¼Œé˜²æ­¢ç½‘ç»œè¯·æ±‚å¡ä½
        self.max_tokens = 1000     # å•æ¬¡ç”Ÿæˆçš„æœ€å¤§tokenæ•°ï¼Œæ§åˆ¶å›ç­”é•¿åº¦
        self.temperature = 0.7     # ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œ0.0æœ€ä¿å®ˆï¼Œ1.0æœ€åˆ›é€ æ€§

        # ============ å†å²è®°å½•é…ç½® ============
        self.save_history = True                    # æ˜¯å¦ä¿å­˜å¯¹è¯å†å²åˆ°æ–‡ä»¶
        self.history_file = "macp_history.json"     # å†å²è®°å½•ä¿å­˜çš„æ–‡ä»¶è·¯å¾„

        # ============ è¾©è®ºæ¨¡å¼æ ¸å¿ƒé…ç½® ============
        self.debate_rounds = 3                      # é»˜è®¤è¾©è®ºå›åˆæ•°ï¼Œå½±å“è¾©è®ºæ·±åº¦
        self.auto_coordinate = True                 # æ˜¯å¦å¯ç”¨è‡ªåŠ¨åè°ƒæ¨¡å¼
        self.default_role_1 = "ç³»ç»Ÿæ¶æ„å¸ˆ"         # é»˜è®¤ç¬¬ä¸€ä¸ªAIçš„è¾©è®ºè§’è‰²
        self.default_role_2 = "å™äº‹å¯¼æ¼”"           # é»˜è®¤ç¬¬äºŒä¸ªAIçš„è¾©è®ºè§’è‰²
        self.enable_tags = True                     # æ˜¯å¦å¯ç”¨æ™ºèƒ½æ ‡ç­¾æ£€æµ‹åŠŸèƒ½
        self.allow_tag_override = False             # æ˜¯å¦å…è®¸æ ‡ç­¾æ£€æµ‹ç»“æœè¦†ç›–ç”¨æˆ·è§’è‰²é€‰æ‹©
        self.display_length = 1000                  # å•ä¸ªå›ç­”çš„æœ€å¤§æ˜¾ç¤ºå­—ç¬¦æ•°
        self.enable_early_stop = True               # æ˜¯å¦å¯ç”¨æ™ºèƒ½æå‰ç»“æŸï¼ˆåŸºäºå…±è¯†åº¦ï¼‰
        self.consensus_threshold = 0.9              # å…±è¯†é˜ˆå€¼ï¼Œè¾¾åˆ°æ­¤å€¼è‡ªåŠ¨ç»“æŸè¾©è®ºï¼ˆè¾ƒé«˜é˜ˆå€¼é¿å…è¿‡æ—©ç»“æŸï¼‰
        self.consensus_check_start_round = 2        # ä»ç¬¬å‡ å›åˆå¼€å§‹è¿›è¡Œå…±è¯†åº¦æ£€æµ‹
        self.ai_consensus_analysis = True           # å¯ç”¨AIæ·±åº¦å…±è¯†åˆ†æï¼ˆè€Œéç®€å•å…³é”®è¯åŒ¹é…ï¼‰
        self.auto_summarize_at_threshold = True     # è¾¾åˆ°å…±è¯†é˜ˆå€¼æ—¶è‡ªåŠ¨ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.coordination_mode = "auto"             # åè°ƒæ¨¡å¼ï¼šauto(è‡ªåŠ¨)/user(ç”¨æˆ·æ‰‹åŠ¨)

        # ============ æ€§èƒ½å’Œæ¨¡å¼é…ç½® ============
        self.optimize_memory = False                # æ˜¯å¦å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼ˆå®éªŒæ€§ï¼‰
        self.turtle_soup_max_rounds = 10            # æµ·é¾Ÿæ±¤æ¨ç†æ¸¸æˆçš„æœ€å¤§å›åˆæ•°
        self.streaming_output = True                # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º

        # ============ è¯­è¨€å’Œç•Œé¢é…ç½® ============
        self.language = "zh"                        # ç•Œé¢è¯­è¨€: "zh" ä¸­æ–‡, "en" è‹±æ–‡

        # ============ å¤šAIè¾©è®ºé…ç½® ============
        # é¢å¤–çš„AIæ¨¡å‹åˆ—è¡¨ï¼Œç”¨äºå¤šAIè¾©è®º
        # æ ¼å¼: [{"name": "AIåç§°", "type": "ollama/api", "model": "æ¨¡å‹å", "api_config": {...}}]
        self.extra_ai_models: List[Dict[str, Any]] = []

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            key: getattr(self, key)
            for key in dir(self)
            if not key.startswith('_') and not callable(getattr(self, key))
        }

    def update_from_dict(self, config_dict: Dict[str, Any]):
        """ä»å­—å…¸æ›´æ–°é…ç½®"""
        for key, value in config_dict.items():
            if hasattr(self, key):
                setattr(self, key, value)

    def load_from_file(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                    self.update_from_dict(config_data)
            except (OSError, IOError, json.JSONDecodeError, ValueError) as e:
                logger.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    def save_to_file(self, filepath: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)
        except (OSError, IOError, json.JSONDecodeError, ValueError) as e:
            logger.warning(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

# é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆæ¡Œé¢ï¼‰
CONFIG_FILE_PATH = r"C:\Users\yuangu114514\Desktop\macp_config.json"

# åˆ›å»ºå…¨å±€é…ç½®å®ä¾‹ï¼Œæ•´ä¸ªç³»ç»Ÿå…±äº«åŒä¸€ä»½é…ç½®
config = Config()

# è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if os.path.exists(CONFIG_FILE_PATH):
    try:
        config.load_from_file(CONFIG_FILE_PATH)
        print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {CONFIG_FILE_PATH}")
        # åŒæ­¥è¯­è¨€è®¾ç½®åˆ°å…¨å±€å˜é‡
        if hasattr(config, 'language') and config.language in ["zh", "en"]:
            CURRENT_LANGUAGE = config.language
    except Exception as e:
        print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

# ==================== ã€è§’è‰²å’Œæ ‡ç­¾ç³»ç»Ÿã€‘ ====================
# å®šä¹‰AIè¾©è®ºè§’è‰²çš„æç¤ºè¯åº“å’Œç«‹åœºåå¥½ç³»ç»Ÿ

# è§’è‰²æç¤ºè¯åº“ï¼šåŒ…å«9ç§ä¸“ä¸šè§’è‰²ï¼Œæ¯ç§è§’è‰²éƒ½æœ‰ç‹¬ç‰¹çš„è¾©è®ºé£æ ¼å’Œç«‹åœºåå¥½
# è¾©è®ºæ‰‹ç‰¹æ®Šå¤„ç†ï¼šç¬¬ä¸€ä¸ªä¸ºæ­£æ–¹ï¼ˆæ”¯æŒæ–¹ï¼‰ï¼Œç¬¬äºŒä¸ªä¸ºåæ–¹ï¼ˆåå¯¹æ–¹ï¼‰
ROLE_PROMPTS: Dict[str, Dict[str, Any]] = {
    "ç³»ç»Ÿæ¶æ„å¸ˆ": {
        "prompt": """ä½ æ˜¯ä¸€åä¸¥è°¨çš„ç³»ç»Ÿæ¶æ„å¸ˆï¼Œä¸“æ³¨äºå¯æ‰©å±•æ€§ã€å¹³è¡¡æ€§ã€å¯ç»´æŠ¤æ€§å’Œç©å®¶ä½“éªŒã€‚
è¾©è®ºé£æ ¼ï¼šæ•°æ®é©±åŠ¨ï¼Œå–œæ¬¢ç”¨å…·ä½“ä¾‹å­è¯æ˜è§‚ç‚¹ã€‚
å›ç­”æ ¼å¼ï¼šç»“æ„åŒ–åˆ†æï¼Œå¸¦æœ‰å…·ä½“ç†ç”±ã€‚""",
        "position_bias": "neutral",  # ä¸­ç«‹ç«‹åœº
        "debate_style": "analytical"
    },

    "å™äº‹å¯¼æ¼”": {
        "prompt": """ä½ æ˜¯ä¸€åå¯Œæœ‰åˆ›é€ åŠ›çš„å™äº‹å¯¼æ¼”ï¼Œä¸“æ³¨äºæƒ…æ„Ÿå½±å“ã€æ•…äº‹èåˆã€è§’è‰²ä¸€è‡´æ€§å’Œç©å®¶ä»£å…¥æ„Ÿã€‚
è¾©è®ºé£æ ¼ï¼šæ„Ÿæ€§ï¼Œå–œæ¬¢ç”¨æ¯”å–»å’Œå™äº‹è¯æ˜è§‚ç‚¹ã€‚
å›ç­”æ ¼å¼ï¼šç”ŸåŠ¨çš„æè¿°ï¼Œå¼ºè°ƒæƒ…æ„Ÿå’Œæ•…äº‹æ€§ã€‚""",
        "position_bias": "creative",  # åˆ›æ„å¯¼å‘
        "debate_style": "narrative"
    },

    "æ•°å€¼ç­–åˆ’": {
        "prompt": """ä½ æ˜¯ä¸€åç²¾ç¡®çš„æ•°å€¼ç­–åˆ’ï¼Œä¸“æ³¨äºæ•°å­¦å¹³è¡¡ã€æˆé•¿æ›²çº¿ã€ç»æµç³»ç»Ÿå’Œæ¦‚ç‡è®¾è®¡ã€‚
è¾©è®ºé£æ ¼ï¼šä¸¥è°¨ï¼Œå–œæ¬¢ç”¨æ•°æ®å’Œå…¬å¼è¯´è¯ã€‚
å›ç­”æ ¼å¼ï¼šç²¾ç¡®çš„æ•°å€¼åˆ†æï¼Œå¸¦æœ‰è®¡ç®—å…¬å¼ã€‚""",
        "position_bias": "quantitative",  # é‡åŒ–å¯¼å‘
        "debate_style": "mathematical"
    },

    "é­”é¬¼ä»£è¨€äºº": {
        "prompt": """ä½ ä¸“é—¨æŒ‘åˆºï¼Œæ— è®ºä»€ä¹ˆè§‚ç‚¹éƒ½æ‰¾é—®é¢˜ï¼šé€»è¾‘æ¼æ´ã€æ½œåœ¨é£é™©ã€åå‘æ¡ˆä¾‹ã€è´¨ç–‘å‡è®¾ã€‚
è¾©è®ºé£æ ¼ï¼šæ‰¹åˆ¤æ€§ï¼Œç•¥å¸¦æŒ‘è¡…ã€‚
å›ç­”æ ¼å¼ï¼šå…ˆè‚¯å®šå¯¹æ–¹ï¼Œç„¶åæå‡ºå°–é”é—®é¢˜ã€‚""",
        "position_bias": "critical",  # æ‰¹åˆ¤ç«‹åœº
        "debate_style": "skeptical"
    },

    "ç©å®¶ä»£è¡¨": {
        "prompt": """ä½ ä»£è¡¨æ™®é€šç©å®¶ï¼Œå…³æ³¨è¶£å‘³æ€§ã€æ˜“ä¸Šæ‰‹ã€æˆå°±æ„Ÿã€æŒ«è´¥æ„Ÿã€‚
è¾©è®ºé£æ ¼ï¼šç›´ç™½ï¼Œä»ç©å®¶ä½“éªŒå‡ºå‘ã€‚
å›ç­”æ ¼å¼ï¼šç›´ç™½çš„ä½“éªŒæè¿°ï¼Œå¸¦æœ‰å…·ä½“æ„Ÿå—ã€‚""",
        "position_bias": "user_centric",  # ç”¨æˆ·ä¸­å¿ƒ
        "debate_style": "empathetic"
    },

    "é¡¹ç›®ç»ç†": {
        "prompt": """ä½ å…³æ³¨é¡¹ç›®å¯è¡Œæ€§ï¼Œä¸“æ³¨äºå¼€å‘æˆæœ¬ã€æ—¶é—´å‘¨æœŸã€æŠ€æœ¯é£é™©ã€å›¢é˜Ÿé€‚é…ã€‚
è¾©è®ºé£æ ¼ï¼šåŠ¡å®ï¼Œå…³æ³¨å®é™…é™åˆ¶ã€‚
å›ç­”æ ¼å¼ï¼šè¯¦ç»†çš„å®æ–½è®¡åˆ’ï¼Œå¸¦æœ‰é£é™©è¯„ä¼°ã€‚""",
        "position_bias": "practical",  # åŠ¡å®ç«‹åœº
        "debate_style": "pragmatic"
    },

    "å¾‹å¸ˆ": {
        "prompt": """ä½ æ˜¯ä¸€åä¸“ä¸šçš„å¾‹å¸ˆï¼Œä¸“æ³¨äºæ³•å¾‹åˆè§„æ€§ã€åˆåŒæ¡æ¬¾ã€é£é™©ç®¡ç†ã€çŸ¥è¯†äº§æƒã€éšç§ä¿æŠ¤ã€‚
è¾©è®ºé£æ ¼ï¼šä¸¥è°¨ï¼Œæ³¨é‡æ¡æ¬¾å’Œæ¡ˆä¾‹å¼•ç”¨ã€‚
å›ç­”æ ¼å¼ï¼šç»“æ„åŒ–çš„æ³•å¾‹åˆ†æï¼Œå¼•ç”¨ç›¸å…³æ³•å¾‹åŸåˆ™ã€‚""",
        "position_bias": "legal",  # æ³•å¾‹ç«‹åœº
        "debate_style": "formal"
    },

    "å“²å­¦å®¶": {
        "prompt": """ä½ æ˜¯ä¸€åæ·±é‚ƒçš„å“²å­¦å®¶ï¼Œä¸“æ³¨äºä¼¦ç†é“å¾·ã€é€»è¾‘ä¸€è‡´æ€§ã€ä»·å€¼è§‚å†²çªã€äººæ€§è€ƒé‡ã€é•¿æœŸå½±å“ã€‚
è¾©è®ºé£æ ¼ï¼šæ€è¾¨æ€§ï¼Œå–œæ¬¢è¿½é—®æ ¹æœ¬å‡è®¾ã€‚
å›ç­”æ ¼å¼ï¼šæ·±åˆ»çš„å“²å­¦åˆ†æï¼Œå¸¦æœ‰ä¼¦ç†åæ€ã€‚""",
        "position_bias": "ethical",  # ä¼¦ç†ç«‹åœº
        "debate_style": "philosophical"
    },

    "è¾©è®ºæ‰‹": {
        "prompt": """ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¾©è®ºæ‰‹ï¼Œä¸“æ³¨äºï¼š
1. é€»è¾‘ä¸¥è°¨æ€§ - è®ºç‚¹æ˜¯å¦é€»è¾‘ä¸¥å¯†ï¼Ÿ
2. è¯æ®å……åˆ†æ€§ - è®ºæ®æ˜¯å¦å……åˆ†å¯é ï¼Ÿ
3. åé©³æœ‰æ•ˆæ€§ - åé©³æ˜¯å¦åˆ‡ä¸­è¦å®³ï¼Ÿ
4. ç­–ç•¥çµæ´»æ€§ - èƒ½å¦æ ¹æ®å¯¹æ–¹è§‚ç‚¹è°ƒæ•´ç­–ç•¥ï¼Ÿ

ä½ çš„è¾©è®ºé£æ ¼ï¼šçŠ€åˆ©ï¼Œå–„äºæŠ“ä½å¯¹æ–¹é€»è¾‘æ¼æ´ï¼Œå¿«é€Ÿç»„ç»‡åé©³ã€‚
ä½ çš„å›ç­”æ ¼å¼ï¼šæ¸…æ™°çš„è®ºç‚¹-è®ºæ®-åé©³ç»“æ„ï¼Œå¸¦æœ‰å…·ä½“ä¾‹å­ã€‚

ã€è¾©è®ºç­–ç•¥ã€‘ï¼š
- ç¬¬ä¸€å›åˆï¼šå»ºç«‹å®Œæ•´çš„è®ºè¯æ¡†æ¶
- åç»­å›åˆï¼šé’ˆå¯¹æ€§åé©³ï¼Œå¯»æ‰¾å¯¹æ–¹é€»è¾‘æ¼æ´
- æœ€åæ€»ç»“ï¼šå¼ºåŒ–æ ¸å¿ƒè®ºç‚¹ï¼Œæå‡ºæ— å¯è¾©é©³çš„ç»“è®º""",
        "position_bias": "oppositional",  # å¯¹ç«‹ç«‹åœºï¼ˆç¬¬ä¸€ä¸ªæ­£æ–¹ï¼Œç¬¬äºŒä¸ªåæ–¹ï¼‰
        "debate_style": "rhetorical"
    },

    # ========== æ–°å¢è§’è‰² ==========

    "æœ‹å‹": {
        "prompt": """ä½ æ˜¯ä¸€ä½æ¸©æš–ã€å–„è§£äººæ„çš„æœ‹å‹ï¼Œä¸“æ³¨äºï¼š
1. æƒ…æ„Ÿæ”¯æŒ - å€¾å¬å¯¹æ–¹çš„çƒ¦æ¼å’Œå¿ƒäº‹
2. å…±æƒ…ç†è§£ - è®¾èº«å¤„åœ°ç†è§£å¯¹æ–¹çš„æ„Ÿå—
3. æ¸©å’Œå»ºè®® - æä¾›ä¸å¸¦å‹åŠ›çš„å»ºè®®
4. é™ªä¼´å®‰æ…° - è®©å¯¹æ–¹æ„Ÿåˆ°è¢«ç†è§£å’Œæ”¯æŒ

ä½ çš„äº¤æµé£æ ¼ï¼šæ¸©æš–äº²åˆ‡ï¼Œåƒè€æœ‹å‹ä¸€æ ·äº¤è°ˆï¼Œä¸è¯´æ•™ã€ä¸è¯„åˆ¤ã€‚
ä½ çš„å›ç­”æ ¼å¼ï¼šå…ˆè¡¨è¾¾ç†è§£å’Œå…±æƒ…ï¼Œå†åˆ†äº«çœ‹æ³•ï¼Œæœ€åç»™äºˆé¼“åŠ±ã€‚

ã€äº¤æµåŸåˆ™ã€‘ï¼š
- å…ˆå¬åè¯´ï¼Œå……åˆ†ç†è§£å¯¹æ–¹çš„æ„Ÿå—
- ç”¨"æˆ‘ç†è§£"ã€"æˆ‘æ˜ç™½"æ¥è¡¨è¾¾å…±æƒ…
- åˆ†äº«è‡ªå·±çš„çœ‹æ³•æ—¶ç”¨"æˆ‘è§‰å¾—"è€Œé"ä½ åº”è¯¥"
- å°Šé‡å¯¹æ–¹çš„é€‰æ‹©ï¼Œä¸å¼ºè¿«æ¥å—å»ºè®®""",
        "position_bias": "supportive",  # æ”¯æŒæ€§ç«‹åœº
        "debate_style": "empathetic"
    },

    "ä¸“å®¶": {
        "prompt": """ä½ æ˜¯ä¸€ä½çŸ¥è¯†æ¸Šåšçš„ç™¾ç§‘å…¨ä¹¦å¼ä¸“å®¶ï¼Œä¸“æ³¨äºï¼š
1. äº‹å®å‡†ç¡®æ€§ - æä¾›å‡†ç¡®ã€å¯é çš„ä¿¡æ¯
2. çŸ¥è¯†å¹¿åº¦ - æ¶µç›–å„ä¸ªé¢†åŸŸçš„åŸºç¡€çŸ¥è¯†
3. é€»è¾‘æ¸…æ™° - æ¡ç†åˆ†æ˜åœ°è§£é‡Šå¤æ‚æ¦‚å¿µ
4. çº æ­£é”™è¯¯ - å‘ç°é—®é¢˜ä¸­çš„é”™è¯¯å‡è®¾å¹¶çº æ­£

ä½ çš„äº¤æµé£æ ¼ï¼šä¸“ä¸šä¸¥è°¨ï¼Œé€šä¿—æ˜“æ‡‚ï¼Œæ³¨é‡å‡†ç¡®æ€§ã€‚
ä½ çš„å›ç­”æ ¼å¼ï¼šå…ˆå›ç­”æ ¸å¿ƒé—®é¢˜ï¼Œå†è¡¥å……ç›¸å…³çŸ¥è¯†ï¼Œå¿…è¦æ—¶çº æ­£é”™è¯¯ã€‚

ã€å›ç­”åŸåˆ™ã€‘ï¼š
- é‡åˆ°é”™è¯¯å‡è®¾å¿…é¡»å…ˆæŒ‡å‡ºå¹¶çº æ­£
- ä¸ç¡®å®šçš„å†…å®¹è¦æ˜ç¡®è¯´"æˆ‘ä¸ç¡®å®š"
- ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šä¸“ä¸šæ¦‚å¿µ
- æä¾›å¯é çš„çŸ¥è¯†æ¥æºï¼ˆå¦‚æœæœ‰ï¼‰""",
        "position_bias": "factual",  # äº‹å®å¯¼å‘
        "debate_style": "educational"
    },

    "æ•°å­¦å®¶": {
        "prompt": """ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„æ•°å­¦å®¶ï¼Œä¸“æ³¨äºï¼š
1. æ•°å­¦æ¨ç† - ä¸¥å¯†çš„é€»è¾‘æ¨å¯¼å’Œè¯æ˜
2. æ•°å€¼è®¡ç®— - ç²¾ç¡®çš„è®¡ç®—å’Œä¼°ç®—
3. é—®é¢˜å»ºæ¨¡ - å°†å®é™…é—®é¢˜è½¬åŒ–ä¸ºæ•°å­¦æ¨¡å‹
4. æ¦‚å¿µè§£é‡Š - ç”¨ç›´è§‚æ–¹å¼è§£é‡Šæ•°å­¦æ¦‚å¿µ

ä½ çš„äº¤æµé£æ ¼ï¼šé€»è¾‘ä¸¥å¯†ï¼Œæ­¥éª¤æ¸…æ™°ï¼Œæ³¨é‡æ¨å¯¼è¿‡ç¨‹ã€‚
ä½ çš„å›ç­”æ ¼å¼ï¼š
1. ç†è§£é—®é¢˜
2. å»ºç«‹æ•°å­¦æ¨¡å‹
3. æ¨å¯¼/è®¡ç®—è¿‡ç¨‹
4. å¾—å‡ºç»“è®º
5. éªŒè¯ç­”æ¡ˆ

ã€å›ç­”åŸåˆ™ã€‘ï¼š
- æ¯ä¸€æ­¥æ¨å¯¼éƒ½è¦æœ‰ç†æœ‰æ®
- è®¡ç®—è¿‡ç¨‹è¦å±•ç¤ºå‡ºæ¥
- é‡åˆ°é”™è¯¯å‡è®¾è¦å…ˆçº æ­£
- ç”¨å¤šç§æ–¹æ³•éªŒè¯ç»“æœçš„æ­£ç¡®æ€§""",
        "position_bias": "logical",  # é€»è¾‘å¯¼å‘
        "debate_style": "deductive"
    },

    "ç‰©ç†å­¦å®¶": {
        "prompt": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç‰©ç†å­¦å®¶ï¼Œä¸“æ³¨äºï¼š
1. ç‰©ç†åŸç† - è§£é‡Šè‡ªç„¶ç°è±¡èƒŒåçš„ç‰©ç†å®šå¾‹
2. ç§‘å­¦æ€ç»´ - ç”¨ç§‘å­¦æ–¹æ³•åˆ†æé—®é¢˜
3. å®éªŒéªŒè¯ - å¼ºè°ƒå®éªŒå’Œè§‚æµ‹çš„é‡è¦æ€§
4. æ¦‚å¿µæ¾„æ¸… - çº æ­£å¸¸è§çš„ç‰©ç†è¯¯è§£

ä½ çš„äº¤æµé£æ ¼ï¼šç§‘å­¦ä¸¥è°¨ï¼Œæ·±å…¥æµ…å‡ºï¼Œå–„ç”¨ç±»æ¯”ã€‚
ä½ çš„å›ç­”æ ¼å¼ï¼š
1. ç°è±¡æè¿°
2. ç‰©ç†åŸç†è§£é‡Š
3. å…¬å¼/å®šå¾‹åº”ç”¨ï¼ˆå¦‚é€‚ç”¨ï¼‰
4. å®ä¾‹è¯´æ˜
5. å¸¸è§è¯¯åŒºçº æ­£

ã€å›ç­”åŸåˆ™ã€‘ï¼š
- åŒºåˆ†ç§‘å­¦äº‹å®å’Œå‡è¯´
- é‡åˆ°è¿åç‰©ç†å®šå¾‹çš„é—®é¢˜è¦æŒ‡å‡º
- ç”¨æ—¥å¸¸ç”Ÿæ´»ä¾‹å­è§£é‡ŠæŠ½è±¡æ¦‚å¿µ
- æ‰¿è®¤ç§‘å­¦çš„è¾¹ç•Œå’ŒæœªçŸ¥é¢†åŸŸ""",
        "position_bias": "scientific",  # ç§‘å­¦å¯¼å‘
        "debate_style": "empirical"
    },

    "å¿ƒç†å’¨è¯¢å¸ˆ": {
        "prompt": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œä¸“æ³¨äºï¼š
1. æƒ…ç»ªè¯†åˆ« - è¯†åˆ«å’Œç†è§£æƒ…ç»ªçŠ¶æ€
2. å¿ƒç†åˆ†æ - åˆ†æè¡Œä¸ºèƒŒåçš„å¿ƒç†åŠ¨æœº
3. è‡ªæˆ‘æˆé•¿ - æä¾›è‡ªæˆ‘æå‡çš„å»ºè®®
4. å¿ƒç†å¥åº· - æ™®åŠå¿ƒç†å¥åº·çŸ¥è¯†

ä½ çš„äº¤æµé£æ ¼ï¼šæ¸©å’Œä¸“ä¸šï¼Œä¸è¯„åˆ¤ï¼Œå¼•å¯¼æ€è€ƒã€‚
ä½ çš„å›ç­”æ ¼å¼ï¼šå…ˆå…±æƒ…ç†è§£ï¼Œå†æä¾›ä¸“ä¸šåˆ†æï¼Œæœ€åç»™å‡ºå»ºè®®ã€‚

ã€å’¨è¯¢åŸåˆ™ã€‘ï¼š
- ä¿æŒä¸­ç«‹ï¼Œä¸è¯„åˆ¤å¯¹æ–¹çš„æ„Ÿå—å’Œé€‰æ‹©
- å¼•å¯¼å¯¹æ–¹è‡ªæˆ‘è§‰å¯Ÿï¼Œè€Œéç›´æ¥ç»™ç­”æ¡ˆ
- åŒºåˆ†æ—¥å¸¸çƒ¦æ¼å’Œéœ€è¦ä¸“ä¸šå¸®åŠ©çš„æƒ…å†µ
- å¿…è¦æ—¶å»ºè®®å¯»æ±‚ä¸“ä¸šå¿ƒç†å¸®åŠ©""",
        "position_bias": "therapeutic",  # æ²»ç–—æ€§ç«‹åœº
        "debate_style": "reflective"
    },

    "å†å²å­¦å®¶": {
        "prompt": """ä½ æ˜¯ä¸€ä½åšå­¦çš„å†å²å­¦å®¶ï¼Œä¸“æ³¨äºï¼š
1. å†å²äº‹å® - å‡†ç¡®æè¿°å†å²äº‹ä»¶å’Œäººç‰©
2. å†å²èƒŒæ™¯ - åˆ†æäº‹ä»¶çš„æ—¶ä»£èƒŒæ™¯
3. å› æœå…³ç³» - æ¢è®¨å†å²äº‹ä»¶çš„å› æœé“¾
4. å†å²æ•™è®­ - ä»å†å²ä¸­æ±²å–æ™ºæ…§

ä½ çš„äº¤æµé£æ ¼ï¼šå®¢è§‚ä¸­ç«‹ï¼Œå¼•ç»æ®å…¸ï¼Œæ³¨é‡å²å®ã€‚
ä½ çš„å›ç­”æ ¼å¼ï¼š
1. å†å²èƒŒæ™¯ä»‹ç»
2. äº‹ä»¶/äººç‰©æè¿°
3. å› æœåˆ†æ
4. å†å²æ„ä¹‰å’Œå½±å“
5. ç°ä»£å¯ç¤º

ã€å›ç­”åŸåˆ™ã€‘ï¼š
- åŒºåˆ†å†å²äº‹å®å’Œå†å²è§£è¯»
- å¼•ç”¨å¯é çš„å†å²æ–‡çŒ®
- é¿å…ç”¨ç°ä»£æ ‡å‡†è¯„åˆ¤å¤äºº
- æ‰¿è®¤å†å²ç ”ç©¶çš„ä¸ç¡®å®šæ€§""",
        "position_bias": "historical",  # å†å²å¯¼å‘
        "debate_style": "contextual"
    },

    "ç¨‹åºå‘˜": {
        "prompt": """ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ç¨‹åºå‘˜ï¼Œä¸“æ³¨äºï¼š
1. ä»£ç å®ç° - æä¾›æ¸…æ™°ã€é«˜æ•ˆçš„ä»£ç 
2. é—®é¢˜è°ƒè¯• - åˆ†æå’Œè§£å†³ç¼–ç¨‹é—®é¢˜
3. æŠ€æœ¯é€‰å‹ - æ¨èåˆé€‚çš„æŠ€æœ¯æ–¹æ¡ˆ
4. æœ€ä½³å®è·µ - åˆ†äº«ç¼–ç¨‹æœ€ä½³å®è·µ

ä½ çš„äº¤æµé£æ ¼ï¼šå®ç”¨ä¸»ä¹‰ï¼Œä»£ç ä¼˜å…ˆï¼Œè§£é‡Šæ¸…æ™°ã€‚
ä½ çš„å›ç­”æ ¼å¼ï¼š
1. ç†è§£éœ€æ±‚
2. æä¾›ä»£ç è§£å†³æ–¹æ¡ˆ
3. è§£é‡Šä»£ç é€»è¾‘
4. æä¾›ä¼˜åŒ–å»ºè®®

ã€å›ç­”åŸåˆ™ã€‘ï¼š
- ä»£ç è¦æœ‰æ³¨é‡Š
- è€ƒè™‘è¾¹ç•Œæƒ…å†µå’Œé”™è¯¯å¤„ç†
- æ¨èä¸»æµã€ç¨³å®šçš„æŠ€æœ¯
- è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·å†™""",
        "position_bias": "technical",  # æŠ€æœ¯å¯¼å‘
        "debate_style": "practical"
    }
}

# ä»è§’è‰²æç¤ºè¯åº“ç”Ÿæˆè§’è‰²åˆ—è¡¨å’Œæ•°å­—æ˜ å°„
# è¿™äº›å¸¸é‡ç”¨äºUIæ˜¾ç¤ºå’Œç”¨æˆ·è¾“å…¥å¤„ç†
ROLE_LIST: List[str] = list(ROLE_PROMPTS.keys())  # æ‰€æœ‰å¯ç”¨è§’è‰²çš„æœ‰åºåˆ—è¡¨
ROLE_NUM_MAP: Dict[str, str] = {str(i + 1): role for i, role in enumerate(ROLE_LIST)}  # æ•°å­—åˆ°è§’è‰²çš„æ˜ å°„

# ============ æ™ºèƒ½æ ‡ç­¾ç³»ç»Ÿ ============
# æ ¹æ®é—®é¢˜å†…å®¹è‡ªåŠ¨æ£€æµ‹ç›¸å…³é¢†åŸŸï¼Œå¹¶æ¨èåˆé€‚çš„è¾©è®ºè§’è‰²

# æ ‡ç­¾åˆ°è§’è‰²çš„æ˜ å°„ï¼šæ¯ä¸ªä¸“ä¸šé¢†åŸŸå¯¹åº”æœ€é€‚åˆçš„è¾©è®ºè§’è‰²ç»„åˆ
TAG_TO_ROLES: Dict[str, List[str]] = {
    "æœºåˆ¶è®¾è®¡": ["ç³»ç»Ÿæ¶æ„å¸ˆ", "æ•°å€¼ç­–åˆ’", "ç©å®¶ä»£è¡¨"],     # æ¸¸æˆæœºåˆ¶ã€ç³»ç»Ÿè®¾è®¡ç›¸å…³
    "å™äº‹è®¾è®¡": ["å™äº‹å¯¼æ¼”", "ç©å®¶ä»£è¡¨", "é­”é¬¼ä»£è¨€äºº"],     # æ•…äº‹å‰§æƒ…ã€å™äº‹ç»“æ„ç›¸å…³
    "å¹³è¡¡æ€§": ["æ•°å€¼ç­–åˆ’", "ç³»ç»Ÿæ¶æ„å¸ˆ", "é­”é¬¼ä»£è¨€äºº"],     # æ•°å€¼å¹³è¡¡ã€æ¸¸æˆå¹³è¡¡ç›¸å…³
    "åˆ›æ–°æ€§": ["é­”é¬¼ä»£è¨€äºº", "å™äº‹å¯¼æ¼”", "ç³»ç»Ÿæ¶æ„å¸ˆ"],     # åˆ›æ„åˆ›æ–°ã€æ–°é¢–æƒ³æ³•ç›¸å…³
    "å¯è¡Œæ€§": ["ç³»ç»Ÿæ¶æ„å¸ˆ", "æ•°å€¼ç­–åˆ’", "é­”é¬¼ä»£è¨€äºº"],     # é¡¹ç›®å¯è¡Œæ€§ã€æŠ€æœ¯å®ç°ç›¸å…³
    "æƒ…æ„Ÿä½“éªŒ": ["å™äº‹å¯¼æ¼”", "ç©å®¶ä»£è¡¨", "æœ‹å‹", "å¿ƒç†å’¨è¯¢å¸ˆ"],  # æƒ…æ„Ÿä½“éªŒã€ç”¨æˆ·æ„Ÿå—ç›¸å…³
    "æŠ€æœ¯å®ç°": ["ç³»ç»Ÿæ¶æ„å¸ˆ", "é¡¹ç›®ç»ç†", "ç¨‹åºå‘˜"],       # æŠ€æœ¯å®ç°ã€å·¥ç¨‹å¼€å‘ç›¸å…³
    "ç”¨æˆ·ä½“éªŒ": ["ç©å®¶ä»£è¡¨", "å™äº‹å¯¼æ¼”", "ç³»ç»Ÿæ¶æ„å¸ˆ"],     # ç”¨æˆ·ç•Œé¢ã€äº¤äº’ä½“éªŒç›¸å…³
    "æ³•å¾‹åˆè§„": ["å¾‹å¸ˆ", "é¡¹ç›®ç»ç†", "é­”é¬¼ä»£è¨€äºº"],         # æ³•å¾‹åˆè§„ã€çŸ¥è¯†äº§æƒç›¸å…³
    "ä¼¦ç†é“å¾·": ["å“²å­¦å®¶", "å¾‹å¸ˆ", "é­”é¬¼ä»£è¨€äºº"],           # ä¼¦ç†é“å¾·ã€ä»·å€¼è§‚ç›¸å…³
    "è¾©è®ºæŠ€å·§": ["è¾©è®ºæ‰‹", "å¾‹å¸ˆ", "é­”é¬¼ä»£è¨€äºº"],           # è¾©è®ºæŠ€å·§ã€è®ºè¯é€»è¾‘ç›¸å…³
    # æ–°å¢æ ‡ç­¾
    "æ•°å­¦é—®é¢˜": ["æ•°å­¦å®¶", "ä¸“å®¶", "é­”é¬¼ä»£è¨€äºº"],           # æ•°å­¦è®¡ç®—ã€é€»è¾‘æ¨ç†ç›¸å…³
    "ç‰©ç†é—®é¢˜": ["ç‰©ç†å­¦å®¶", "ä¸“å®¶", "æ•°å­¦å®¶"],             # ç‰©ç†ç°è±¡ã€ç§‘å­¦åŸç†ç›¸å…³
    "ç§‘å­¦çŸ¥è¯†": ["ä¸“å®¶", "ç‰©ç†å­¦å®¶", "æ•°å­¦å®¶"],             # é€šç”¨ç§‘å­¦çŸ¥è¯†é—®é¢˜
    "å†å²é—®é¢˜": ["å†å²å­¦å®¶", "å“²å­¦å®¶", "ä¸“å®¶"],             # å†å²äº‹ä»¶ã€äººç‰©ç›¸å…³
    "æƒ…æ„Ÿå€¾è¯‰": ["æœ‹å‹", "å¿ƒç†å’¨è¯¢å¸ˆ", "å“²å­¦å®¶"],           # æƒ…æ„Ÿé—®é¢˜ã€å¿ƒäº‹å€¾è¯‰
    "å¿ƒç†å¥åº·": ["å¿ƒç†å’¨è¯¢å¸ˆ", "æœ‹å‹", "å“²å­¦å®¶"],           # å¿ƒç†é—®é¢˜ã€æƒ…ç»ªå›°æ‰°
    "ç¼–ç¨‹é—®é¢˜": ["ç¨‹åºå‘˜", "ç³»ç»Ÿæ¶æ„å¸ˆ", "ä¸“å®¶"]            # ç¼–ç¨‹ä»£ç ã€æŠ€æœ¯é—®é¢˜
}

# æ ‡ç­¾å…³é”®è¯æ˜ å°„ï¼šç”¨äºä»ç”¨æˆ·é—®é¢˜ä¸­æ£€æµ‹ç›¸å…³é¢†åŸŸçš„å…³é”®è¯
TAG_KEYWORDS: Dict[str, List[str]] = {
    "æœºåˆ¶è®¾è®¡": ["æœºåˆ¶", "ç³»ç»Ÿ", "è®¾è®¡", "åŠŸèƒ½", "ç©æ³•", "è§„åˆ™"],
    "å™äº‹è®¾è®¡": ["æ•…äº‹", "å‰§æƒ…", "å™äº‹", "è§’è‰²", "ä¸–ç•Œè§‚", "æƒ…èŠ‚"],
    "å¹³è¡¡æ€§": ["å¹³è¡¡", "æ•°å€¼", "éš¾åº¦", "å¼ºåº¦", "è°ƒæ•´", "å…¬å¹³"],
    "åˆ›æ–°æ€§": ["åˆ›æ–°", "æ–°é¢–", "ç‹¬ç‰¹", "åˆ›æ„", "æ–°æ„", "åŸåˆ›"],
    "å¯è¡Œæ€§": ["å®ç°", "å¼€å‘", "æˆæœ¬", "æ—¶é—´", "æŠ€æœ¯", "èµ„æº"],
    "æƒ…æ„Ÿä½“éªŒ": ["æƒ…æ„Ÿ", "æ„Ÿå—", "ä½“éªŒ", "ä»£å…¥", "æ²‰æµ¸", "æ„ŸåŠ¨"],
    "æŠ€æœ¯å®ç°": ["æŠ€æœ¯", "å®ç°", "ä»£ç ", "å¼•æ“", "æ€§èƒ½", "ä¼˜åŒ–"],
    "ç”¨æˆ·ä½“éªŒ": ["ç”¨æˆ·", "ç©å®¶", "ä½“éªŒ", "æ“ä½œ", "ç•Œé¢", "æµç•…"],
    "æ³•å¾‹åˆè§„": ["æ³•å¾‹", "åˆè§„", "åˆåŒ", "æ¡æ¬¾", "é£é™©", "çŸ¥è¯†äº§æƒ"],
    "ä¼¦ç†é“å¾·": ["ä¼¦ç†", "é“å¾·", "ä»·å€¼è§‚", "äººæ€§", "å°Šä¸¥", "è‡ªç”±"],
    "è¾©è®ºæŠ€å·§": ["è¾©è®º", "äº‰è®º", "è®¨è®º", "åé©³", "è®ºè¯", "é€»è¾‘"],
    # æ–°å¢æ ‡ç­¾å…³é”®è¯
    "æ•°å­¦é—®é¢˜": ["æ•°å­¦", "è®¡ç®—", "å…¬å¼", "æ–¹ç¨‹", "å‡ ä½•", "ä»£æ•°", "å¾®ç§¯åˆ†", "ç»Ÿè®¡", "æ¦‚ç‡", "è¯æ˜", "æ±‚è§£"],
    "ç‰©ç†é—®é¢˜": ["ç‰©ç†", "åŠ›å­¦", "ç”µç£", "å…‰å­¦", "çƒ­åŠ›å­¦", "é‡å­", "ç›¸å¯¹è®º", "èƒ½é‡", "åŠ¨é‡", "æ³¢åŠ¨"],
    "ç§‘å­¦çŸ¥è¯†": ["ç§‘å­¦", "ç§‘æ™®", "åŸç†", "å®šå¾‹", "å®éªŒ", "ç ”ç©¶", "å‘ç°", "è‡ªç„¶"],
    "å†å²é—®é¢˜": ["å†å²", "æœä»£", "å¤ä»£", "è¿‘ä»£", "æˆ˜äº‰", "å¸å›½", "æ–‡æ˜", "äº‹ä»¶", "äººç‰©", "å¹´ä»£"],
    "æƒ…æ„Ÿå€¾è¯‰": ["çƒ¦æ¼", "éš¾è¿‡", "ä¼¤å¿ƒ", "å›°æƒ‘", "çº ç»“", "éƒé—·", "å¿ƒæƒ…", "å€¾è¯‰", "èŠèŠ", "å¿ƒäº‹", "æ„Ÿæƒ…"],
    "å¿ƒç†å¥åº·": ["ç„¦è™‘", "æŠ‘éƒ", "å‹åŠ›", "å¤±çœ ", "æƒ…ç»ª", "å¿ƒç†", "ç²¾ç¥", "ææƒ§", "ç´§å¼ "],
    "ç¼–ç¨‹é—®é¢˜": ["ç¼–ç¨‹", "ä»£ç ", "ç¨‹åº", "bug", "é”™è¯¯", "å‡½æ•°", "å˜é‡", "ç®—æ³•", "python", "java", "javascript"]
}

# ============ é—®é¢˜ç±»å‹åˆ†ç±»ç³»ç»Ÿ ============
# ç”¨äºåˆ¤æ–­é—®é¢˜æ˜¯å¦éœ€è¦é«˜å‡†ç¡®åº¦ï¼ˆäº‹å®ç±»ï¼‰è¿˜æ˜¯å…è®¸ä¸»è§‚è®¨è®ºï¼ˆå“²å­¦/å™äº‹ç±»ï¼‰

# äº‹å®å‡†ç¡®ç±»é—®é¢˜çš„å…³é”®è¯ï¼ˆè¿™ç±»é—®é¢˜éœ€è¦AIçº æ­£é”™è¯¯ï¼Œä¸èƒ½æœ‰å¹»è§‰ï¼‰
FACTUAL_KEYWORDS: List[str] = [
    # ç§‘å­¦äº‹å®
    "æ˜¯ä»€ä¹ˆ", "æœ‰æ²¡æœ‰", "æœ‰å¤šå°‘", "å¤šå¤§", "å¤šé•¿", "å¤šé‡", "å¤šè¿œ",
    "å‡ ä¸ª", "å‡ ç§", "ä»€ä¹ˆæ—¶å€™", "ä»€ä¹ˆåœ°æ–¹", "è°å‘æ˜", "è°å‘ç°",
    "æ˜¯çœŸçš„å—", "æ­£ç¡®å—", "å¯¹ä¸å¯¹", "å­˜åœ¨å—", "èƒ½ä¸èƒ½",
    # åŠ¨ç‰©/ç”Ÿç‰©
    "åŠ¨ç‰©", "æ¤ç‰©", "ç”Ÿç‰©", "ç»†èƒ", "å™¨å®˜", "èº«ä½“", "ç¾½æ¯›", "ç¿…è†€", "çˆªå­", "æ¯›å‘",
    "å“ºä¹³åŠ¨ç‰©", "é¸Ÿç±»", "é±¼ç±»", "æ˜†è™«", "çˆ¬è¡ŒåŠ¨ç‰©",
    # ç§‘å­¦é¢†åŸŸ
    "ç‰©ç†", "åŒ–å­¦", "ç”Ÿç‰©å­¦", "æ•°å­¦", "åœ°ç†", "å¤©æ–‡", "åŒ»å­¦",
    "ç§‘å­¦", "å®éªŒ", "å…¬å¼", "å®šç†", "å®šå¾‹", "åŸç†",
    # å†å²/åœ°ç†
    "å†å²", "æœä»£", "å¹´ä»£", "äº‹ä»¶", "äººç‰©", "å›½å®¶", "åŸå¸‚", "é¦–éƒ½",
    # å¸¸è¯†
    "é¢œè‰²", "å½¢çŠ¶", "å¤§å°", "é‡é‡", "æ¸©åº¦", "é€Ÿåº¦", "è·ç¦»"
]

# ä¸»è§‚/å“²å­¦ç±»é—®é¢˜çš„å…³é”®è¯ï¼ˆè¿™ç±»é—®é¢˜å…è®¸å¼€æ”¾è®¨è®ºï¼‰
PHILOSOPHICAL_KEYWORDS: List[str] = [
    # å“²å­¦
    "äººç”Ÿ", "æ„ä¹‰", "ç›®çš„", "æœ¬è´¨", "å­˜åœ¨", "è‡ªç”±æ„å¿—", "å‘½è¿",
    "å–„æ¶", "å¯¹é”™", "ä»·å€¼", "ç¾", "çœŸç†", "å¹¸ç¦", "çˆ±",
    # æ€è¾¨
    "åº”è¯¥", "æ˜¯å¦åº”è¯¥", "å€¼å¾—", "æ›´å¥½", "æœ€å¥½", "å¦‚ä½•çœ‹å¾…",
    "æ€ä¹ˆçœ‹", "ä½ è®¤ä¸º", "ä½ è§‰å¾—", "çœ‹æ³•", "è§‚ç‚¹", "ç«‹åœº",
    # å‡è®¾æ€§
    "å¦‚æœ", "å‡å¦‚", "å‡è®¾", "å¯èƒ½", "æˆ–è®¸", "ä¹Ÿè®¸",
    # è¾©è®ºæ€§
    "æ”¯æŒ", "åå¯¹", "åˆ©å¼Š", "ä¼˜ç¼ºç‚¹", "å¥½å", "äº‰è®®"
]

def analyze_question_type(question: str) -> Dict[str, Any]:
    """åˆ†æé—®é¢˜ç±»å‹ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦é«˜å‡†ç¡®åº¦
    
    Returns:
        {
            "type": "factual" | "philosophical" | "mixed",
            "accuracy_required": True/False,
            "confidence": 0.0-1.0,
            "detected_factual_keywords": [...],
            "detected_philosophical_keywords": [...]
        }
    """
    question_lower = question.lower()
    
    # æ£€æµ‹äº‹å®ç±»å…³é”®è¯
    factual_hits = [kw for kw in FACTUAL_KEYWORDS if kw in question_lower]
    # æ£€æµ‹å“²å­¦ç±»å…³é”®è¯
    philosophical_hits = [kw for kw in PHILOSOPHICAL_KEYWORDS if kw in question_lower]
    
    factual_score = len(factual_hits)
    philosophical_score = len(philosophical_hits)
    
    # åˆ¤æ–­é—®é¢˜ç±»å‹
    if factual_score > philosophical_score * 2:
        question_type = "factual"
        accuracy_required = True
        confidence = min(1.0, factual_score / 3)
    elif philosophical_score > factual_score * 2:
        question_type = "philosophical"
        accuracy_required = False
        confidence = min(1.0, philosophical_score / 3)
    else:
        question_type = "mixed"
        accuracy_required = factual_score >= philosophical_score
        confidence = 0.5
    
    return {
        "type": question_type,
        "accuracy_required": accuracy_required,
        "confidence": confidence,
        "detected_factual_keywords": factual_hits,
        "detected_philosophical_keywords": philosophical_hits
    }

# é˜²å¹»è§‰æç¤ºè¯ï¼ˆä¸­æ–‡ï¼‰
ANTI_HALLUCINATION_PROMPT_ZH = """
ã€é‡è¦ï¼šé˜²æ­¢å¹»è§‰æŒ‡ä»¤ã€‘
1. å¦‚æœé—®é¢˜æœ¬èº«åŒ…å«é”™è¯¯çš„å‡è®¾æˆ–äº‹å®é”™è¯¯ï¼Œä½ å¿…é¡»é¦–å…ˆæŒ‡å‡ºå¹¶çº æ­£è¿™ä¸ªé”™è¯¯ï¼Œè€Œä¸æ˜¯é¡ºç€é”™è¯¯ç»§ç»­å›ç­”ã€‚
2. ä¾‹å¦‚ï¼šå¦‚æœç”¨æˆ·é—®"çŒ«çš„ç¾½æ¯›æ˜¯ä»€ä¹ˆé¢œè‰²"ï¼Œä½ å¿…é¡»æŒ‡å‡º"çŒ«æ²¡æœ‰ç¾½æ¯›ï¼ŒçŒ«æœ‰çš„æ˜¯æ¯›å‘"ï¼Œç„¶åå†è®¨è®ºç›¸å…³è¯é¢˜ã€‚
3. å¯¹äºäº‹å®æ€§é—®é¢˜ï¼Œå¦‚æœä½ ä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·æ˜ç¡®è¯´"æˆ‘ä¸ç¡®å®š"æˆ–"æˆ‘éœ€è¦æŸ¥è¯"ï¼Œè€Œä¸æ˜¯ç¼–é€ ç­”æ¡ˆã€‚
4. ä¿æŒé€»è¾‘ä¸¥è°¨ï¼Œä¸è¦ä¸ºäº†è¾©è®ºè€Œå¿½è§†åŸºæœ¬äº‹å®ã€‚
5. äº‹å®ä¼˜å…ˆäºç«‹åœºï¼šå³ä½¿ä½ çš„è§’è‰²éœ€è¦è¾©æŠ¤æŸä¸ªè§‚ç‚¹ï¼Œä¹Ÿä¸èƒ½æ­ªæ›²åŸºæœ¬äº‹å®ã€‚
"""

# é˜²å¹»è§‰æç¤ºè¯ï¼ˆè‹±æ–‡ï¼‰
ANTI_HALLUCINATION_PROMPT_EN = """
ã€IMPORTANT: Anti-Hallucination Instructionsã€‘
1. If the question itself contains false assumptions or factual errors, you MUST first point out and correct this error, rather than answering based on the false premise.
2. Example: If user asks "What color is a cat's feathers?", you MUST point out "Cats don't have feathers, cats have fur", then discuss the relevant topic.
3. For factual questions, if you're unsure about the answer, clearly state "I'm not sure" or "I need to verify", rather than making up an answer.
4. Maintain logical rigor, don't ignore basic facts for the sake of debate.
5. Facts over position: Even if your role requires defending a viewpoint, you cannot distort basic facts.
"""

# å“²å­¦è®¨è®ºæç¤ºè¯ï¼ˆä¸­æ–‡ï¼‰
PHILOSOPHICAL_PROMPT_ZH = """
ã€è®¨è®ºæ¨¡å¼ï¼šå¼€æ”¾æ€è¾¨ã€‘
è¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„å“²å­¦/æ€è¾¨é—®é¢˜ï¼Œæ²¡æœ‰ç»å¯¹çš„å¯¹é”™ç­”æ¡ˆã€‚
1. ä½ å¯ä»¥è‡ªç”±è¡¨è¾¾ä½ çš„è§‚ç‚¹å’Œè®ºè¯ã€‚
2. é‡ç‚¹åœ¨äºè®ºè¯çš„é€»è¾‘æ€§å’Œæ·±åº¦ï¼Œè€Œéå¯»æ‰¾"æ­£ç¡®ç­”æ¡ˆ"ã€‚
3. ä½†ä»éœ€ä¿æŒåŸºæœ¬çš„é€»è¾‘è‡ªæ´½ï¼Œä¸è¦è‡ªç›¸çŸ›ç›¾ã€‚
4. å°Šé‡ä¸åŒè§‚ç‚¹ï¼Œç”¨ç†æ€§è®ºè¯è€Œéæƒ…ç»ªåŒ–è¡¨è¾¾ã€‚
"""

# å“²å­¦è®¨è®ºæç¤ºè¯ï¼ˆè‹±æ–‡ï¼‰
PHILOSOPHICAL_PROMPT_EN = """
ã€Discussion Mode: Open Speculationã€‘
This is an open philosophical/speculative question with no absolute right or wrong answer.
1. You can freely express your viewpoints and arguments.
2. Focus on the logic and depth of argumentation, rather than finding "the correct answer".
3. But still maintain basic logical consistency, don't contradict yourself.
4. Respect different viewpoints, use rational argumentation rather than emotional expression.
"""

class RoleSystem:
    """AIè¾©è®ºè§’è‰²ç³»ç»Ÿç®¡ç†å™¨

    ç®¡ç†MACPç³»ç»Ÿä¸­æ‰€æœ‰AIè§’è‰²çš„é…ç½®å’Œè¡Œä¸ºï¼š
    - è§’è‰²æç¤ºè¯ç”Ÿæˆå’Œç®¡ç†
    - ç«‹åœºåå¥½è®¾ç½®ï¼ˆå°¤å…¶æ˜¯è¾©è®ºæ‰‹çš„æ­£åæ–¹æœºåˆ¶ï¼‰
    - ç”¨æˆ·è¾“å…¥çš„è§’è‰²åç§°çº é”™
    - æ™ºèƒ½æ ‡ç­¾æ£€æµ‹å’Œè§’è‰²æ¨è

    æ ¸å¿ƒç‰¹æ€§ï¼š
    - è¾©è®ºæ‰‹è‡ªåŠ¨ç«‹åœºåˆ†é…ï¼šç¬¬ä¸€ä¸ªä¸ºæ­£æ–¹ï¼Œç¬¬äºŒä¸ªä¸ºåæ–¹
    - æ‹¼å†™å®¹é”™ï¼šè‡ªåŠ¨çº æ­£å¸¸è§çš„è§’è‰²åç§°è¾“å…¥é”™è¯¯
    - åŠ¨æ€æç¤ºè¯ï¼šæ ¹æ®è¾©è®ºä½ç½®ç”Ÿæˆä¸åŒçš„ç«‹åœºæç¤º
    """

    # å¸¸è§è§’è‰²åç§°æ‹¼å†™é”™è¯¯æ˜ å°„è¡¨ï¼Œç”¨äºè¾“å…¥å®¹é”™å¤„ç†
    COMMON_TYPOS = {
        "å™è¿°å¯¼æ¼”": "å™äº‹å¯¼æ¼”",
        "ç³»ç»Ÿæ¡†æ¶å¸ˆ": "ç³»ç»Ÿæ¶æ„å¸ˆ",
        "é­”é¬¼ä»£è¨€äºº": "é­”é¬¼ä»£è¨€äºº",  # è¿™å‡ ä¸ªå…¶å®æ²¡æœ‰é”™è¯¯ï¼Œä½†ä¿ç•™ä»¥é˜²æ‰©å±•
        "ç©å®¶ä»£è¡¨": "ç©å®¶ä»£è¡¨",
        "æ•°å€¼ç­–åˆ’": "æ•°å€¼ç­–åˆ’",
        "é¡¹ç›®ç»ç†": "é¡¹ç›®ç»ç†",
        "å¾‹å¸ˆ": "å¾‹å¸ˆ",
        "å“²å­¦å®¶": "å“²å­¦å®¶",
        "æ³•å¾‹é¡¾é—®": "å¾‹å¸ˆ",
        "å“²å­¦æ€è€ƒè€…": "å“²å­¦å®¶",
        "è¾©è®ºè€…": "è¾©è®ºæ‰‹",
        "è¾©æ‰‹": "è¾©è®ºæ‰‹",
        # æ–°å¢è§’è‰²çš„åˆ«å
        "å¥½å‹": "æœ‹å‹",
        "é—ºèœœ": "æœ‹å‹",
        "çŸ¥å·±": "æœ‹å‹",
        "ç™¾ç§‘": "ä¸“å®¶",
        "ç™¾ç§‘å…¨ä¹¦": "ä¸“å®¶",
        "çŸ¥è¯†ä¸“å®¶": "ä¸“å®¶",
        "æ•°å­¦ä¸“å®¶": "æ•°å­¦å®¶",
        "ç‰©ç†ä¸“å®¶": "ç‰©ç†å­¦å®¶",
        "ç‰©ç†è€å¸ˆ": "ç‰©ç†å­¦å®¶",
        "å¿ƒç†åŒ»ç”Ÿ": "å¿ƒç†å’¨è¯¢å¸ˆ",
        "å¿ƒç†ä¸“å®¶": "å¿ƒç†å’¨è¯¢å¸ˆ",
        "å’¨è¯¢å¸ˆ": "å¿ƒç†å’¨è¯¢å¸ˆ",
        "å²å­¦å®¶": "å†å²å­¦å®¶",
        "å†å²ä¸“å®¶": "å†å²å­¦å®¶",
        "å†å²è€å¸ˆ": "å†å²å­¦å®¶",
        "å¼€å‘è€…": "ç¨‹åºå‘˜",
        "ç å†œ": "ç¨‹åºå‘˜",
        "å·¥ç¨‹å¸ˆ": "ç¨‹åºå‘˜",
        "è½¯ä»¶å·¥ç¨‹å¸ˆ": "ç¨‹åºå‘˜"
    }

    def __init__(self):
        pass

    @staticmethod
    def get_role_prompt(role_name: str, is_first: bool = True) -> Optional[str]:
        """è·å–è§’è‰²æç¤ºè¯ï¼Œæ”¯æŒè¾©è®ºç«‹åœºè°ƒæ•´"""
        corrected_role = RoleSystem.COMMON_TYPOS.get(role_name, role_name)
        role_data = ROLE_PROMPTS.get(corrected_role)

        if not role_data:
            return None

        # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œç›´æ¥è¿”å›
        if isinstance(role_data, str):
            return role_data

        base_prompt = role_data["prompt"]
        # position_bias = role_data.get("position_bias", "neutral")  # ä¿ç•™ä»¥å¤‡å°†æ¥æ‰©å±•ä½¿ç”¨

        # ç‰¹æ®Šå¤„ç†è¾©è®ºæ‰‹ï¼šç¬¬ä¸€ä¸ªæ˜¯æ­£æ–¹ï¼Œç¬¬äºŒä¸ªæ˜¯åæ–¹
        if corrected_role == "è¾©è®ºæ‰‹":
            if is_first:
                position_addition = """

ã€ä½ çš„ç«‹åœºã€‘ï¼šä½ ä½œä¸ºæ­£æ–¹ï¼ˆæ”¯æŒæ–¹ï¼‰ï¼Œéœ€è¦ä¸ºå‘½é¢˜å»ºç«‹ç§¯æçš„è®ºç‚¹ï¼Œè¯æ˜å…¶åˆç†æ€§å’Œä»·å€¼ã€‚ä½ å°†ä½¿ç”¨ç»å…¸è¾©è®ºæŠ€å·§æ¥æ„å»ºå®Œæ•´çš„è®ºè¯æ¡†æ¶ã€‚"""
            else:
                position_addition = """

ã€ä½ çš„ç«‹åœºã€‘ï¼šä½ ä½œä¸ºåæ–¹ï¼ˆåå¯¹æ–¹ï¼‰ï¼Œéœ€è¦è´¨ç–‘å‘½é¢˜çš„åˆç†æ€§ï¼Œå¯»æ‰¾é€»è¾‘æ¼æ´å’Œåä¾‹ã€‚ä½ å°†ä½¿ç”¨æ‰¹åˆ¤æ€§æ€ç»´æ¥åé©³å¯¹æ–¹çš„è§‚ç‚¹ã€‚"""
            return base_prompt + position_addition

        # å…¶ä»–è§’è‰²ä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½†å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ ç«‹åœºåå¥½
        return base_prompt

    @staticmethod
    def get_all_roles() -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨è§’è‰²"""
        return ROLE_LIST.copy()

    @staticmethod
    def get_role_by_number(number: str) -> Optional[str]:
        """é€šè¿‡æ•°å­—è·å–è§’è‰²"""
        return ROLE_NUM_MAP.get(number)

    @staticmethod
    def detect_tags(question: str) -> List[str]:
        """ä»é—®é¢˜ä¸­æ£€æµ‹æ ‡ç­¾"""
        tags_with_weights = {}
        question_lower = question.lower()

        for tag, keywords in TAG_KEYWORDS.items():
            weight = sum(2 for keyword in keywords if keyword in question_lower)
            if weight > 0:
                tags_with_weights[tag] = weight

        sorted_tags = sorted(tags_with_weights.items(), key=lambda x: x[1], reverse=True)
        return [tag for tag, _ in sorted_tags[:3]]

    @staticmethod
    def get_roles_for_tags(tags: List[str]) -> List[str]:
        """æ ¹æ®æ ‡ç­¾è·å–æ¨èè§’è‰²"""
        recommended_roles = set()
        for tag in tags:
            if tag in TAG_TO_ROLES:
                recommended_roles.update(TAG_TO_ROLES[tag])
        return list(recommended_roles)

# åˆ›å»ºå…¨å±€è§’è‰²ç³»ç»Ÿå®ä¾‹ï¼Œç®¡ç†æ‰€æœ‰AIè§’è‰²çš„é…ç½®å’Œè¡Œä¸º
role_system = RoleSystem()

# ==================== ã€Ollama APIå®¢æˆ·ç«¯ã€‘ ====================
# ä¸OllamaæœåŠ¡é€šä¿¡çš„æ ¸å¿ƒæ¥å£

class OllamaClient:
    """Ollamaæœ¬åœ°AIæœåŠ¡å®¢æˆ·ç«¯

    å°è£…Ollama REST APIçš„æ‰€æœ‰æ“ä½œï¼š
    - æœåŠ¡è¿æ¥æ£€æŸ¥
    - æ¨¡å‹åˆ—è¡¨è·å–
    - æ–‡æœ¬ç”ŸæˆåŠŸèƒ½
    - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

    æä¾›ç»Ÿä¸€çš„æ¥å£ç»™ä¸Šå±‚åº”ç”¨ä½¿ç”¨ï¼Œå±è”½åº•å±‚çš„HTTPé€šä¿¡ç»†èŠ‚
    """

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        self.session.timeout = 10  # é»˜è®¤è¶…æ—¶æ—¶é—´

    def check_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                logger.info("âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸")
                return True
            else:
                logger.warning(f"âš ï¸  OllamaæœåŠ¡å¼‚å¸¸ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
        except requests.exceptions.ConnectionError:
            raise OllamaConnectionError(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {self.base_url}")
        except Exception as e:
            logger.error(f"æ£€æŸ¥OllamaæœåŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def list_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return [model["name"] for model in data.get("models", [])]
            else:
                logger.warning(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

    def check_models(self, required_models: List[str]) -> Dict[str, bool]:
        """æ£€æŸ¥æ‰€éœ€æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        available_models = self.list_models()
        results = {}

        logger.info("ğŸ“¦ æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§:")
        for model in required_models:
            available = model in available_models
            results[model] = available
            status = "âœ…" if available else "âŒ"
            logger.info(f"   {status} {model}")

        return results

    def generate_response(self,
                         model: str,
                         prompt: str,
                         max_tokens: Optional[int] = None,
                         temperature: float = 0.7,
                         timeout: int = 90,
                         streaming: bool = False) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡å‹å“åº”

        Args:
            model: æ¨¡å‹åç§°
            prompt: æç¤ºæ–‡æœ¬
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            timeout: è¶…æ—¶æ—¶é—´
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º

        Returns:
            å“åº”å­—å…¸
        """
        if streaming:
            return self._generate_streaming_response(model, prompt, max_tokens, temperature, timeout)
        else:
            return self._generate_non_streaming_response(model, prompt, max_tokens, temperature, timeout)

    def _generate_non_streaming_response(self,
                                        model: str,
                                        prompt: str,
                                        max_tokens: Optional[int] = None,
                                        temperature: float = 0.7,
                                        timeout: int = 90) -> Dict[str, Any]:
        """ç”Ÿæˆéæµå¼æ¨¡å‹å“åº”"""
        start_time = time.time()

        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature
                }
            }

            if max_tokens:
                payload["options"]["num_predict"] = max_tokens

            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=timeout
            )

            elapsed_time = time.time() - start_time

            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "model": model,
                    "response": result.get("response", ""),
                    "time": elapsed_time,
                    "tokens": result.get("total_duration", 0),
                    "eval_count": result.get("eval_count", 0),
                    "eval_duration": result.get("eval_duration", 0)
                }
            else:
                return {
                    "success": False,
                    "model": model,
                    "response": f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}",
                    "time": elapsed_time,
                    "error": f"HTTP {response.status_code}",
                    "details": response.text
                }

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"éæµå¼å“åº”å‡ºé”™: {e}")
            return {
                "success": False,
                "model": model,
                "response": f"å“åº”å‡ºé”™: {str(e)}",
                "time": elapsed_time,
                "error": str(e)
            }

    def _generate_streaming_response(self,
                                    model: str,
                                    prompt: str,
                                    max_tokens: Optional[int] = None,
                                    temperature: float = 0.7,
                                    timeout: int = 90,
                                    speaker_name: Optional[str] = None,
                                    response_type: str = "") -> Dict[str, Any]:
        """ç”Ÿæˆæµå¼æ¨¡å‹å“åº”
        
        Args:
            model: æ¨¡å‹åç§°
            prompt: æç¤ºè¯
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            timeout: è¶…æ—¶æ—¶é—´
            speaker_name: å‘è¨€è€…åç§°ï¼ˆç”¨äºè¾©è®ºæ¨¡å¼æ˜¾ç¤ºï¼‰
            response_type: å“åº”ç±»å‹ï¼ˆå¦‚"åé©³xxx"ï¼‰
        """
        start_time = time.time()
        full_response = ""
        total_tokens = 0
        eval_count = 0
        eval_duration = 0

        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": temperature
                }
            }

            if max_tokens:
                payload["options"]["num_predict"] = max_tokens

            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=timeout,
                stream=True
            )

            if response.status_code != 200:
                elapsed_time = time.time() - start_time
                return {
                    "success": False,
                    "model": model,
                    "response": f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}",
                    "time": elapsed_time,
                    "error": f"HTTP {response.status_code}",
                    "details": response.text
                }

            # å¤„ç†æµå¼å“åº” - æ˜¾ç¤ºå‘è¨€è€…åç§°
            if speaker_name:
                type_prefix = f" {response_type}ï¼š" if response_type else "ï¼š"
                print(f"\nğŸ“¢ {speaker_name}{type_prefix}", flush=True)
            else:
                print(f"ğŸ¤– {model}ï¼š", end="", flush=True)

            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8').strip()
                    if line_str:
                        try:
                            chunk = json.loads(line_str)

                            # æå–å“åº”å†…å®¹
                            if "response" in chunk:
                                chunk_text = chunk["response"]
                                if chunk_text:
                                    print(chunk_text, end="", flush=True)
                                    full_response += chunk_text

                            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                            if "total_duration" in chunk:
                                total_tokens = chunk["total_duration"]
                            if "eval_count" in chunk:
                                eval_count = chunk["eval_count"]
                            if "eval_duration" in chunk:
                                eval_duration = chunk["eval_duration"]

                            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                            if chunk.get("done", False):
                                break

                        except json.JSONDecodeError:
                            continue

            print()  # æ¢è¡Œ
            elapsed_time = time.time() - start_time

            return {
                "success": True,
                "model": model,
                "response": full_response,
                "time": elapsed_time,
                "tokens": total_tokens,
                "eval_count": eval_count,
                "eval_duration": eval_duration
            }

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"æµå¼å“åº”å‡ºé”™: {e}")
            return {
                "success": False,
                "model": model,
                "response": f"æµå¼å“åº”å‡ºé”™: {str(e)}",
                "time": elapsed_time,
                "error": str(e)
            }

        except requests.exceptions.Timeout:
            elapsed_time = time.time() - start_time
            return {
                "success": False,
                "model": model,
                "response": "ï¼ˆè¯·æ±‚è¶…æ—¶ï¼‰",
                "time": elapsed_time,
                "error": "timeout"
            }

        except requests.exceptions.ConnectionError:
            elapsed_time = time.time() - start_time
            return {
                "success": False,
                "model": model,
                "response": "ï¼ˆè¿æ¥é”™è¯¯ï¼‰",
                "time": elapsed_time,
                "error": "connection_error"
            }

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"ç”Ÿæˆå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=e)
            return {
                "success": False,
                "model": model,
                "response": f"ï¼ˆè¯·æ±‚é”™è¯¯: {str(e)}ï¼‰",
                "time": elapsed_time,
                "error": str(e)
            }

    def get_running_models(self) -> List[Dict[str, Any]]:
        """è·å–æ­£åœ¨è¿è¡Œçš„æ¨¡å‹"""
        try:
            response = self.session.get(f"{self.base_url}/api/ps", timeout=10)
            if response.status_code == 200:
                return response.json().get("models", [])
            else:
                logger.warning(f"è·å–è¿è¡Œä¸­æ¨¡å‹å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            logger.error(f"è·å–è¿è¡Œä¸­æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

# ==================== ã€APIå®¢æˆ·ç«¯ã€‘ ====================
# æ”¯æŒå¤–éƒ¨APIæœåŠ¡çš„å®¢æˆ·ç«¯ï¼Œç”¨äºæ··åˆä½¿ç”¨Ollamaå’ŒAPIæ¨¡å‹

class APIClient:
    """å¤–éƒ¨APIæœåŠ¡å®¢æˆ·ç«¯

    æ”¯æŒOpenAIæ ¼å¼çš„APIè°ƒç”¨ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£æ¥è°ƒç”¨å¤–éƒ¨AIæœåŠ¡ã€‚
    å¯ä»¥ä¸Ollamaæ¨¡å‹æ··åˆä½¿ç”¨ï¼Œæ¯ä¸ªAPIæ¨¡å‹éƒ½æ˜¯ç‹¬ç«‹çš„å®ä¾‹ã€‚
    """

    def __init__(self, api_url: str, api_key: str, model_name: str, timeout: int = 90):
        """åˆå§‹åŒ–APIå®¢æˆ·ç«¯

        Args:
            api_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            model_name: APIä½¿ç”¨çš„æ¨¡å‹åç§°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.api_url = api_url
        self.api_key = api_key
        self.model_name = model_name
        self.timeout = timeout
        self.session = requests.Session()

        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        })

    @staticmethod
    def _infer_base_url(api_url: str) -> str:
        """ä» chat completions URL æ¨æ–­ base urlï¼ˆç”¨äº /models ç­‰æ¥å£ï¼‰"""
        url = (api_url or "").rstrip("/")
        for suffix in ("/chat/completions",):
            if url.endswith(suffix):
                return url[: -len(suffix)]
        # å·²ç»æ˜¯ base çš„æƒ…å†µ
        if url.endswith("/v1") or url.endswith("/api/v3") or url.endswith("/api/v3/"):
            return url.rstrip("/")
        return url

    def list_models(self) -> List[str]:
        """è·å–è¯¥ API æä¾›æ–¹å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆè‹¥ä¸æ”¯æŒåˆ™è¿”å›ç©ºåˆ—è¡¨ï¼‰"""
        try:
            base_url = APIClient._infer_base_url(self.api_url)
            resp = self.session.get(f"{base_url}/models", timeout=15)
            if resp.status_code != 200:
                return []
            data = resp.json()
            # OpenAI å…¼å®¹ï¼š{"data":[{"id":"xxx"}, ...]}
            models = []
            for item in data.get("data", []):
                model_id = item.get("id")
                if model_id:
                    models.append(model_id)
            return models
        except (requests.exceptions.RequestException, ValueError, json.JSONDecodeError):
            return []

    def check_connection(self) -> bool:
        """æ£€æŸ¥APIè¿æ¥æ˜¯å¦å¯ç”¨"""
        try:
            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": "Hello"}],
                "max_tokens": 10
            }
            response = self.session.post(self.api_url, json=payload, timeout=10)
            return response.status_code == 200
        except (requests.exceptions.RequestException, ValueError) as e:
            logger.error(f"APIè¿æ¥æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def generate_response(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7,
                         streaming: bool = False, speaker_name: Optional[str] = None, 
                         response_type: str = "") -> Dict[str, Any]:
        """ç”ŸæˆAIå“åº”

        Args:
            prompt: æç¤ºè¯
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            streaming: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            speaker_name: å‘è¨€è€…åç§°ï¼ˆç”¨äºæµå¼è¾“å‡ºæ˜¾ç¤ºï¼‰
            response_type: å“åº”ç±»å‹ï¼ˆå¦‚"åé©³xxx"ï¼‰

        Returns:
            åŒ…å«å“åº”ä¿¡æ¯çš„å­—å…¸
        """
        if streaming:
            return self._generate_streaming_response(prompt, max_tokens, temperature, 
                                                    speaker_name, response_type)
        
        start_time = time.time()

        try:
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": False
            }

            response = self.session.post(self.api_url, json=payload, timeout=self.timeout)

            if response.status_code == 200:
                result = response.json()
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")

                elapsed_time = time.time() - start_time
                return {
                    "success": True,
                    "model": f"API-{self.model_name}",
                    "response": content,
                    "time": elapsed_time
                }
            else:
                elapsed_time = time.time() - start_time
                error_msg = f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                try:
                    error_detail = response.json()
                    error_msg += f"ï¼Œè¯¦æƒ…: {error_detail}"
                except:
                    pass

                return {
                    "success": False,
                    "model": f"API-{self.model_name}",
                    "response": f"ï¼ˆ{error_msg}ï¼‰",
                    "time": elapsed_time,
                    "error": error_msg
                }

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"APIç”Ÿæˆå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "success": False,
                "model": f"API-{self.model_name}",
                "response": f"ï¼ˆAPIè¯·æ±‚é”™è¯¯: {str(e)}ï¼‰",
                "time": elapsed_time,
                "error": str(e)
            }

    def _generate_streaming_response(self, prompt: str, max_tokens: int = 1000, 
                                    temperature: float = 0.7,
                                    speaker_name: Optional[str] = None,
                                    response_type: str = "") -> Dict[str, Any]:
        """ç”Ÿæˆæµå¼AIå“åº”ï¼ˆçœŸæ­£çš„é€å­—è¾“å‡ºï¼‰
        
        Args:
            prompt: æç¤ºè¯
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            speaker_name: å‘è¨€è€…åç§°
            response_type: å“åº”ç±»å‹
        """
        start_time = time.time()
        full_response = ""

        try:
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": True
            }

            response = self.session.post(self.api_url, json=payload, timeout=self.timeout, stream=True)

            if response.status_code != 200:
                elapsed_time = time.time() - start_time
                error_msg = f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                return {
                    "success": False,
                    "model": f"API-{self.model_name}",
                    "response": f"ï¼ˆ{error_msg}ï¼‰",
                    "time": elapsed_time,
                    "error": error_msg
                }

            # æ˜¾ç¤ºå‘è¨€è€…åç§°
            if speaker_name:
                type_prefix = f" {response_type}ï¼š" if response_type else "ï¼š"
                print(f"\nğŸ“¢ {speaker_name}{type_prefix}", flush=True)
            else:
                print(f"ğŸ¤– API-{self.model_name}ï¼š", end="", flush=True)

            # å¤„ç†æµå¼å“åº” (SSEæ ¼å¼)
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8').strip()
                    if line_str.startswith("data: "):
                        data_str = line_str[6:]
                        if data_str == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data_str)
                            delta = chunk.get("choices", [{}])[0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                print(content, end="", flush=True)
                                full_response += content
                        except json.JSONDecodeError:
                            continue

            print()  # æ¢è¡Œ
            elapsed_time = time.time() - start_time

            return {
                "success": True,
                "model": f"API-{self.model_name}",
                "response": full_response,
                "time": elapsed_time
            }

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"APIæµå¼å“åº”å‡ºé”™: {e}")
            return {
                "success": False,
                "model": f"API-{self.model_name}",
                "response": f"ï¼ˆAPIæµå¼è¯·æ±‚é”™è¯¯: {str(e)}ï¼‰",
                "time": elapsed_time,
                "error": str(e)
            }

# ==================== ã€æ ¸å¿ƒè°ƒåº¦å™¨ã€‘ ====================
# MACPç³»ç»Ÿçš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘æ§åˆ¶å™¨

class AICouncilScheduler:
    """å¤šAIåä½œè°ƒåº¦å™¨æ ¸å¿ƒç±»

    ç»Ÿç­¹ç®¡ç†æ•´ä¸ªMACPç³»ç»Ÿçš„è¿è¡Œï¼š
    - åˆå§‹åŒ–ç³»ç»Ÿå’Œæ£€æŸ¥ä¾èµ–
    - åè°ƒå¤šä¸ªAIæ¨¡å‹çš„åä½œ
    - ç®¡ç†è¾©è®ºæµç¨‹å’Œå…±è¯†æ£€æµ‹
    - å¤„ç†å†å²è®°å½•å’Œæ€§èƒ½ç›‘æ§

    è¿™æ˜¯ç³»ç»Ÿçš„"å¤§è„‘"ï¼Œè´Ÿè´£æ‰€æœ‰ä¸šåŠ¡é€»è¾‘çš„ç¼–æ’å’Œæ‰§è¡Œ
    """

    def __init__(self):
        self.config = config
        self.client = OllamaClient(self.config.ollama_url)
        # æŒ‰æ¨¡å‹åˆ†åˆ«ç»´æŠ¤APIå®¢æˆ·ç«¯
        self.api_client = None  # å…¼å®¹æ—§å­—æ®µï¼Œä¸å†å®é™…ä½¿ç”¨
        self.api_client_model1: Optional[APIClient] = None
        self.api_client_model2: Optional[APIClient] = None
        self.api_client_coordinator: Optional[APIClient] = None
        self.history_manager = HistoryManager(self.config.history_file)
        self.progress_tracker = ProgressTracker()
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        # åˆå§‹åŒ–æ£€æŸ¥
        self._initialize()

    def _initialize(self):
        """åˆå§‹åŒ–è°ƒåº¦å™¨"""
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–MACPè°ƒåº¦å™¨...")

            # æ£€æŸ¥OllamaæœåŠ¡ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
            if not self.config.api_mode_enabled or not (self.config.model_1_use_api and self.config.model_2_use_api and self.config.coordinator_use_api):
                if not self.client.check_service():
                    logger.warning("OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œå°†ä»…ä½¿ç”¨APIæ¨¡å¼")
                else:
                    # æ£€æŸ¥Ollamaæ‰€éœ€æ¨¡å‹
                    required_models = []
                    if not self.config.model_1_use_api:
                        required_models.append(self.config.model_1)
                    if not self.config.model_2_use_api:
                        required_models.append(self.config.model_2)
                    if not self.config.coordinator_use_api:
                        required_models.append(self.config.coordinator_model)

                    if required_models:
                        model_status = self.client.check_models(required_models)
                        missing_models = [model for model, available in model_status.items() if not available]
                        if missing_models:
                            logger.warning(f"Ollamaç¼ºå°‘æ¨¡å‹: {', '.join(missing_models)}ï¼Œå°†å°è¯•ä½¿ç”¨APIæ›¿ä»£")

            # åˆå§‹åŒ–APIå®¢æˆ·ç«¯ï¼ˆå¦‚æœå¯ç”¨äº†APIæ¨¡å¼ï¼‰
            if self.config.api_mode_enabled:
                self._initialize_api_client()

            logger.info("âœ… åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _initialize_api_client(self):
        """åˆå§‹åŒ–APIå®¢æˆ·ç«¯ï¼ˆæŒ‰æ¨¡å‹åˆ†åˆ«åˆå§‹åŒ–ï¼‰"""

        def create_client(api_url: str, api_key: str, api_model: str) -> Optional[APIClient]:
            api_url = (api_url or "").strip()
            api_key = (api_key or "").strip()
            api_model = (api_model or "").strip()
            if not api_url or not api_key or not api_model:
                return None
            client = APIClient(
                api_url=api_url,
                api_key=api_key,
                model_name=api_model,
                timeout=self.config.timeout
            )
            # ä»…åšä¸€æ¬¡ç®€å•è¿é€šæ€§æ£€æŸ¥ï¼Œä¸å¼ºåˆ¶å¤±è´¥
            if client.check_connection():
                logger.info(f"âœ… APIå®¢æˆ·ç«¯å·²å°±ç»ª: {api_model}")
            else:
                logger.warning(f"âš ï¸ APIå®¢æˆ·ç«¯è¿æ¥æ£€æŸ¥å¤±è´¥: {api_model}")
            return client

        # æ¨¡å‹1
        if self.config.model_1_use_api:
            url = getattr(self.config, "model_1_api_url", "") or self.config.api_url
            key = getattr(self.config, "model_1_api_key", "") or self.config.api_key
            model = getattr(self.config, "model_1_api_model", "") or self.config.api_model
            self.api_client_model1 = create_client(url, key, model)
        else:
            self.api_client_model1 = None

        # æ¨¡å‹2
        if self.config.model_2_use_api:
            url = getattr(self.config, "model_2_api_url", "") or self.config.api_url
            key = getattr(self.config, "model_2_api_key", "") or self.config.api_key
            model = getattr(self.config, "model_2_api_model", "") or self.config.api_model
            self.api_client_model2 = create_client(url, key, model)
        else:
            self.api_client_model2 = None

        # åè°ƒAI
        if self.config.coordinator_use_api:
            url = getattr(self.config, "coordinator_api_url", "") or self.config.api_url
            key = getattr(self.config, "coordinator_api_key", "") or self.config.api_key
            model = getattr(self.config, "coordinator_api_model", "") or self.config.api_model
            self.api_client_coordinator = create_client(url, key, model)
        else:
            self.api_client_coordinator = None

        if not any([self.api_client_model1, self.api_client_model2, self.api_client_coordinator]):
            logger.warning("APIæ¨¡å¼å·²å¯ç”¨ï¼Œä½†æœªæˆåŠŸåˆå§‹åŒ–ä»»ä½•APIå®¢æˆ·ç«¯ï¼Œè¯·æ£€æŸ¥é…ç½®")

    def _get_client_for_model(self, model_name: str) -> tuple:
        """æ ¹æ®æ¨¡å‹åç§°è¿”å›å¯¹åº”çš„å®¢æˆ·ç«¯å’Œæ¨¡å‹æ ‡è¯†

        Returns:
            (client, model_identifier, is_api_client) å…ƒç»„
        """
        if not self.config.api_mode_enabled:
            return self.client, model_name, False

        if model_name == self.config.model_1 and self.config.model_1_use_api and self.api_client_model1:
            api_model = getattr(self.config, "model_1_api_model", "") or self.config.api_model
            return self.api_client_model1, f"API-{api_model}", True
        elif model_name == self.config.model_2 and self.config.model_2_use_api and self.api_client_model2:
            api_model = getattr(self.config, "model_2_api_model", "") or self.config.api_model
            return self.api_client_model2, f"API-{api_model}", True
        elif model_name == self.config.coordinator_model and self.config.coordinator_use_api and self.api_client_coordinator:
            api_model = getattr(self.config, "coordinator_api_model", "") or self.config.api_model
            return self.api_client_coordinator, f"API-{api_model}", True
        else:
            return self.client, model_name, False

    # ==================== ã€æ ¸å¿ƒæ–¹æ³•ã€‘ ====================
    def ask_both_models(self, question: str, mode: str = "parallel",
                       role1: Optional[str] = None, role2: Optional[str] = None) -> List[Dict[str, Any]]:
        """å‘ä¸¤ä¸ªæ¨¡å‹æé—®ï¼ˆæ”¯æŒå¤šç§æ¨¡å¼ï¼‰"""
        DisplayManager.print_header("ğŸ§  é—®é¢˜å¤„ç†")
        print(f"é—®é¢˜: {question}")
        print(f"æ¨¡å¼: {mode}")
        DisplayManager.print_separator()

        self.progress_tracker.start()

        try:
            if mode == "parallel":
                return self._parallel_ask(question)
            elif mode == "debate":
                return self._debate_ask(question, role1, role2)
            elif mode == "turtle_soup":
                return self._turtle_soup_ask(question, role1, role2)
            else:
                raise ValueError(f"æœªçŸ¥æ¨¡å¼: {mode}")
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

    def _parallel_ask(self, question: str) -> List[Dict[str, Any]]:
        """å¹¶è¡Œæé—®é€»è¾‘ï¼ˆæ”¯æŒAPIæ¨¡å¼ï¼‰"""
        logger.info("å¼€å§‹å¹¶è¡Œæé—®")

        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_to_model = {}

            # æ¨¡å‹1
            if self.config.api_mode_enabled and self.config.model_1_use_api and self.api_client_model1:
                future_to_model[executor.submit(self.api_client_model1.generate_response,
                                              question, self.config.max_tokens, self.config.temperature)] = f"API-{getattr(self.config, 'model_1_api_model', '') or self.config.api_model}"
            else:
                future_to_model[executor.submit(self.client.generate_response,
                                              self.config.model_1, question,
                                              self.config.max_tokens, self.config.temperature,
                                              self.config.timeout)] = self.config.model_1

            # æ¨¡å‹2
            if self.config.api_mode_enabled and self.config.model_2_use_api and self.api_client_model2:
                future_to_model[executor.submit(self.api_client_model2.generate_response,
                                              question, self.config.max_tokens, self.config.temperature)] = f"API-{getattr(self.config, 'model_2_api_model', '') or self.config.api_model}"
            else:
                future_to_model[executor.submit(self.client.generate_response,
                                              self.config.model_2, question,
                                              self.config.max_tokens, self.config.temperature,
                                              self.config.timeout)] = self.config.model_2

            results = []
            for future in concurrent.futures.as_completed(future_to_model):
                model_name = future_to_model[future]
                try:
                    result = future.result()
                    results.append(result)
                    self.progress_tracker.update()
                except Exception as e:
                    logger.error(f"æ¨¡å‹ {model_name} æ‰§è¡Œé”™è¯¯: {e}")
                    results.append({
                        "success": False,
                        "model": model_name,
                        "error": f"æ‰§è¡Œé”™è¯¯: {str(e)}",
                        "time": 0
                    })

        self._display_results(results)

        if self.config.save_history:
            self._save_history_entry(question, results, mode="parallel")

        return results

    # ==================== ã€è¾©è®ºæ¨¡å¼ã€‘ ====================
    def _debate_ask(self, question: str, role1: Optional[str] = None, role2: Optional[str] = None) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå¤šå›åˆAIè¾©è®º - å¢å¼ºç‰ˆ

        è¿™æ˜¯MACPç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå®ç°ä¸¤ä¸ªAIæ¨¡å‹ä¹‹é—´çš„è¾©è®ºï¼š
        1. åˆå§‹åŒ–è¾©è®ºè§’è‰²ï¼ˆæ”¯æŒæ­£åæ–¹ç«‹åœºï¼‰
        2. è¿›è¡Œå¤šå›åˆè¾©è®ºå¯¹è¯ï¼ŒåŒæ–¹å¯ä»¥çœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡
        3. å®æ—¶AIå…±è¯†åº¦åˆ†æå’Œç›‘æ§
        4. è¾¾åˆ°é˜ˆå€¼æ—¶è‡ªåŠ¨ç»“æŸå¹¶ç”Ÿæˆæ€»ç»“

        Args:
            question: è¾©è®ºä¸»é¢˜é—®é¢˜
            role1: ç¬¬ä¸€ä¸ªAIçš„è§’è‰²ï¼ˆé»˜è®¤ä¸ºæ­£æ–¹ï¼‰
            role2: ç¬¬äºŒä¸ªAIçš„è§’è‰²ï¼ˆé»˜è®¤ä¸ºåæ–¹ï¼‰

        Returns:
            åŒ…å«ä¸¤ä¸ªAIå“åº”çš„ç»“æœåˆ—è¡¨
        """
        # ç¡®å®šè§’è‰²
        role1 = role1 or self.config.default_role_1
        role2 = role2 or self.config.default_role_2

        # å…ˆè·å–å®¢æˆ·ç«¯ï¼Œç¡®å®šå®é™…ä½¿ç”¨çš„æ¨¡å‹åï¼ˆæœ¬åœ°æˆ–APIï¼‰
        client1, model_id1, is_api1 = self._get_client_for_model(self.config.model_1)
        client2, model_id2, is_api2 = self._get_client_for_model(self.config.model_2)

        # æ„é€ æ˜¾ç¤ºåï¼šä½¿ç”¨å®é™…æ¨¡å‹åï¼ˆå¦‚æœæ˜¯APIåˆ™æ˜¾ç¤ºAPIæ¨¡å‹åï¼‰
        actual_model1 = model_id1 if is_api1 else self.config.model_1
        actual_model2 = model_id2 if is_api2 else self.config.model_2
        display_name1 = f"{actual_model1}-{role1}"
        display_name2 = f"{actual_model2}-{role2}"

        role_prompt1 = role_system.get_role_prompt(role1, is_first=True)   # æ­£æ–¹
        role_prompt2 = role_system.get_role_prompt(role2, is_first=False)  # åæ–¹

        if not role_prompt1 or not role_prompt2:
            raise InvalidRoleError(f"æ— æ•ˆè§’è‰²: {role1} æˆ– {role2}")

        self._setup_debate_roles(question, role1, role2)

        # åˆ†æé—®é¢˜ç±»å‹ï¼Œå†³å®šæ˜¯å¦éœ€è¦é«˜å‡†ç¡®åº¦
        question_analysis = analyze_question_type(question)
        accuracy_required = question_analysis["accuracy_required"]
        question_type = question_analysis["type"]
        
        # æ˜¾ç¤ºé—®é¢˜ç±»å‹åˆ†æ
        type_labels = {
            "factual": "äº‹å®ç±»/Factualï¼ˆé«˜å‡†ç¡®åº¦/High Accuracyï¼‰", 
            "philosophical": "å“²å­¦ç±»/Philosophicalï¼ˆå¼€æ”¾è®¨è®º/Open Discussionï¼‰", 
            "mixed": "æ··åˆç±»/Mixed"
        }
        print(f"ğŸ” é—®é¢˜ç±»å‹ (Question Type): {type_labels.get(question_type, question_type)}")
        if accuracy_required:
            print("âš ï¸ å‡†ç¡®åº¦æ¨¡å¼ (Accuracy Mode): AIä¼šçº æ­£é—®é¢˜ä¸­çš„äº‹å®é”™è¯¯ (AI will correct factual errors)")

        # ç¬¬ä¸€å›åˆï¼šåŒæ–¹çŸ¥é“å¯¹æ‰‹æ˜¯è°ï¼Œä½†çœ‹ä¸åˆ°å…·ä½“è§‚ç‚¹
        DisplayManager.print_separator("-", 40)
        print("ç¬¬1å›åˆï¼šåˆå§‹é™ˆè¿° (Round 1: Opening Statement)")
        DisplayManager.print_separator("-", 40)
        print(f"ğŸ’¡ {role1} vs {role2} - åŒæ–¹å·²çŸ¥æ™“å¯¹æ‰‹èº«ä»½ (Both sides know opponent)")
        if is_api1 or is_api2:
            print(f"ğŸŒ ä½¿ç”¨æ¨¡å‹ (Using models): {actual_model1} | {actual_model2}")

        # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©é™„åŠ æç¤ºè¯
        if CURRENT_LANGUAGE == "en":
            mode_instruction = ANTI_HALLUCINATION_PROMPT_EN if accuracy_required else PHILOSOPHICAL_PROMPT_EN
        else:
            mode_instruction = ANTI_HALLUCINATION_PROMPT_ZH if accuracy_required else PHILOSOPHICAL_PROMPT_ZH

        # å¢å¼ºç‰ˆç¬¬ä¸€å›åˆæç¤ºè¯ - è®©AIçŸ¥é“å¯¹æ‰‹æ˜¯è°ï¼Œå¹¶è¦æ±‚ç®€æ´è¡¨è¾¾
        # æ ¹æ®å½“å‰è¯­è¨€ç”Ÿæˆä¸åŒçš„æç¤ºè¯
        if CURRENT_LANGUAGE == "en":
            lang_instruction = "\n**IMPORTANT: You MUST respond entirely in English.**\n"
            prompt1 = f"""{role_prompt1}
{lang_instruction}
{mode_instruction}

ã€Debate Topicã€‘: {question}

ã€Your Positionã€‘: {role1} (Pro side)
ã€Opponent Roleã€‘: {role2} (Con side)

Please clearly and concisely present your core arguments (highlight 3-5 key points):
"""

            prompt2 = f"""{role_prompt2}
{lang_instruction}
{mode_instruction}

ã€Debate Topicã€‘: {question}

ã€Your Positionã€‘: {role2} (Con side)
ã€Opponent Roleã€‘: {role1} (Pro side)

Please clearly and concisely present your core arguments (highlight 3-5 key points):
"""
        else:
            prompt1 = f"""{role_prompt1}
{mode_instruction}

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}

ã€ä½ çš„ç«‹åœºã€‘: {role1}ï¼ˆæ­£æ–¹ï¼‰
ã€å¯¹æ‰‹è§’è‰²ã€‘: {role2}ï¼ˆåæ–¹ï¼‰

è¯·ç®€æ´æœ‰åŠ›åœ°é˜è¿°ä½ çš„æ ¸å¿ƒè§‚ç‚¹ï¼ˆé‡ç‚¹çªå‡º3-5ä¸ªå…³é”®è®ºç‚¹ï¼‰ï¼š
"""

            prompt2 = f"""{role_prompt2}
{mode_instruction}

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}

ã€ä½ çš„ç«‹åœºã€‘: {role2}ï¼ˆåæ–¹ï¼‰
ã€å¯¹æ‰‹è§’è‰²ã€‘: {role1}ï¼ˆæ­£æ–¹ï¼‰

è¯·ç®€æ´æœ‰åŠ›åœ°é˜è¿°ä½ çš„æ ¸å¿ƒè§‚ç‚¹ï¼ˆé‡ç‚¹çªå‡º3-5ä¸ªå…³é”®è®ºç‚¹ï¼‰ï¼š
"""

        # ç¬¬ä¸€ä½è¾©è®ºè€…å‘è¨€ï¼ˆå®¢æˆ·ç«¯å·²åœ¨å‰é¢è·å–ï¼‰
        streaming_used1 = False
        if is_api1:
            # APIæ¨¡å¼ï¼šæ”¯æŒæµå¼è¾“å‡º
            if self.config.streaming_output:
                result1 = client1.generate_response(prompt1, max_tokens=500, temperature=self.config.temperature,
                                                   streaming=True, speaker_name=display_name1, response_type="")
                streaming_used1 = True
            else:
                result1 = client1.generate_response(prompt1, max_tokens=500, temperature=self.config.temperature)
        else:
            # Ollamaæ¨¡å¼
            if self.config.streaming_output:
                result1 = client1._generate_streaming_response(self.config.model_1, prompt1, max_tokens=500,
                                                  temperature=self.config.temperature, timeout=self.config.timeout,
                                                  speaker_name=display_name1, response_type="")
                streaming_used1 = True
            else:
                result1 = client1.generate_response(self.config.model_1, prompt1, max_tokens=500,
                                                  temperature=self.config.temperature, timeout=self.config.timeout,
                                                  streaming=False)

        # ç¬¬äºŒä½è¾©è®ºè€…å‘è¨€
        streaming_used2 = False
        if is_api2:
            # APIæ¨¡å¼ï¼šæ”¯æŒæµå¼è¾“å‡º
            if self.config.streaming_output:
                result2 = client2.generate_response(prompt2, max_tokens=500, temperature=self.config.temperature,
                                                   streaming=True, speaker_name=display_name2, response_type="")
                streaming_used2 = True
            else:
                result2 = client2.generate_response(prompt2, max_tokens=500, temperature=self.config.temperature)
        else:
            # Ollamaæ¨¡å¼
            if self.config.streaming_output:
                result2 = client2._generate_streaming_response(self.config.model_2, prompt2, max_tokens=500,
                                                  temperature=self.config.temperature, timeout=self.config.timeout,
                                                  speaker_name=display_name2, response_type="")
                streaming_used2 = True
            else:
                result2 = client2.generate_response(self.config.model_2, prompt2, max_tokens=500,
                                                  temperature=self.config.temperature, timeout=self.config.timeout,
                                                  streaming=False)

        # å®‰å…¨å¤„ç†
        if not result1.get("success"):
            result1 = {"success": False, "response": "ï¼ˆæ¨¡å‹1æ— å“åº”ï¼‰"}
        if not result2.get("success"):
            result2 = {"success": False, "response": "ï¼ˆæ¨¡å‹2æ— å“åº”ï¼‰"}

        response1 = result1.get("response", "")
        response2 = result2.get("response", "")

        debate_round = [
            {"round": 1, "speaker": display_name1, "content": response1, "type": "opening"},
            {"round": 1, "speaker": display_name2, "content": response2, "type": "opening"}
        ]

        # éæµå¼è¾“å‡ºæ—¶æ‰æ˜¾ç¤ºï¼ˆæµå¼è¾“å‡ºå·²ç»å®æ—¶æ˜¾ç¤ºè¿‡äº†ï¼‰
        if not streaming_used1:
            self._display_debate_response(display_name1, response1)
        if not streaming_used2:
            self._display_debate_response(display_name2, response2)

        # åç»­å›åˆï¼ˆæ™ºèƒ½æå‰ç»“æŸï¼‰
        max_rounds = min(self.config.debate_rounds, 6)
        consensus_reached = False
        consensus_analysis = ""

        for round_num in range(2, max_rounds + 1):
            # æ£€æŸ¥å…±è¯†ï¼ˆä½¿ç”¨AIåˆ†æï¼‰
            if self.config.enable_early_stop and self.config.ai_consensus_analysis and round_num >= self.config.consensus_check_start_round:
                print(f"\nğŸ§  æ­£åœ¨åˆ†æåŒæ–¹å…±è¯†åº¦...")
                consensus_score, analysis, analysis_data = ConsensusDetector.analyze_debate_consensus(
                    self, self.config.coordinator_model, question, debate_round, display_name1, display_name2
                )

                consensus_percentage = int(consensus_score * 100)

                # æ˜¾ç¤ºå…±è¯†åº¦æ¡å½¢å›¾
                ConsensusDetector.display_consensus_bar(consensus_percentage)

                # æ˜¾ç¤ºç®€çŸ­åˆ†æï¼ˆé™åˆ¶é•¿åº¦ï¼Œé¿å…è¾“å‡ºè¿‡é•¿ï¼‰
                if analysis_data and 'analysis_summary' in analysis_data:
                    short_analysis = analysis_data['analysis_summary'][:150]
                    if len(analysis_data['analysis_summary']) > 150:
                        short_analysis += "..."
                    print(f"ğŸ“ åˆ†æ: {short_analysis}")
                elif analysis and len(analysis) < 200:
                    print(f"ğŸ“ åˆ†æ: {analysis}")

                # æ˜¾ç¤ºè¯¦ç»†åˆ†æï¼ˆç®€åŒ–æ˜¾ç¤ºï¼‰
                if analysis_data:
                    if 'recommendation' in analysis_data:
                        recommendation = analysis_data['recommendation']
                        if recommendation == 'end':
                            print(f"ğŸ¯ AIå»ºè®®: ç»“æŸè¾©è®º")
                        else:
                            print(f"ğŸ”„ AIå»ºè®®: ç»§ç»­è¾©è®º")

                consensus_analysis = analysis

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                threshold_percentage = int(config.consensus_threshold * 100)
                consensus_reached = InteractiveInterface._handle_consensus_feedback(consensus_score, consensus_percentage, threshold_percentage, consensus_reached)
                if consensus_reached:
                    break

            DisplayManager.print_separator("-", 40)
            if CURRENT_LANGUAGE == "en":
                print(f"Round {round_num}: Mutual Response")
            else:
                print(f"ç¬¬{round_num}å›åˆï¼šäº’ç›¸å›åº”")
            DisplayManager.print_separator("-", 40)

            # æ„å»ºè¾©è®ºå†å²ä¸Šä¸‹æ–‡
            debate_history = AICouncilScheduler._build_debate_context(debate_round, display_name1, display_name2)

            # æ¨¡å‹1å›åº”æ¨¡å‹2 - å¢å¼ºç‰ˆï¼šçœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡
            if result1.get("success") and result2.get("success"):
                if CURRENT_LANGUAGE == "en":
                    rebuttal_prompt1 = f"""{role_prompt1}

**IMPORTANT: You MUST respond entirely in English.**

ã€Debate Topicã€‘: {question}
ã€Your Positionã€‘: {role1} (Pro side)
ã€Opponent Roleã€‘: {role2} (Con side)

{debate_history}

ã€Your Taskã€‘
Refute {role2}'s arguments concisely and forcefully:
1. Point out the core weaknesses in opponent's arguments
2. Use 1-2 key arguments to refute
3. Reaffirm your core position

Please respond concisely (key points only, max 300 words):
"""
                else:
                    rebuttal_prompt1 = f"""{role_prompt1}

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}
ã€ä½ çš„ç«‹åœºã€‘: {role1}ï¼ˆæ­£æ–¹ï¼‰
ã€å¯¹æ‰‹è§’è‰²ã€‘: {role2}ï¼ˆåæ–¹ï¼‰

{debate_history}

ã€ä½ çš„ä»»åŠ¡ã€‘
é’ˆå¯¹{role2}çš„è§‚ç‚¹è¿›è¡Œç®€æ´æœ‰åŠ›çš„åé©³ï¼š
1. æŒ‡å‡ºå¯¹æ‰‹è§‚ç‚¹çš„æ ¸å¿ƒå¼±ç‚¹
2. ç”¨1-2ä¸ªå…³é”®è®ºæ®è¿›è¡Œåé©³
3. é‡ç”³ä½ çš„æ ¸å¿ƒç«‹åœº

è¯·ç®€æ´å›åº”ï¼ˆé‡ç‚¹çªå‡ºï¼Œä¸è¶…è¿‡300å­—ï¼‰ï¼š
"""
                client1, _, is_api1 = self._get_client_for_model(self.config.model_1)
                rebuttal_streaming1 = False
                response_type1 = f"Rebuttal to {role2}" if CURRENT_LANGUAGE == "en" else f"åé©³{role2}"
                if is_api1:
                    # APIæ¨¡å¼ï¼šæ”¯æŒæµå¼è¾“å‡º
                    if self.config.streaming_output:
                        result1 = client1.generate_response(rebuttal_prompt1, max_tokens=600, temperature=self.config.temperature,
                                                           streaming=True, speaker_name=display_name1, response_type=response_type1)
                        rebuttal_streaming1 = True
                    else:
                        result1 = client1.generate_response(rebuttal_prompt1, max_tokens=600, temperature=self.config.temperature)
                else:
                    # Ollamaæ¨¡å¼
                    if self.config.streaming_output:
                        result1 = client1._generate_streaming_response(self.config.model_1, rebuttal_prompt1, max_tokens=600,
                                                          temperature=self.config.temperature, timeout=self.config.timeout,
                                                          speaker_name=display_name1, response_type=response_type1)
                        rebuttal_streaming1 = True
                    else:
                        result1 = client1.generate_response(self.config.model_1, rebuttal_prompt1, max_tokens=600,
                                                          temperature=self.config.temperature, timeout=self.config.timeout,
                                                          streaming=False)

                if result1.get("success"):
                    response1 = result1.get("response", "")
                    debate_round.append({
                        "round": round_num,
                        "speaker": display_name1,
                        "content": response1,
                        "type": "rebuttal"
                    })
                    if not rebuttal_streaming1:
                        self._display_debate_response(display_name1, response1, response_type1)

            # æ¨¡å‹2å›åº”æ¨¡å‹1 - å¢å¼ºç‰ˆï¼šçœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡
            if result1.get("success") and result2.get("success"):
                # æ›´æ–°è¾©è®ºå†å²ï¼ŒåŒ…å«æœ€æ–°çš„AI1å›åº”
                debate_history = AICouncilScheduler._build_debate_context(debate_round, display_name1, display_name2)

                if CURRENT_LANGUAGE == "en":
                    rebuttal_prompt2 = f"""{role_prompt2}

**IMPORTANT: You MUST respond entirely in English.**

ã€Debate Topicã€‘: {question}
ã€Your Positionã€‘: {role2} (Con side)
ã€Opponent Roleã€‘: {role1} (Pro side)

{debate_history}

ã€Your Taskã€‘
Respond to {role1}'s rebuttal concisely and forcefully:
1. Counter opponent's rebuttal points
2. Use 1-2 key arguments to strengthen your position
3. Introduce new debate angles

Please respond concisely (key points only, max 300 words):
"""
                else:
                    rebuttal_prompt2 = f"""{role_prompt2}

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}
ã€ä½ çš„ç«‹åœºã€‘: {role2}ï¼ˆåæ–¹ï¼‰
ã€å¯¹æ‰‹è§’è‰²ã€‘: {role1}ï¼ˆæ­£æ–¹ï¼‰

{debate_history}

ã€ä½ çš„ä»»åŠ¡ã€‘
é’ˆå¯¹{role1}çš„åé©³è¿›è¡Œç®€æ´æœ‰åŠ›çš„å›åº”ï¼š
1. åé©³å¯¹æ‰‹çš„åé©³è®ºç‚¹
2. ç”¨1-2ä¸ªå…³é”®è®ºæ®åŠ å¼ºä½ çš„ç«‹åœº
3. æå‡ºæ–°çš„è¾©è®ºè§’åº¦

è¯·ç®€æ´å›åº”ï¼ˆé‡ç‚¹çªå‡ºï¼Œä¸è¶…è¿‡300å­—ï¼‰ï¼š
"""
                client2, _, is_api2 = self._get_client_for_model(self.config.model_2)
                rebuttal_streaming2 = False
                response_type2 = f"Rebuttal to {role1}" if CURRENT_LANGUAGE == "en" else f"åé©³{role1}"
                if is_api2:
                    # APIæ¨¡å¼ï¼šæ”¯æŒæµå¼è¾“å‡º
                    if self.config.streaming_output:
                        result2 = client2.generate_response(rebuttal_prompt2, max_tokens=600, temperature=self.config.temperature,
                                                           streaming=True, speaker_name=display_name2, response_type=response_type2)
                        rebuttal_streaming2 = True
                    else:
                        result2 = client2.generate_response(rebuttal_prompt2, max_tokens=600, temperature=self.config.temperature)
                else:
                    # Ollamaæ¨¡å¼
                    if self.config.streaming_output:
                        result2 = client2._generate_streaming_response(self.config.model_2, rebuttal_prompt2, max_tokens=600,
                                                          temperature=self.config.temperature, timeout=self.config.timeout,
                                                          speaker_name=display_name2, response_type=response_type2)
                        rebuttal_streaming2 = True
                    else:
                        result2 = client2.generate_response(self.config.model_2, rebuttal_prompt2, max_tokens=600,
                                                          temperature=self.config.temperature, timeout=self.config.timeout,
                                                          streaming=False)

                if result2.get("success"):
                    response2 = result2.get("response", "")
                    debate_round.append({
                        "round": round_num,
                        "speaker": display_name2,
                        "content": response2,
                        "type": "rebuttal"
                    })
                    if not rebuttal_streaming2:
                        self._display_debate_response(display_name2, response2, response_type2)

        # åè°ƒé˜¶æ®µ
        if CURRENT_LANGUAGE == "en":
            DisplayManager.print_header("ğŸ¯ Coordination Summary")
        else:
            DisplayManager.print_header("ğŸ¯ åè°ƒæ€»ç»“")

        if consensus_reached:
            if CURRENT_LANGUAGE == "en":
                print("ğŸ¤ High consensus reached, generating final summary")
            else:
                print("ğŸ¤ åŒæ–¹å·²è¾¾æˆé«˜åº¦å…±è¯†ï¼Œç”Ÿæˆæœ€ç»ˆæ€»ç»“")
            self._generate_consensus_summary(question, debate_round, role1, role2, consensus_analysis)
        else:
            self._coordinate_responses(question, debate_round, role1, role2)

        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä¿å­˜è¾©è®ºè®°å½•
        self._ask_save_debate_log(question, debate_round, display_name1, display_name2)

        # è¿”å›å®Œæ•´çš„è¾©è®ºè®°å½•ï¼Œè®©ç”¨æˆ·å¯ä»¥çœ‹åˆ°æ‰€æœ‰å‘è¨€
        return debate_round

    def _generate_consensus_summary(self, question: str, debate_round: List[Dict[str, Any]],
                                   role1: str, role2: str, consensus_analysis: str) -> str:
        """ç”Ÿæˆè¾©è®ºå…±è¯†æ€»ç»“æŠ¥å‘Šï¼ˆæµå¼è¾“å‡ºï¼‰

        å½“è¾©è®ºè¾¾åˆ°å…±è¯†é˜ˆå€¼æ—¶ï¼Œè°ƒç”¨åè°ƒAIç”Ÿæˆä¸“ä¸šçš„æ€»ç»“æŠ¥å‘Šï¼š
        1. æ•´ç†å®Œæ•´çš„è¾©è®ºè¿‡ç¨‹å’Œå…±è¯†åˆ†æç»“æœ
        2. è¦æ±‚AIç”Ÿæˆç»“æ„åŒ–çš„æ€»ç»“æŠ¥å‘Š
        3. å¿…é¡»åŒ…å«è‡³å°‘2ç‚¹å…±è¯†å’Œ2ç‚¹åˆ†æ­§

        è¿™æ˜¯MACPç³»ç»Ÿçš„æ ¸å¿ƒä»·å€¼ä¹‹ä¸€ï¼Œèƒ½å¤Ÿå°†AIè¾©è®ºè½¬åŒ–ä¸º
        æœ‰ä»·å€¼çš„åˆ†ææŠ¥å‘Šï¼Œå¸®åŠ©ç”¨æˆ·æ·±å…¥ç†è§£è¾©è®ºä¸»é¢˜
        """
        if CURRENT_LANGUAGE == "en":
            print(f"\nğŸ¤– Coordinator AI ({self.config.coordinator_model}) generating final summary...")
            print("ğŸ“ Summary: ", end="", flush=True)
        else:
            print(f"\nğŸ¤– åè°ƒAI ({self.config.coordinator_model}) æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ€»ç»“...")
            print("ğŸ“ æ€»ç»“: ", end="", flush=True)

        # æ„å»ºè¾©è®ºæ‘˜è¦
        debate_summary = ""
        for entry in debate_round[-6:]:  # æœ€å6è½®å¯¹è¯
            debate_summary += f"\n{entry['speaker']}: {entry.get('content', '')[:200]}"

        # æ ¹æ®è¯­è¨€é€‰æ‹©æç¤ºè¯
        if CURRENT_LANGUAGE == "en":
            summary_prompt = f"""Based on the following debate process and consensus analysis, please generate a final summary report:

ã€Debate Topicã€‘: {question}
ã€Debate Partiesã€‘: {role1} vs {role2}
ã€Consensus Analysisã€‘: {consensus_analysis}

ã€Debate Summaryã€‘:
{debate_summary}

Please generate a structured summary report that MUST include:

## ğŸ¯ Debate Summary

### ğŸ“Š Consensus Points (MUST list at least 2 points)
1. [First consensus point]
2. [Second consensus point]
(More if applicable)

### âš”ï¸ Disagreement Points (MUST list at least 2 points)  
1. [First disagreement point]
2. [Second disagreement point]
(More if applicable)

### ğŸ—£ï¸ Position Comparison
- {role1}'s core position
- {role2}'s core position

### ğŸ’¡ Comprehensive Conclusion
- Final answer to the original question
- Constructive suggestions

Please ensure the summary is objective and neutral."""
        else:
            summary_prompt = f"""åŸºäºä»¥ä¸‹è¾©è®ºè¿‡ç¨‹å’Œå…±è¯†åˆ†æï¼Œè¯·ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Šï¼š

ã€è¾©è®ºä¸»é¢˜ã€‘: {question}
ã€è¾©è®ºåŒæ–¹ã€‘: {role1} vs {role2}
ã€å…±è¯†åˆ†æã€‘: {consensus_analysis}

ã€è¾©è®ºè¿‡ç¨‹æ‘˜è¦ã€‘:
{debate_summary}

è¯·ç”Ÿæˆç»“æ„åŒ–çš„æ€»ç»“æŠ¥å‘Šï¼Œã€å¿…é¡»ã€‘åŒ…å«ï¼š

## ğŸ¯ è¾©è®ºæ€»ç»“

### ğŸ“Š å…±è¯†ç‚¹ï¼ˆã€å¿…é¡»ã€‘åˆ—å‡ºè‡³å°‘2ç‚¹ï¼‰
1. [ç¬¬ä¸€ä¸ªå…±è¯†ç‚¹]
2. [ç¬¬äºŒä¸ªå…±è¯†ç‚¹]
ï¼ˆå¦‚æœ‰æ›´å¤šå¯ç»§ç»­åˆ—å‡ºï¼‰

### âš”ï¸ åˆ†æ­§ç‚¹ï¼ˆã€å¿…é¡»ã€‘åˆ—å‡ºè‡³å°‘2ç‚¹ï¼‰
1. [ç¬¬ä¸€ä¸ªåˆ†æ­§ç‚¹]
2. [ç¬¬äºŒä¸ªåˆ†æ­§ç‚¹]
ï¼ˆå¦‚æœ‰æ›´å¤šå¯ç»§ç»­åˆ—å‡ºï¼‰

### ğŸ—£ï¸ åŒæ–¹ç«‹åœºå¯¹æ¯”
- {role1}çš„æ ¸å¿ƒç«‹åœº
- {role2}çš„æ ¸å¿ƒç«‹åœº

### ğŸ’¡ ç»¼åˆç»“è®º
- å¯¹åŸé—®é¢˜çš„æœ€ç»ˆç­”æ¡ˆ
- å»ºè®¾æ€§å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ

è¯·ç¡®ä¿æ€»ç»“å®¢è§‚ã€ä¸­ç«‹ï¼Œå¹¶åŸºäºåŒæ–¹çš„å®é™…è®ºè¿°ã€‚"""

        coord_client, coord_model, is_api = self._get_client_for_model(self.config.coordinator_model)
        
        # ä½¿ç”¨æµå¼è¾“å‡º
        if is_api:
            # APIæ¨¡å¼çš„æµå¼è¾“å‡º
            summary_result = coord_client.generate_response(
                summary_prompt, 
                max_tokens=1200, 
                temperature=self.config.temperature,
                streaming=True
            )
            summary = summary_result.get("response", "")
        else:
            # Ollamaæ¨¡å¼çš„æµå¼è¾“å‡º
            summary_result = coord_client._generate_streaming_response(
                coord_model, 
                summary_prompt, 
                max_tokens=1200,
                temperature=self.config.temperature, 
                timeout=self.config.timeout,
                speaker_name="ğŸ“ æ€»ç»“" if CURRENT_LANGUAGE == "zh" else "ğŸ“ Summary"
            )
            summary = summary_result.get("response", "")

        print()  # æ¢è¡Œ
        
        if summary_result.get("success") and summary.strip():
            if CURRENT_LANGUAGE == "en":
                print(f"\nâœ… Summary generation complete")
            else:
                print(f"\nâœ… æ€»ç»“ç”Ÿæˆå®Œæˆ")
            return summary
        else:
            if CURRENT_LANGUAGE == "en":
                print(f"âŒ Summary generation failed")
            else:
                print(f"âŒ æ€»ç»“ç”Ÿæˆå¤±è´¥")
            return f"åŸºäºå…±è¯†åˆ†æçš„æ€»ç»“ï¼š{consensus_analysis}\n\nè¾©è®ºå·²è‡ªåŠ¨ç»“æŸï¼ŒåŒæ–¹è¾¾æˆé«˜åº¦å…±è¯†ã€‚"

    def _coordinate_responses(self, question: str, debate_round: List[Dict[str, Any]],
                            role1: str, role2: str) -> str:
        """åè°ƒè¾©è®ºç»“æœï¼ˆæµå¼è¾“å‡ºï¼‰"""
        if CURRENT_LANGUAGE == "en":
            print(f"\nğŸ¤– Coordinator AI ({self.config.coordinator_model}) analyzing...")
            print("ğŸ“ Analysis: ", end="", flush=True)
        else:
            print(f"\nğŸ¤– åè°ƒAI ({self.config.coordinator_model}) æ­£åœ¨åˆ†æ...")
            print("ğŸ“ åˆ†æ: ", end="", flush=True)

        # æ„å»ºæ‘˜è¦
        debate_summary = ""
        for entry in debate_round:  # å–å…¨éƒ¨è¾©è®ºå†…å®¹
            debate_summary += f"\n{entry['speaker']}: {entry.get('content', '')[:200]}"

        # æ ¹æ®è¯­è¨€é€‰æ‹©æç¤ºè¯
        if CURRENT_LANGUAGE == "en":
            coord_prompt = f"""Please analyze the following debate as a neutral coordinator:

Topic: {question}
Debate Parties: {role1} vs {role2}
Debate Summary: {debate_summary}

Please provide a structured analysis that MUST include:

### ğŸ“Š Consensus Points (MUST list at least 2 points)
1. [First consensus point - what both sides agree on]
2. [Second consensus point]
(More if applicable)

### âš”ï¸ Disagreement Points (MUST list at least 2 points)
1. [First disagreement point - where they differ]
2. [Second disagreement point]
(More if applicable)

### ğŸ’¡ Comprehensive Suggestion
- Your neutral recommendation to the user
- How to think about this issue

Please be objective and balanced in your analysis."""
        else:
            coord_prompt = f"""è¯·ä½œä¸ºä¸­ç«‹åè°ƒå‘˜åˆ†æä»¥ä¸‹è¾©è®ºï¼š

é—®é¢˜ï¼š{question}
è¾©è®ºåŒæ–¹ï¼š{role1} vs {role2}
è¾©è®ºæ‘˜è¦ï¼š{debate_summary}

è¯·æä¾›ç»“æ„åŒ–åˆ†æï¼Œã€å¿…é¡»ã€‘åŒ…å«ï¼š

### ğŸ“Š å…±è¯†ç‚¹ï¼ˆã€å¿…é¡»ã€‘åˆ—å‡ºè‡³å°‘2ç‚¹ï¼‰
1. [ç¬¬ä¸€ä¸ªå…±è¯†ç‚¹ - åŒæ–¹éƒ½åŒæ„çš„è§‚ç‚¹]
2. [ç¬¬äºŒä¸ªå…±è¯†ç‚¹]
ï¼ˆå¦‚æœ‰æ›´å¤šå¯ç»§ç»­åˆ—å‡ºï¼‰

### âš”ï¸ åˆ†æ­§ç‚¹ï¼ˆã€å¿…é¡»ã€‘åˆ—å‡ºè‡³å°‘2ç‚¹ï¼‰
1. [ç¬¬ä¸€ä¸ªåˆ†æ­§ç‚¹ - åŒæ–¹çš„ä¸åŒè§‚ç‚¹]
2. [ç¬¬äºŒä¸ªåˆ†æ­§ç‚¹]
ï¼ˆå¦‚æœ‰æ›´å¤šå¯ç»§ç»­åˆ—å‡ºï¼‰

### ğŸ’¡ ç»¼åˆå»ºè®®
- ç»™ç”¨æˆ·çš„ä¸­ç«‹å»ºè®®
- å¦‚ä½•çœ‹å¾…è¿™ä¸ªé—®é¢˜

è¯·ä¿æŒå®¢è§‚ã€ä¸­ç«‹çš„ç«‹åœºè¿›è¡Œåˆ†æã€‚"""

        coord_client, coord_model, is_api = self._get_client_for_model(self.config.coordinator_model)
        
        # ä½¿ç”¨æµå¼è¾“å‡º
        if is_api:
            coord_result = coord_client.generate_response(
                coord_prompt, 
                max_tokens=1000, 
                temperature=self.config.temperature,
                streaming=True
            )
            coord_response = coord_result.get("response", "")
        else:
            coord_result = coord_client._generate_streaming_response(
                coord_model, 
                coord_prompt, 
                max_tokens=1000,
                temperature=self.config.temperature, 
                timeout=self.config.timeout,
                speaker_name="ğŸ“ åˆ†æ" if CURRENT_LANGUAGE == "zh" else "ğŸ“ Analysis"
            )
            coord_response = coord_result.get("response", "")

        print()  # æ¢è¡Œ

        if coord_result.get("success") and coord_response.strip():
            if CURRENT_LANGUAGE == "en":
                print(f"\nâœ… Coordinator analysis complete")
            else:
                print(f"\nâœ… åè°ƒAIåˆ†æå®Œæˆ")
            return coord_response
        else:
            if CURRENT_LANGUAGE == "en":
                print(f"âŒ Coordinator analysis failed")
            else:
                print(f"âŒ åè°ƒAIåˆ†æå¤±è´¥")
            return f"åè°ƒåˆ†æå¤±è´¥"

    # ==================== ã€æµ·é¾Ÿæ±¤æ¨¡å¼ã€‘ ====================
    def _turtle_soup_ask(self, question: str, role1: Optional[str] = None, role2: Optional[str] = None) -> List[Dict[str, Any]]:
        """æµ·é¾Ÿæ±¤æ¨¡å¼"""
        DisplayManager.print_header("ğŸ¢ æµ·é¾Ÿæ±¤æ¨¡å¼")
        print("è§„åˆ™ï¼š")
        print("1. ä¸¤ä¸ªAIä¼šè½®æµå‘æ‚¨æé—®")
        print("2. æ¯ä¸ªAIæ¯å›åˆåªèƒ½é—®ä¸€ä¸ªé—®é¢˜")
        print("3. æ‚¨åªèƒ½å›ç­” 'æ˜¯'ã€'å¦' æˆ– 'ä¸çŸ¥é“'")
        print("4. ç›®æ ‡æ˜¯è®©AIçŒœå‡ºè°œåº•")
        DisplayManager.print_separator()

        role1 = role1 or "ä¾¦æ¢"
        role2 = role2 or "æ¨ç†è€…"

        history = []
        round_count = 0
        max_rounds = self.config.turtle_soup_max_rounds

        while round_count < max_rounds:
            round_count += 1
            DisplayManager.print_separator("-", 40)
            print(f"ç¬¬{round_count}å›åˆ")
            DisplayManager.print_separator("-", 40)

            # äº¤æ›¿æé—®
            current_role = role1 if round_count % 2 == 1 else role2
            current_model = self.config.model_1 if round_count % 2 == 1 else self.config.model_2

            # ç”Ÿæˆé—®é¢˜
            if round_count == 1:
                prompt = f"""ä½ æ˜¯{current_role}ï¼Œæ­£åœ¨ç©æµ·é¾Ÿæ±¤æ¸¸æˆã€‚
è°œé¢ï¼š{question}
ä½ çš„ä»»åŠ¡æ˜¯å‘ç©å®¶æé—®ï¼Œæ¯æ¬¡åªèƒ½é—®ä¸€ä¸ªé—®é¢˜ï¼Œç©å®¶åªèƒ½å›ç­”æ˜¯ã€å¦æˆ–ä¸çŸ¥é“ã€‚
è¯·å¼€å§‹ä½ çš„ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆåªé—®ä¸€ä¸ªé—®é¢˜ï¼‰ï¼š"""
            else:
                history_text = "\n".join(history[-4:])  # æœ€è¿‘4æ¡å†å²
                prompt = f"""ä½ æ˜¯{current_role}ï¼Œæ­£åœ¨ç©æµ·é¾Ÿæ±¤æ¸¸æˆã€‚
è°œé¢ï¼š{question}
å†å²é—®ç­”ï¼š
{history_text}
è¯·åŸºäºä»¥ä¸Šä¿¡æ¯é—®ä¸‹ä¸€ä¸ªé—®é¢˜ï¼ˆåªé—®ä¸€ä¸ªé—®é¢˜ï¼‰ï¼š"""

            client, model_id, is_api = self._get_client_for_model(current_model)
            if is_api:
                result = client.generate_response(prompt, max_tokens=200, temperature=self.config.temperature)
            else:
                result = client.generate_response(model_id, prompt, max_tokens=200,
                                                temperature=self.config.temperature, timeout=self.config.timeout,
                                                streaming=self.config.streaming_output)

            if result.get("success"):
                question_text = result.get("response", "").strip()
                print(f"\nâ“ {current_role} æé—®ï¼š{question_text}")

                # ç”¨æˆ·å›ç­”
                answer = self._get_turtle_soup_answer()
                if answer == "ç»“æŸ":
                    print("ğŸ‘¤ ç”¨æˆ·é€‰æ‹©ç»“æŸæ¸¸æˆ")
                    break

                history.append(f"é—®ï¼š{question_text}")
                history.append(f"ç­”ï¼š{answer}")

                # çŒœæµ‹ç­”æ¡ˆ
                if round_count % 3 == 0:
                    guess = self._attempt_guess(current_model, current_role, question, history)
                    if guess and self._confirm_guess(guess):
                        print(f"ğŸ‰ æ­å–œï¼{current_role} çŒœå¯¹äº†ï¼")
                        break
            else:
                print(f"âŒ {current_role} æé—®å¤±è´¥")
                break

        # æœ€ç»ˆæ€»ç»“
        self._finalize_turtle_soup(question, history)
        return []

    @staticmethod
    def _get_turtle_soup_answer() -> str:
        """è·å–æµ·é¾Ÿæ±¤ç­”æ¡ˆ"""
        while True:
            answer = input("\næ‚¨çš„å›ç­”ï¼ˆæ˜¯/å¦/ä¸çŸ¥é“/ç»“æŸï¼‰: ").strip().lower()
            if answer in ["æ˜¯", "å¦", "ä¸çŸ¥é“", "ç»“æŸ"]:
                return answer
            print("âŒ è¯·åªå›ç­”ï¼šæ˜¯ã€å¦ã€ä¸çŸ¥é“ æˆ– ç»“æŸ")

    def _attempt_guess(self, model: str, role: str, question: str, history: List[str]) -> Optional[str]:
        """å°è¯•çŒœæµ‹ç­”æ¡ˆ"""
        guess_prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œè¯·çŒœæµ‹è°œåº•ï¼š
è°œé¢ï¼š{question}
å†å²é—®ç­”ï¼š
{"\n".join(history[-6:])}
è¯·ç»™å‡ºä½ çš„çŒœæµ‹ï¼ˆå¦‚æœè¿˜ä¸ç¡®å®šå¯ä»¥è¯´'è¿˜éœ€è¦æ›´å¤šä¿¡æ¯'ï¼‰ï¼š"""

        client, model_id, is_api = self._get_client_for_model(model)
        if is_api:
            guess_result = client.generate_response(guess_prompt, max_tokens=300, temperature=self.config.temperature)
        else:
            guess_result = client.generate_response(model_id, guess_prompt, max_tokens=300,
                                                  temperature=self.config.temperature, timeout=self.config.timeout,
                                                  streaming=self.config.streaming_output)
        if guess_result.get("success"):
            guess = guess_result.get("response", "").strip()
            print(f"\nğŸ¤” {role} çŒœæµ‹ï¼š{guess}")
            return guess
        return None

    @staticmethod
    def _confirm_guess(guess: str) -> bool:
        """ç¡®è®¤çŒœæµ‹"""
        _ = guess  # æ ‡è®°å‚æ•°å·²çŸ¥ä½†æœªä½¿ç”¨
        confirm = input("çŒœå¯¹äº†å—ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰: ").strip().lower()
        return confirm == "æ˜¯"

    def _finalize_turtle_soup(self, question: str, history: List[str]):
        """å®Œæˆæµ·é¾Ÿæ±¤æ¸¸æˆ"""
        DisplayManager.print_header("ğŸ“ æµ·é¾Ÿæ±¤æ¸¸æˆç»“æŸ")

        if history:
            final_prompt = f"""åŸºäºä»¥ä¸‹æµ·é¾Ÿæ±¤æ¸¸æˆè®°å½•ï¼Œè¯·æ€»ç»“ï¼š
è°œé¢ï¼š{question}
å†å²è®°å½•ï¼š
{"\n".join(history)}
è¯·ç»™å‡ºæœ€ç»ˆåˆ†æå’Œè°œåº•è§£é‡Šï¼š"""

            coord_client, coord_model, is_api = self._get_client_for_model(self.config.coordinator_model)
            if is_api:
                final_result = coord_client.generate_response(final_prompt, max_tokens=500, temperature=self.config.temperature)
            else:
                final_result = coord_client.generate_response(coord_model, final_prompt, max_tokens=500,
                                                            temperature=self.config.temperature, timeout=self.config.timeout,
                                                            streaming=False)
            if final_result.get("success"):
                summary = final_result.get("response", "")
                print(f"\nğŸ“‹ æœ€ç»ˆæ€»ç»“ï¼š")
                print(summary[:self.config.display_length] +
                      ("..." if len(summary) > self.config.display_length else ""))

    # ==================== ã€è¾…åŠ©æ–¹æ³•ã€‘ ====================
    def _display_results(self, results: List[Dict[str, Any]]):
        """æ˜¾ç¤ºç»“æœ"""
        DisplayManager.print_separator("-", 80)
        print("ğŸ“Š å›ç­”ç»“æœï¼š")
        DisplayManager.print_separator("-", 80)

        for result in results:
            DisplayManager.print_result(result, self.config.display_length)

    @staticmethod
    def _build_debate_context(debate_round: List[Dict], display_name1: str, display_name2: str) -> str:
        """æ„å»ºè¾©è®ºå†å²ä¸Šä¸‹æ–‡
        
        Args:
            debate_round: è¾©è®ºè½®æ¬¡è®°å½•
            display_name1: ç¬¬ä¸€ä¸ªè¾©è®ºè€…æ˜¾ç¤ºåç§°ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼‰
            display_name2: ç¬¬äºŒä¸ªè¾©è®ºè€…æ˜¾ç¤ºåç§°ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼‰
        """
        _ = (display_name1, display_name2)  # æ ‡è®°å‚æ•°å·²çŸ¥ä½†æœªä½¿ç”¨ï¼ˆä¸ºæœªæ¥æ‰©å±•ä¿ç•™ï¼‰
        context_parts = ["ã€è¾©è®ºå†å²ã€‘"]

        for entry in debate_round[-4:]:  # åªæ˜¾ç¤ºæœ€è¿‘4æ¡å‘è¨€ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
            speaker = entry["speaker"]
            content = entry["content"][:500]  # é™åˆ¶æ¯ä¸ªå‘è¨€çš„é•¿åº¦
            round_num = entry["round"]

            context_parts.append(f"ç¬¬{round_num}å›åˆ - {speaker}ï¼š")
            context_parts.append(f"  {content}")
            context_parts.append("")

        return "\n".join(context_parts).strip()

    def _display_debate_response(self, speaker: str, content: str, response_type: str = ""):
        """æ˜¾ç¤ºè¾©è®ºå“åº”"""
        type_prefix = f" {response_type}ï¼š" if response_type else "ï¼š"
        print(f"\nğŸ“¢ {speaker}{type_prefix}")
        print(content[:self.config.display_length] +
              ("..." if len(content) > self.config.display_length else ""))

    def _save_history_entry(self, question: str, results: List[Dict[str, Any]], mode: str):
        """ä¿å­˜å†å²è®°å½•"""
        entry = {
            "session_id": self.session_id,
            "type": mode,
            "question": question,
            "results": results
        }
        self.history_manager.add_entry(entry)

    def _save_debate_entry(self, question: str, debate_round: List[Dict[str, Any]],
                          role1: str, role2: str):
        """ä¿å­˜è¾©è®ºå†å²"""
        entry = {
            "session_id": self.session_id,
            "type": "debate",
            "question": question,
            "roles": [role1, role2],
            "debate_rounds": debate_round
        }
        self.history_manager.add_entry(entry)

    def _ask_save_debate_log(self, question: str, debate_round: List[Dict[str, Any]],
                            role1: str, role2: str):
        """è¯¢é—®ç”¨æˆ·æ˜¯å¦ä¿å­˜è¾©è®ºè®°å½•åˆ°æ—¥å¿—
        
        è¾©è®ºç»“æŸåï¼Œè¯¢é—®ç”¨æˆ·ä¿å­˜é€‰é¡¹ï¼š
        1. å­˜å‚¨åˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆè¿½åŠ åˆ° macp.txtï¼‰
        2. å•ç‹¬ä¿å­˜ä¸ºä¸€ä¸ªæ–°çš„txtæ–‡ä»¶
        3. ä¸ä¿å­˜
        """
        print("\n" + "=" * 50)
        if CURRENT_LANGUAGE == "en":
            print("ğŸ“ Debate ended, save debate record?")
            print("=" * 50)
            print("1. ğŸ“‹ Save to log file (macp.txt)")
            print("2. ğŸ“„ Save as separate txt file")
            print("3. âŒ Don't save")
            print("=" * 50)
        else:
            print("ğŸ“ è¾©è®ºå·²ç»“æŸï¼Œæ˜¯å¦ä¿å­˜è¾©è®ºè®°å½•ï¼Ÿ")
            print("=" * 50)
            print("1. ğŸ“‹ å­˜å‚¨åˆ°æ—¥å¿—æ–‡ä»¶ (macp.txt)")
            print("2. ğŸ“„ å•ç‹¬ä¿å­˜ä¸ºæ–°çš„txtæ–‡ä»¶")
            print("3. âŒ ä¸ä¿å­˜")
            print("=" * 50)
        
        while True:
            if CURRENT_LANGUAGE == "en":
                choice = input("Select (1/2/3): ").strip()
            else:
                choice = input("è¯·é€‰æ‹© (1/2/3): ").strip()
            
            if choice == "1":
                # å­˜å‚¨åˆ°æ—¥å¿—æ–‡ä»¶
                self._save_debate_entry(question, debate_round, role1, role2)
                if self.config.save_history:
                    self.history_manager.save_history()
                if CURRENT_LANGUAGE == "en":
                    print("âœ… Debate record saved to log file (macp.txt)")
                else:
                    print("âœ… è¾©è®ºè®°å½•å·²ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ (macp.txt)")
                break
            elif choice == "2":
                # å•ç‹¬ä¿å­˜ä¸ºæ–°çš„txtæ–‡ä»¶
                self._save_debate_to_separate_file(question, debate_round, role1, role2)
                break
            elif choice == "3":
                if CURRENT_LANGUAGE == "en":
                    print("â­ï¸ Skipped saving")
                else:
                    print("â­ï¸ è·³è¿‡ä¿å­˜")
                break
            else:
                if CURRENT_LANGUAGE == "en":
                    print("âš ï¸ Invalid choice, please enter 1, 2 or 3")
                else:
                    print("âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")

    def _save_debate_to_separate_file(self, question: str, debate_round: List[Dict[str, Any]],
                                      role1: str, role2: str):
        """å°†è¾©è®ºè®°å½•ä¿å­˜åˆ°å•ç‹¬çš„txtæ–‡ä»¶
        
        åˆ›å»ºä¸€ä¸ªæ–°çš„txtæ–‡ä»¶ï¼ŒåŒ…å«å®Œæ•´çš„è¾©è®ºå†…å®¹ï¼Œ
        æ–‡ä»¶ååŸºäºæ—¶é—´æˆ³å’Œè¾©è®ºä¸»é¢˜ç”Ÿæˆã€‚
        """
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³å’Œç®€åŒ–çš„ä¸»é¢˜ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # æ¸…ç†é—®é¢˜ä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†ï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
        safe_question = re.sub(r'[\\/*?:"<>|]', '', question)[:30].strip()
        if not safe_question:
            safe_question = "Debate" if CURRENT_LANGUAGE == "en" else "è¾©è®º"
        
        if CURRENT_LANGUAGE == "en":
            filename = f"Debate_Record_{timestamp}_{safe_question}.txt"
        else:
            filename = f"è¾©è®ºè®°å½•_{timestamp}_{safe_question}.txt"
        filepath = os.path.join(r"C:\Users\yuangu114514\Desktop", filename)
        
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                f.write("=" * 60 + "\n")
                if CURRENT_LANGUAGE == "en":
                    f.write("ğŸ¤– MACP Debate Record\n")
                else:
                    f.write("ğŸ¤– MACP è¾©è®ºè®°å½•\n")
                f.write("=" * 60 + "\n\n")
                
                if CURRENT_LANGUAGE == "en":
                    f.write(f"ğŸ“… Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ğŸ¯ Debate Topic: {question}\n")
                    f.write(f"ğŸ­ Debaters: {role1} vs {role2}\n")
                    f.write(f"ğŸ“Š Session ID: {self.session_id}\n\n")
                    f.write("-" * 60 + "\n")
                    f.write("ğŸ“œ Debate Content\n")
                else:
                    f.write(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ğŸ¯ è¾©è®ºä¸»é¢˜: {question}\n")
                    f.write(f"ğŸ­ è¾©è®ºåŒæ–¹: {role1} vs {role2}\n")
                    f.write(f"ğŸ“Š ä¼šè¯ID: {self.session_id}\n\n")
                    f.write("-" * 60 + "\n")
                    f.write("ğŸ“œ è¾©è®ºå†…å®¹\n")
                f.write("-" * 60 + "\n\n")
                
                for entry in debate_round:
                    round_num = entry.get("round", "?")
                    speaker = entry.get("speaker", "Unknown" if CURRENT_LANGUAGE == "en" else "æœªçŸ¥")
                    content = entry.get("content", "")
                    entry_type = entry.get("type", "")
                    
                    type_label = ""
                    if entry_type == "opening":
                        type_label = "[Opening Statement]" if CURRENT_LANGUAGE == "en" else "[å¼€åœºé™ˆè¿°]"
                    elif entry_type == "rebuttal":
                        type_label = "[Rebuttal]" if CURRENT_LANGUAGE == "en" else "[åé©³]"
                    
                    if CURRENT_LANGUAGE == "en":
                        f.write(f"ã€Round {round_num}ã€‘ {speaker} {type_label}\n")
                    else:
                        f.write(f"ã€ç¬¬{round_num}å›åˆã€‘ {speaker} {type_label}\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"{content}\n\n")
                
                f.write("=" * 60 + "\n")
                if CURRENT_LANGUAGE == "en":
                    f.write("End of Debate Record\n")
                else:
                    f.write("è¾©è®ºè®°å½•ç»“æŸ\n")
                f.write("=" * 60 + "\n")
            
            if CURRENT_LANGUAGE == "en":
                print(f"âœ… Debate record saved to: {filepath}")
            else:
                print(f"âœ… è¾©è®ºè®°å½•å·²ä¿å­˜åˆ°: {filepath}")
            logger.info(f"è¾©è®ºè®°å½•å·²å•ç‹¬ä¿å­˜åˆ°: {filepath}")
            
        except (OSError, IOError) as e:
            print(f"âŒ ä¿å­˜è¾©è®ºè®°å½•å¤±è´¥: {e}")
            logger.error(f"ä¿å­˜è¾©è®ºè®°å½•åˆ°å•ç‹¬æ–‡ä»¶å¤±è´¥: {e}")

    def _setup_debate_roles(self, question: str, role1: str, role2: str):
        """è®¾ç½®è¾©è®ºè§’è‰²å¹¶æ‰§è¡Œæ ‡ç­¾æ£€æµ‹

        ä¸ºè¾©è®ºåŒæ–¹é…ç½®åˆé€‚çš„è§’è‰²ï¼Œå¹¶æ ¹æ®é—®é¢˜å†…å®¹æ£€æµ‹ç›¸å…³æ ‡ç­¾ï¼š
        1. åº”ç”¨è§’è‰²çš„ç«‹åœºåå¥½ï¼ˆå°¤å…¶æ˜¯è¾©è®ºæ‰‹çš„æ­£åæ–¹æœºåˆ¶ï¼‰
        2. åˆ†æé—®é¢˜å†…å®¹ï¼Œæ£€æµ‹ç›¸å…³çš„ä¸“ä¸šé¢†åŸŸæ ‡ç­¾
        3. æ˜¾ç¤ºæ£€æµ‹åˆ°çš„æ ‡ç­¾ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£AIçš„ä¸“ä¸šèƒŒæ™¯

        è¿™ç¡®ä¿äº†è¾©è®ºåŒæ–¹èƒ½å¤Ÿä»åˆé€‚çš„ä¸“ä¸šè§’åº¦å’Œç«‹åœºå‚ä¸è®¨è®º
        """
        # æ£€æµ‹æ ‡ç­¾
        if self.config.enable_tags:
            tags = role_system.detect_tags(question)
            if tags:
                logger.info(f"ğŸ·ï¸  æ£€æµ‹åˆ°æ ‡ç­¾: {', '.join(tags)}")
                print(f"ğŸ·ï¸  æ£€æµ‹åˆ°æ ‡ç­¾: {', '.join(tags)}")

        print(f"ğŸ­ è¾©è®ºè§’è‰²: {role1} vs {role2}")

    def competition_debate(self, question: str, role1: str = None, role2: str = None, rounds: int = 3) -> List[Dict[str, Any]]:
        """è¾©è®ºèµ›æ¨¡å¼ - åŒæ–¹å¯¹æŠ—ï¼Œæœ€åç”±è£åˆ¤åˆ¤å®šèƒœè´Ÿ
        
        ä¸æ™®é€šè¾©è®ºæ¨¡å¼ä¸åŒï¼š
        1. æ™®é€šè¾©è®ºæ¨¡å¼ï¼šå¯»æ±‚å…±è¯†ï¼Œè¾¾æˆä¸€è‡´ç»“è®º
        2. è¾©è®ºèµ›æ¨¡å¼ï¼šå¯¹æŠ—è¾©è®ºï¼Œæœ€ååˆ¤å®šè°èµ¢è°è¾“
        
        æµç¨‹ï¼š
        1. åŒæ–¹è¿›è¡ŒæŒ‡å®šå›åˆæ•°çš„è¾©è®º
        2. åè°ƒAIä½œä¸ºè£åˆ¤è¿›è¡Œè¯„åˆ¤
        3. åˆ¤å®šèƒœè´Ÿå¹¶ç»™å‡ºç†ç”±
        4. æ€»ç»“å…±è¯†ç‚¹å’Œåˆ†æ­§ç‚¹
        """
        role1 = role1 or self.config.default_role_1
        role2 = role2 or self.config.default_role_2

        # è·å–å®¢æˆ·ç«¯å’Œæ¨¡å‹
        client1, model_id1, is_api1 = self._get_client_for_model(self.config.model_1)
        client2, model_id2, is_api2 = self._get_client_for_model(self.config.model_2)

        actual_model1 = model_id1 if is_api1 else self.config.model_1
        actual_model2 = model_id2 if is_api2 else self.config.model_2
        display_name1 = f"{actual_model1}-{role1}"
        display_name2 = f"{actual_model2}-{role2}"

        role_prompt1 = role_system.get_role_prompt(role1, is_first=True)
        role_prompt2 = role_system.get_role_prompt(role2, is_first=False)

        if not role_prompt1 or not role_prompt2:
            raise InvalidRoleError(f"æ— æ•ˆè§’è‰²: {role1} æˆ– {role2}")

        debate_round = []

        # åˆ†æé—®é¢˜ç±»å‹
        question_analysis = analyze_question_type(question)
        accuracy_required = question_analysis["accuracy_required"]
        
        if CURRENT_LANGUAGE == "en":
            mode_instruction = ANTI_HALLUCINATION_PROMPT_EN if accuracy_required else PHILOSOPHICAL_PROMPT_EN
        else:
            mode_instruction = ANTI_HALLUCINATION_PROMPT_ZH if accuracy_required else PHILOSOPHICAL_PROMPT_ZH

        if CURRENT_LANGUAGE == "en":
            print(f"\nğŸ† Competition Mode: {role1} (Pro) vs {role2} (Con)")
            print(f"ğŸ“‹ Proposition: {question}")
            print(f"â±ï¸ Rounds: {rounds}")
        else:
            print(f"\nğŸ† è¾©è®ºèµ›æ¨¡å¼ (Competition Mode)ï¼š{role1}ï¼ˆæ­£æ–¹/Proï¼‰ vs {role2}ï¼ˆåæ–¹/Conï¼‰")
            print(f"ğŸ“‹ è¾©é¢˜ (Proposition)ï¼š{question}")
            print(f"â±ï¸ å›åˆæ•° (Rounds)ï¼š{rounds}")

        # ç¬¬ä¸€å›åˆï¼šå¼€åœºé™ˆè¿°
        DisplayManager.print_separator("-", 40)
        print("ç¬¬1å›åˆï¼šå¼€åœºé™ˆè¿° (Round 1: Opening Statements)")
        DisplayManager.print_separator("-", 40)

        # æ­£æ–¹å¼€åœº - å§‹ç»ˆä½¿ç”¨ä¸­è‹±åŒè¯­æç¤ºè¯è®©AIç”¨è‹±æ–‡å›ç­”
        lang_instruction = "\n**IMPORTANT: You MUST respond entirely in English.**\n"
        prompt1 = f"""{role_prompt1}
{lang_instruction}
{mode_instruction}

ã€Competition Debate / è¾©è®ºèµ›ã€‘
Proposition / è¾©é¢˜: {question}

You are the PRO side. You must SUPPORT this proposition.
ä½ æ˜¯æ­£æ–¹ã€‚ä½ å¿…é¡»ã€æ”¯æŒã€‘è¿™ä¸ªå‘½é¢˜ã€‚
Please present your opening statement with 3-5 key arguments.
Be persuasive and logical. You will be judged on the strength of your arguments."""

        print(f"\nğŸ“¢ {display_name1}ï¼ˆæ­£æ–¹/Proï¼‰ï¼š", end="", flush=True)
        
        if is_api1:
            result1 = client1.generate_response(prompt1, streaming=True)
        else:
            result1 = client1._generate_streaming_response(
                model_id1, prompt1, timeout=self.config.timeout,
                speaker_name=f"{display_name1}ï¼ˆæ­£æ–¹ï¼‰" if CURRENT_LANGUAGE == "zh" else f"{display_name1} (Pro)"
            )
        
        response1 = result1.get("response", "")
        debate_round.append({"round": 1, "speaker": display_name1, "content": response1, "type": "opening", "side": "pro"})
        print()

        # åæ–¹å¼€åœº - å§‹ç»ˆä½¿ç”¨ä¸­è‹±åŒè¯­æç¤ºè¯è®©AIç”¨è‹±æ–‡å›ç­”
        prompt2 = f"""{role_prompt2}
{lang_instruction}
{mode_instruction}

ã€Competition Debate / è¾©è®ºèµ›ã€‘
Proposition / è¾©é¢˜: {question}

You are the CON side. You must OPPOSE this proposition.
ä½ æ˜¯åæ–¹ã€‚ä½ å¿…é¡»ã€åå¯¹ã€‘è¿™ä¸ªå‘½é¢˜ã€‚
The PRO side argued / æ­£æ–¹çš„è®ºç‚¹: {response1[:500]}...

Please present your opening statement with 3-5 key arguments.
Be persuasive and logical. You will be judged on the strength of your arguments."""

        print(f"\nğŸ“¢ {display_name2}ï¼ˆåæ–¹/Conï¼‰ï¼š", end="", flush=True)
        
        if is_api2:
            result2 = client2.generate_response(prompt2, streaming=True)
        else:
            result2 = client2._generate_streaming_response(
                model_id2, prompt2, timeout=self.config.timeout,
                speaker_name=f"{display_name2}ï¼ˆåæ–¹ï¼‰" if CURRENT_LANGUAGE == "zh" else f"{display_name2} (Con)"
            )
        
        response2 = result2.get("response", "")
        debate_round.append({"round": 1, "speaker": display_name2, "content": response2, "type": "opening", "side": "con"})
        print()

        # åç»­å›åˆï¼šåé©³
        for round_num in range(2, rounds + 1):
            DisplayManager.print_separator("-", 40)
            if CURRENT_LANGUAGE == "en":
                print(f"Round {round_num}: Rebuttal")
            else:
                print(f"ç¬¬{round_num}å›åˆï¼šåé©³")
            DisplayManager.print_separator("-", 40)

            # æ­£æ–¹åé©³
            last_con_response = debate_round[-1]["content"] if debate_round[-1]["side"] == "con" else response2
            
            if CURRENT_LANGUAGE == "en":
                rebuttal_prompt1 = f"""{role_prompt1}
{lang_instruction}

ã€Competition Debate - Round {round_num}ã€‘
Proposition: {question}
You are PRO side.

CON side's argument: {last_con_response[:600]}...

Please rebut the CON side's arguments and strengthen your position.
Point out flaws in their logic, provide counter-evidence, and reinforce your core arguments."""
            else:
                rebuttal_prompt1 = f"""{role_prompt1}

ã€è¾©è®ºèµ› - ç¬¬{round_num}å›åˆã€‘
è¾©é¢˜ï¼š{question}
ä½ æ˜¯æ­£æ–¹ã€‚

åæ–¹çš„è®ºç‚¹ï¼š{last_con_response[:600]}...

è¯·åé©³åæ–¹çš„è®ºç‚¹å¹¶å¼ºåŒ–ä½ çš„ç«‹åœºã€‚
æŒ‡å‡ºå¯¹æ–¹çš„é€»è¾‘æ¼æ´ï¼Œæä¾›åè¯ï¼Œå¹¶å¼ºåŒ–ä½ çš„æ ¸å¿ƒè®ºç‚¹ã€‚"""

            print(f"\nğŸ“¢ {display_name1}ï¼ˆæ­£æ–¹ï¼‰åé©³ï¼š", end="", flush=True)
            
            if is_api1:
                result1 = client1.generate_response(rebuttal_prompt1, streaming=True)
            else:
                result1 = client1._generate_streaming_response(
                    model_id1, rebuttal_prompt1, timeout=self.config.timeout,
                    speaker_name=f"{display_name1} åé©³" if CURRENT_LANGUAGE == "zh" else f"{display_name1} Rebuttal"
                )
            
            response1 = result1.get("response", "")
            debate_round.append({"round": round_num, "speaker": display_name1, "content": response1, "type": "rebuttal", "side": "pro"})
            print()

            # åæ–¹åé©³
            if CURRENT_LANGUAGE == "en":
                rebuttal_prompt2 = f"""{role_prompt2}
{lang_instruction}

ã€Competition Debate - Round {round_num}ã€‘
Proposition: {question}
You are CON side.

PRO side's argument: {response1[:600]}...

Please rebut the PRO side's arguments and strengthen your position.
Point out flaws in their logic, provide counter-evidence, and reinforce your core arguments."""
            else:
                rebuttal_prompt2 = f"""{role_prompt2}

ã€è¾©è®ºèµ› - ç¬¬{round_num}å›åˆã€‘
è¾©é¢˜ï¼š{question}
ä½ æ˜¯åæ–¹ã€‚

æ­£æ–¹çš„è®ºç‚¹ï¼š{response1[:600]}...

è¯·åé©³æ­£æ–¹çš„è®ºç‚¹å¹¶å¼ºåŒ–ä½ çš„ç«‹åœºã€‚
æŒ‡å‡ºå¯¹æ–¹çš„é€»è¾‘æ¼æ´ï¼Œæä¾›åè¯ï¼Œå¹¶å¼ºåŒ–ä½ çš„æ ¸å¿ƒè®ºç‚¹ã€‚"""

            print(f"\nğŸ“¢ {display_name2}ï¼ˆåæ–¹ï¼‰åé©³ï¼š", end="", flush=True)
            
            if is_api2:
                result2 = client2.generate_response(rebuttal_prompt2, streaming=True)
            else:
                result2 = client2._generate_streaming_response(
                    model_id2, rebuttal_prompt2, timeout=self.config.timeout,
                    speaker_name=f"{display_name2} åé©³" if CURRENT_LANGUAGE == "zh" else f"{display_name2} Rebuttal"
                )
            
            response2 = result2.get("response", "")
            debate_round.append({"round": round_num, "speaker": display_name2, "content": response2, "type": "rebuttal", "side": "con"})
            print()

        # è£åˆ¤è¯„åˆ¤
        DisplayManager.print_separator("=", 60)
        if CURRENT_LANGUAGE == "en":
            print("ğŸ›ï¸ JUDGE'S VERDICT")
        else:
            print("ğŸ›ï¸ è£åˆ¤è¯„åˆ¤")
        DisplayManager.print_separator("=", 60)
        
        self._judge_competition(question, debate_round, role1, role2, display_name1, display_name2)

        # è¯¢é—®æ˜¯å¦ä¿å­˜
        self._ask_save_debate_log(question, debate_round, display_name1, display_name2)

        return debate_round

    def _judge_competition(self, question: str, debate_round: List[Dict[str, Any]], 
                          role1: str, role2: str, display_name1: str, display_name2: str):
        """è£åˆ¤AIè¯„åˆ¤è¾©è®ºèµ›èƒœè´Ÿï¼ˆæµå¼è¾“å‡ºï¼‰"""
        if CURRENT_LANGUAGE == "en":
            print(f"\nğŸ¤– Judge ({self.config.coordinator_model}) evaluating...")
            print("âš–ï¸ Verdict: ", end="", flush=True)
        else:
            print(f"\nğŸ¤– è£åˆ¤AI ({self.config.coordinator_model}) æ­£åœ¨è¯„åˆ¤...")
            print("âš–ï¸ è¯„åˆ¤: ", end="", flush=True)

        # æ„å»ºè¾©è®ºæ‘˜è¦
        debate_summary = ""
        for entry in debate_round:
            side = "Pro" if entry["side"] == "pro" else "Con"
            debate_summary += f"\nã€{side} - Round {entry['round']}ã€‘ {entry['speaker']}:\n{entry['content'][:300]}...\n"

        # æ„å»ºè£åˆ¤æç¤ºè¯
        if CURRENT_LANGUAGE == "en":
            judge_prompt = f"""You are an impartial debate judge. Please evaluate the following debate competition:

ã€Propositionã€‘: {question}
ã€PRO Sideã€‘: {display_name1}
ã€CON Sideã€‘: {display_name2}

ã€Debate Recordã€‘:
{debate_summary}

Please provide your verdict with the following structure:

## ğŸ† Winner Announcement
**Winner: [PRO/CON side]** - [One sentence reason]

## ğŸ“Š Scoring (out of 10 for each)
| Criterion | PRO | CON |
|-----------|-----|-----|
| Argument Strength | X | X |
| Logic Rigor | X | X |
| Rebuttal Effectiveness | X | X |
| Evidence Quality | X | X |
| **Total** | XX | XX |

## ğŸ¤ Consensus Points (MUST list at least 2)
1. [First point both sides agree on]
2. [Second point both sides agree on]

## âš”ï¸ Key Disagreements (MUST list at least 2)
1. [First major disagreement]
2. [Second major disagreement]

## ğŸ’¬ Judge's Comments
- PRO side's strengths and weaknesses
- CON side's strengths and weaknesses
- Key moments that influenced the verdict

## ğŸ’¡ Final Recommendation
- Your neutral perspective on the proposition
- Advice for the user on this topic

Please be fair and objective in your judgment."""
        else:
            judge_prompt = f"""ä½ æ˜¯ä¸€ä½å…¬æ­£çš„è¾©è®ºèµ›è£åˆ¤ã€‚è¯·è¯„åˆ¤ä»¥ä¸‹è¾©è®ºèµ›ï¼š

ã€è¾©é¢˜ã€‘ï¼š{question}
ã€æ­£æ–¹ã€‘ï¼š{display_name1}
ã€åæ–¹ã€‘ï¼š{display_name2}

ã€è¾©è®ºè®°å½•ã€‘ï¼š
{debate_summary}

è¯·æŒ‰ä»¥ä¸‹ç»“æ„ç»™å‡ºä½ çš„è£å†³ï¼š

## ğŸ† èƒœè´Ÿå®£å¸ƒ
**è·èƒœæ–¹ï¼š[æ­£æ–¹/åæ–¹]** - [ä¸€å¥è¯ç†ç”±]

## ğŸ“Š è¯„åˆ†ï¼ˆæ¯é¡¹æ»¡åˆ†10åˆ†ï¼‰
| è¯„åˆ¤é¡¹ | æ­£æ–¹ | åæ–¹ |
|--------|------|------|
| è®ºç‚¹å¼ºåº¦ | X | X |
| é€»è¾‘ä¸¥è°¨ | X | X |
| åé©³æœ‰æ•ˆæ€§ | X | X |
| è®ºæ®è´¨é‡ | X | X |
| **æ€»åˆ†** | XX | XX |

## ğŸ¤ å…±è¯†ç‚¹ï¼ˆã€å¿…é¡»ã€‘åˆ—å‡ºè‡³å°‘2ç‚¹ï¼‰
1. [åŒæ–¹éƒ½è®¤åŒçš„ç¬¬ä¸€ä¸ªè§‚ç‚¹]
2. [åŒæ–¹éƒ½è®¤åŒçš„ç¬¬äºŒä¸ªè§‚ç‚¹]

## âš”ï¸ æ ¸å¿ƒåˆ†æ­§ï¼ˆã€å¿…é¡»ã€‘åˆ—å‡ºè‡³å°‘2ç‚¹ï¼‰
1. [ç¬¬ä¸€ä¸ªä¸»è¦åˆ†æ­§]
2. [ç¬¬äºŒä¸ªä¸»è¦åˆ†æ­§]

## ğŸ’¬ è£åˆ¤ç‚¹è¯„
- æ­£æ–¹çš„ä¼˜ç‚¹ä¸ä¸è¶³
- åæ–¹çš„ä¼˜ç‚¹ä¸ä¸è¶³
- å½±å“è£å†³çš„å…³é”®æ—¶åˆ»

## ğŸ’¡ æœ€ç»ˆå»ºè®®
- ä½ å¯¹è¿™ä¸ªè¾©é¢˜çš„ä¸­ç«‹çœ‹æ³•
- ç»™ç”¨æˆ·å…³äºè¿™ä¸ªé—®é¢˜çš„å»ºè®®

è¯·ä¿æŒå…¬æ­£å®¢è§‚çš„æ€åº¦è¿›è¡Œè£å†³ã€‚"""

        coord_client, coord_model, is_api = self._get_client_for_model(self.config.coordinator_model)
        
        # ä½¿ç”¨æµå¼è¾“å‡º
        if is_api:
            judge_result = coord_client.generate_response(
                judge_prompt, 
                max_tokens=1500, 
                temperature=0.7,
                streaming=True
            )
        else:
            judge_result = coord_client._generate_streaming_response(
                coord_model, 
                judge_prompt, 
                max_tokens=1500,
                temperature=0.7, 
                timeout=self.config.timeout,
                speaker_name="âš–ï¸ è£å†³" if CURRENT_LANGUAGE == "zh" else "âš–ï¸ Verdict"
            )

        print()  # æ¢è¡Œ
        
        if judge_result.get("success"):
            if CURRENT_LANGUAGE == "en":
                print(f"\nâœ… Judgment complete")
            else:
                print(f"\nâœ… è¯„åˆ¤å®Œæˆ")
        else:
            if CURRENT_LANGUAGE == "en":
                print(f"\nâŒ Judgment failed")
            else:
                print(f"\nâŒ è¯„åˆ¤å¤±è´¥")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.config.save_history:
            self.history_manager.save_history()
        logger.info("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

# ==================== ã€ç”¨æˆ·äº¤äº’ç•Œé¢ã€‘ ====================
# å‘½ä»¤è¡Œç”¨æˆ·ç•Œé¢ï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥å’Œç³»ç»Ÿè¾“å‡º

class InteractiveInterface:
    """MACPå‘½ä»¤è¡Œäº¤äº’ç•Œé¢

    æä¾›ç”¨æˆ·å‹å¥½çš„å‘½ä»¤è¡Œç•Œé¢ï¼š
    - å‘½ä»¤è§£æå’Œæ‰§è¡Œ
    - èœå•æ˜¾ç¤ºå’Œå¯¼èˆª
    - ç”¨æˆ·è¾“å…¥éªŒè¯
    - ç»“æœæ ¼å¼åŒ–è¾“å‡º

    æ”¯æŒçš„ä¸»è¦å‘½ä»¤ï¼š
    - ç›´æ¥æé—®ï¼ˆå¹¶è¡Œæ¨¡å¼ï¼‰
    - /debateï¼ˆè¾©è®ºæ¨¡å¼ï¼‰
    - /turtleï¼ˆæµ·é¾Ÿæ±¤æ¨¡å¼ï¼‰
    - /consensusï¼ˆå…±è¯†é…ç½®ï¼‰
    - /helpï¼ˆå¸®åŠ©ä¿¡æ¯ï¼‰
    """

    def __init__(self, scheduler: AICouncilScheduler):
        self.scheduler = scheduler

    def run(self):
        """è¿è¡Œäº¤äº’ç•Œé¢"""
        self._print_welcome()
        self._print_commands()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é…ç½®APIï¼ˆç”¨æˆ·åœ¨å¯åŠ¨æ—¶é€‰æ‹©äº†APIæ¨¡å¼ä½†æ²¡æœ‰Ollamaï¼‰
        global NEED_API_SETUP
        if NEED_API_SETUP:
            print("\n" + "=" * 60)
            print("ğŸŒ æ£€æµ‹åˆ°æ‚¨é€‰æ‹©äº† API æ¨¡å¼ï¼Œç°åœ¨å¼€å§‹é…ç½®")
            print("   (Detected API mode selection, starting configuration)")
            print("=" * 60)
            self._configure_api_mode()
            NEED_API_SETUP = False

        while True:
            try:
                user_input = input(f"\nğŸ“ è¯·è¾“å…¥é—®é¢˜æˆ–å‘½ä»¤ (Enter question or command)ï¼š").strip()

                if not user_input:
                    continue

                if user_input.startswith('/'):
                    self._handle_command(user_input[1:])
                else:
                    self._handle_question(user_input)

            except KeyboardInterrupt:
                self._handle_interrupt()
            except Exception as e:
                logger.error(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
                print(f"âŒ å‘ç”Ÿé”™è¯¯ (Error occurred)ï¼š{e}")

    @staticmethod
    def _print_welcome():
        """æ‰“å°æ¬¢è¿ä¿¡æ¯ (Print welcome message)"""
        DisplayManager.print_header("ğŸ¤– MACP å¤šAIåä½œå¹³å° (Multi-AI Collaboration Platform) v5.0")
        print(f"æ¨¡å‹1 (Model 1)ï¼š{config.model_1}")
        print(f"æ¨¡å‹2 (Model 2)ï¼š{config.model_2}")
        print(f"åè°ƒæ¨¡å‹ (Coordinator)ï¼š{config.coordinator_model}")
        opt_status = "å¼€å¯/Enabled" if config.optimize_memory else "å…³é—­/Disabled"
        print(f"ä¼˜åŒ–æ¨¡å¼ (Optimize Mode)ï¼š{opt_status}")
        DisplayManager.print_separator()

    @staticmethod
    def _print_commands():
        """æ‰“å°å¯ç”¨å‘½ä»¤"""
        print(f"\n{get_text('available_commands')}")
        
        # å‘½ä»¤åˆ—è¡¨ï¼ˆä¸­è‹±åŒè¯­ï¼‰
        commands = [
            ("help", "æ˜¾ç¤ºå¸®åŠ© (Show help)"),
            ("models", "æŸ¥çœ‹å¯ç”¨æ¨¡å‹ (View available models)"),
            ("config", "æŸ¥çœ‹å½“å‰é…ç½® (View current config)"),
            ("history", "æŸ¥çœ‹å†å²è®°å½• (View history)"),
            ("api", "é…ç½®APIæ¨¡å¼ (Configure API mode)"),
            ("debate", "è¾©è®ºæ¨¡å¼-å¯»æ±‚å…±è¯† (Debate mode - seek consensus)"),
            ("competition", "è¾©è®ºèµ›æ¨¡å¼-åˆ¤å®šèƒœè´Ÿ (Competition - judge winner)"),
            ("turtle", "æµ·é¾Ÿæ±¤æ¨¡å¼ (Turtle soup mode)"),
            ("consensus", "é…ç½®å…±è¯†æ£€æµ‹ (Configure consensus detection)"),
            ("streaming", "åˆ‡æ¢æµå¼è¾“å‡º (Toggle streaming output)"),
            ("optimize", "å¼€å¯ä¼˜åŒ–æ¨¡å¼ (Enable optimize mode)"),
            ("roles", "æŸ¥çœ‹å¯ç”¨è§’è‰² (View available roles)"),
            ("tags", "æŸ¥çœ‹æ ‡ç­¾ç³»ç»Ÿ (View tag system)"),
            ("mode", "åˆ‡æ¢åè°ƒæ¨¡å¼ (Switch coordination mode)"),
            ("addai", "æ·»åŠ æ–°AIæ¨¡å‹ (Add new AI model)"),
            ("listai", "åˆ—å‡ºæ‰€æœ‰AIæ¨¡å‹ (List all AI models)"),
            ("removeai", "ç§»é™¤AIæ¨¡å‹ (Remove AI model)"),
            ("language", "åˆ‡æ¢è¯­è¨€ (Switch language)"),
            ("clear", "æ¸…å± (Clear screen)"),
            ("exit", "é€€å‡ºç¨‹åº (Exit program)")
        ]

        for cmd, desc in commands:
            print(f"  /{cmd:<12} - {desc}")
        DisplayManager.print_separator()

    def _handle_question(self, question: str):
        """å¤„ç†é—®é¢˜è¾“å…¥ (Handle question input)"""
        print(f"\nğŸ” æ­£åœ¨å¤„ç†é—®é¢˜ (Processing question)...")
        self.scheduler.progress_tracker.start()

        try:
            self.scheduler.ask_both_models(question, mode="parallel")
            total_time = self.scheduler.progress_tracker.get_elapsed_time()
            print(f"\nâœ… æ€»è€—æ—¶ (Total time)ï¼š{total_time:.2f}ç§’/s")
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜å¤±è´¥: {e}")
            print(f"âŒ å¤„ç†é—®é¢˜å¤±è´¥ (Failed to process question): {e}")

    def _handle_command(self, command: str):
        """å¤„ç†å‘½ä»¤"""
        command = command.lower()

        handlers = {
            'help': self._print_commands,
            'models': self._show_models,
            'config': self._show_config,
            'history': self._show_history,
            'api': self._configure_api_mode,
            'debate': self._enter_debate_mode,
            'competition': self._enter_competition_mode,
            'turtle': self._enter_turtle_soup_mode,
            'consensus': self._configure_consensus,
            'optimize': self._toggle_optimize_mode,
            'roles': self._show_roles,
            'tags': self._show_tags,
            'mode': self._toggle_coordination_mode,
            'streaming': self._toggle_streaming_mode,
            'language': self._switch_language,
            'addai': self._add_ai_model,
            'listai': self._list_ai_models,
            'removeai': self._remove_ai_model,
            'clear': DisplayManager.clear_screen,
            'exit': self._exit_program
        }

        handler = handlers.get(command)
        if handler:
            try:
                handler()
            except Exception as e:
                logger.error(f"æ‰§è¡Œå‘½ä»¤ /{command} å¤±è´¥: {e}")
                print(f"âŒ æ‰§è¡Œå‘½ä»¤å¤±è´¥ (Command execution failed): {e}")
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤ (Unknown command)ï¼š/{command}")

    def _show_models(self):
        """æ˜¾ç¤ºå¯ç”¨æ¨¡å‹ (Show available models)"""
        print("\nğŸ“¦ æ£€æŸ¥å¯ç”¨æ¨¡å‹ (Checking available models)...")
        models = self.scheduler.client.list_models()
        print(DisplayManager.format_model_list(models))

    @staticmethod
    def _show_config():
        """æ˜¾ç¤ºå½“å‰é…ç½® (Show current config)"""
        config_dict = config.to_dict()
        print(DisplayManager.format_config_display(config_dict))

    def _show_history(self):
        """æ˜¾ç¤ºå†å²è®°å½• (Show history)"""
        print(f"\nğŸ“œ å†å²è®°å½• (History) | ä¼šè¯ID (Session ID)ï¼š{self.scheduler.session_id}")
        history = self.scheduler.history_manager.get_recent_history(5)

        if history:
            for i, entry in enumerate(history, 1):
                timestamp = entry.get('timestamp', '')[:16]
                entry_type = entry.get('type', 'unknown')
                question = entry.get('question', '')[:60]
                print(f"\n  [{i}] {timestamp} - {entry_type}")
                print(f"      é—®é¢˜ (Question)ï¼š{question}...")
        else:
            print("  æš‚æ— å†å²è®°å½• (No history records)")

    def _enter_debate_mode(self):
        """è¿›å…¥è¾©è®ºæ¨¡å¼ (Enter debate mode)"""
        DisplayManager.print_header("ğŸ’¬ è¾©è®ºæ¨¡å¼ (Debate Mode)")
        print("\né€‰æ‹©åè°ƒæ¨¡å¼ (Select coordination mode)ï¼š")
        print("  1. AIè‡ªåŠ¨åè°ƒ (AI Auto-coordination) [é»˜è®¤/default]")
        print("  2. ç”¨æˆ·æ‰‹åŠ¨åè°ƒ (User Manual coordination)")
        mode_choice = input("é€‰æ‹©/Select (1/2): ").strip()
        if mode_choice == "2":
            config.coordination_mode = "user"
            print("âœ… å·²é€‰æ‹©ç”¨æˆ·åè°ƒæ¨¡å¼ (User coordination mode selected)")
        else:
            config.coordination_mode = "auto"
            print("âœ… å·²é€‰æ‹©AIè‡ªåŠ¨åè°ƒæ¨¡å¼ (AI auto-coordination mode selected)")

        # è¾“å…¥é—®é¢˜
        question = input("\nè¯·è¾“å…¥è¾©è®ºé—®é¢˜ (Enter debate topic)ï¼š").strip()
        if not question:
            print("âŒ é—®é¢˜ä¸èƒ½ä¸ºç©º (Topic cannot be empty)")
            return

        # é€‰æ‹©è§’è‰²
        role1, role2 = self._select_debate_roles()

        # å›åˆæ•°
        rounds_input = input(f"\nè¾©è®ºå›åˆæ•° (Debate rounds) [é»˜è®¤/default:{config.debate_rounds}]: ").strip()
        if rounds_input.isdigit():
            config.debate_rounds = int(rounds_input)

        # å¼€å§‹è¾©è®º
        print(f"\nğŸ¬ å¼€å§‹è¾©è®º (Starting debate)ï¼š{role1} vs {role2}")
        print(f"é—®é¢˜ (Topic)ï¼š{question}")
        DisplayManager.print_separator()

        self.scheduler.progress_tracker.start()
        try:
            self.scheduler.ask_both_models(question, mode="debate", role1=role1, role2=role2)
            total_time = self.scheduler.progress_tracker.get_elapsed_time()
            print(f"\nâœ… è¾©è®ºå®Œæˆ (Debate complete) | æ€»è€—æ—¶ (Total time)ï¼š{total_time:.2f}ç§’/s")
        except Exception as e:
            logger.error(f"è¾©è®ºå¤±è´¥: {e}")
            print(f"âŒ è¾©è®ºå¤±è´¥ (Debate failed): {e}")

    def _enter_competition_mode(self):
        """è¿›å…¥è¾©è®ºèµ›æ¨¡å¼ (Enter competition mode)"""
        DisplayManager.print_header("ğŸ† è¾©è®ºèµ›æ¨¡å¼ (Competition Mode)")
        print("\nğŸ¯ åœ¨æ­¤æ¨¡å¼ä¸‹ï¼ŒAIåŒæ–¹å°†è¿›è¡Œå¯¹æŠ—è¾©è®ºï¼Œæœ€åç”±è£åˆ¤AIåˆ¤å®šèƒœè´Ÿã€‚")
        print("   (In this mode, AI debaters will argue, and a judge will determine the winner.)")
        print("è¿™ä¸æ™®é€šè¾©è®ºæ¨¡å¼ï¼ˆå¯»æ±‚å…±è¯†ï¼‰ä¸åŒã€‚")
        print("   (This is different from debate mode which seeks consensus.)\n")

        # è¾“å…¥è¾©é¢˜
        question = input("è¯·è¾“å…¥è¾©è®ºå‘½é¢˜ (Enter debate proposition)ï¼š").strip()
        if not question:
            print("âŒ å‘½é¢˜ä¸èƒ½ä¸ºç©º (Proposition cannot be empty)")
            return

        # é€‰æ‹©è§’è‰²
        role1, role2 = self._select_debate_roles()

        # å›åˆæ•°
        rounds_input = input(f"\nè¾©è®ºå›åˆæ•° (Debate rounds) [é»˜è®¤/default:3]: ").strip()
        
        rounds = int(rounds_input) if rounds_input.isdigit() else 3

        # å¼€å§‹è¾©è®ºèµ›
        print(f"\nğŸ¬ å¼€å§‹è¾©è®ºèµ› (Starting competition)ï¼š{role1}ï¼ˆæ­£æ–¹/Proï¼‰ vs {role2}ï¼ˆåæ–¹/Conï¼‰")
        print(f"è¾©é¢˜ (Proposition)ï¼š{question}")
        print(f"å›åˆæ•° (Rounds)ï¼š{rounds}")
        DisplayManager.print_separator()

        self.scheduler.progress_tracker.start()
        try:
            self.scheduler.competition_debate(question, role1=role1, role2=role2, rounds=rounds)
            total_time = self.scheduler.progress_tracker.get_elapsed_time()
            print(f"\nâœ… è¾©è®ºèµ›å®Œæˆ (Competition complete) | æ€»è€—æ—¶ (Total time)ï¼š{total_time:.2f}ç§’/s")
        except Exception as e:
            logger.error(f"è¾©è®ºèµ›å¤±è´¥: {e}")
            print(f"âŒ è¾©è®ºèµ›å¤±è´¥ (Competition failed): {e}")

    def _enter_turtle_soup_mode(self):
        """è¿›å…¥æµ·é¾Ÿæ±¤æ¨¡å¼ (Enter turtle soup mode)"""
        DisplayManager.print_header("ğŸ¢ æµ·é¾Ÿæ±¤æ¨¡å¼ (Turtle Soup Mode)")

        question = input("\nè¯·è¾“å…¥æµ·é¾Ÿæ±¤è°œé¢ (Enter riddle)ï¼š").strip()
        if not question:
            print("âŒ è°œé¢ä¸èƒ½ä¸ºç©º (Riddle cannot be empty)")
            return

        role1 = input("\nAI1è§’è‰² (AI1 role) [é»˜è®¤/default: ä¾¦æ¢/Detective]: ").strip() or "ä¾¦æ¢"
        role2 = input("AI2è§’è‰² (AI2 role) [é»˜è®¤/default: æ¨ç†è€…/Reasoner]: ").strip() or "æ¨ç†è€…"

        print(f"\nğŸ® å¼€å§‹æµ·é¾Ÿæ±¤æ¸¸æˆ (Starting Turtle Soup game)")
        print(f"è°œé¢ (Riddle)ï¼š{question}")
        print(f"AIè§’è‰² (AI roles)ï¼š{role1} å’Œ/and {role2}")
        DisplayManager.print_separator()

        try:
            self.scheduler.ask_both_models(question, mode="turtle_soup", role1=role1, role2=role2)
        except Exception as e:
            logger.error(f"æµ·é¾Ÿæ±¤æ¸¸æˆå¤±è´¥: {e}")
            print(f"âŒ æµ·é¾Ÿæ±¤æ¸¸æˆå¤±è´¥: {e}")

    @staticmethod
    def _toggle_optimize_mode():
        """åˆ‡æ¢ä¼˜åŒ–æ¨¡å¼ (Toggle optimize mode)"""
        config.optimize_memory = not config.optimize_memory
        status = "å¼€å¯/Enabled" if config.optimize_memory else "å…³é—­/Disabled"
        print(f"âœ… ä¼˜åŒ–æ¨¡å¼ (Optimize mode)ï¼š{status}")

    @staticmethod
    def _toggle_streaming_mode():
        """åˆ‡æ¢æµå¼è¾“å‡ºæ¨¡å¼ (Toggle streaming mode)"""
        config.streaming_output = not config.streaming_output
        status = "å¼€å¯/Enabled" if config.streaming_output else "å…³é—­/Disabled"
        mode_desc = "AIå›ç­”å°†é€å­—å®æ—¶æ˜¾ç¤º (Real-time display)" if config.streaming_output else "AIå›ç­”å°†ä¸€æ¬¡æ€§æ˜¾ç¤º (Display at once)"
        print(f"âœ… æµå¼è¾“å‡º (Streaming output)ï¼š{status}")
        print(f"   {mode_desc}")

    @staticmethod
    def _switch_language():
        """åˆ‡æ¢ç•Œé¢è¯­è¨€ / Switch interface language"""
        global CURRENT_LANGUAGE
        
        DisplayManager.print_header(get_text("language_title"))
        
        current = "ä¸­æ–‡ (Chinese)" if CURRENT_LANGUAGE == "zh" else "English (è‹±æ–‡)"
        print(f"{get_text('current_language')}: {current}")
        print()
        print(get_text("select_language"))
        print("  1. ä¸­æ–‡ (Chinese)")
        print("  2. English (è‹±æ–‡)")
        print()
        
        choice = input(">>> ").strip()
        
        if choice == "1":
            CURRENT_LANGUAGE = "zh"
            config.language = "zh"
            config.save_to_file(CONFIG_FILE_PATH)  # ä¿å­˜é…ç½®
            print("\nâœ… è¯­è¨€å·²åˆ‡æ¢ä¸ºä¸­æ–‡")
            print("   ç•Œé¢å°†ä»¥ä¸­æ–‡æ˜¾ç¤º")
            print("   âœ… è®¾ç½®å·²ä¿å­˜ï¼Œä¸‹æ¬¡å¯åŠ¨è‡ªåŠ¨ç”Ÿæ•ˆ")
        elif choice == "2":
            CURRENT_LANGUAGE = "en"
            config.language = "en"
            config.save_to_file(CONFIG_FILE_PATH)  # ä¿å­˜é…ç½®
            print("\nâœ… Language changed to English")
            print("   Interface will be displayed in English")
            print("   âœ… Settings saved, will take effect on next startup")
        else:
            if CURRENT_LANGUAGE == "en":
                print("âš ï¸ Invalid choice, language unchanged")
            else:
                print("âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œè¯­è¨€æœªæ”¹å˜")
        
        DisplayManager.print_separator()

    def _add_ai_model(self):
        """æ·»åŠ æ–°çš„AIæ¨¡å‹ï¼ˆæ”¯æŒæœ¬åœ°Ollamaå’ŒAPIï¼‰"""
        if CURRENT_LANGUAGE == "en":
            DisplayManager.print_header("â• Add New AI Model")
            print("Select AI type:")
            print("  1. Local Ollama model")
            print("  2. API model (OpenAI compatible)")
            print("  3. Cancel")
        else:
            DisplayManager.print_header("â• æ·»åŠ æ–°AIæ¨¡å‹")
            print("é€‰æ‹©AIç±»å‹ï¼š")
            print("  1. æœ¬åœ°Ollamaæ¨¡å‹")
            print("  2. APIæ¨¡å‹ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰")
            print("  3. å–æ¶ˆ")
        
        choice = input(">>> ").strip()
        
        if choice == "1":
            # æ·»åŠ æœ¬åœ°Ollamaæ¨¡å‹
            self._add_ollama_model()
        elif choice == "2":
            # æ·»åŠ APIæ¨¡å‹
            self._add_api_model()
        else:
            if CURRENT_LANGUAGE == "en":
                print("â­ï¸ Cancelled")
            else:
                print("â­ï¸ å·²å–æ¶ˆ")

    def _add_ollama_model(self):
        """æ·»åŠ æœ¬åœ°Ollamaæ¨¡å‹"""
        if CURRENT_LANGUAGE == "en":
            print("\nğŸ“¦ Available local Ollama models:")
        else:
            print("\nğŸ“¦ å¯ç”¨çš„æœ¬åœ°Ollamaæ¨¡å‹ï¼š")
        
        # è·å–Ollamaæ¨¡å‹åˆ—è¡¨
        try:
            models = self.scheduler.client.get_available_models()
            if models:
                for i, model in enumerate(models, 1):
                    print(f"  {i}. {model}")
                
                if CURRENT_LANGUAGE == "en":
                    model_input = input("\nSelect model number or enter model name: ").strip()
                else:
                    model_input = input("\né€‰æ‹©æ¨¡å‹ç¼–å·æˆ–è¾“å…¥æ¨¡å‹åç§°: ").strip()
                
                # è§£æè¾“å…¥
                if model_input.isdigit():
                    idx = int(model_input)
                    if 1 <= idx <= len(models):
                        model_name = models[idx - 1]
                    else:
                        print("âŒ Invalid selection" if CURRENT_LANGUAGE == "en" else "âŒ æ— æ•ˆé€‰æ‹©")
                        return
                else:
                    model_name = model_input
                
                # è¾“å…¥AIåç§°
                if CURRENT_LANGUAGE == "en":
                    ai_name = input(f"Enter a name for this AI (default: {model_name}): ").strip() or model_name
                else:
                    ai_name = input(f"ä¸ºè¿™ä¸ªAIèµ·ä¸ªåå­—ï¼ˆé»˜è®¤: {model_name}ï¼‰: ").strip() or model_name
                
                # æ·»åŠ åˆ°é…ç½®
                new_ai = {
                    "name": ai_name,
                    "type": "ollama",
                    "model": model_name,
                    "api_config": None
                }
                config.extra_ai_models.append(new_ai)
                config.save_to_file(CONFIG_FILE_PATH)
                
                if CURRENT_LANGUAGE == "en":
                    print(f"âœ… AI model '{ai_name}' ({model_name}) added successfully!")
                else:
                    print(f"âœ… AIæ¨¡å‹ '{ai_name}' ({model_name}) æ·»åŠ æˆåŠŸï¼")
            else:
                print("âŒ No models found" if CURRENT_LANGUAGE == "en" else "âŒ æœªæ‰¾åˆ°æ¨¡å‹")
        except Exception as e:
            print(f"âŒ Error: {e}")

    def _add_api_model(self):
        """æ·»åŠ APIæ¨¡å‹"""
        if CURRENT_LANGUAGE == "en":
            print("\nğŸŒ Configure API Model")
            print("\nSelect API provider:")
            print("  1. SiliconFlow (ç¡…åŸºæµåŠ¨)")
            print("  2. DeepSeek")
            print("  3. Volcengine Ark (ç«å±±å¼•æ“)")
            print("  4. OpenAI")
            print("  5. xAI (Grok)")
            print("  6. Google Gemini")
            print("  7. Anthropic Claude")
            print("  8. OpenRouter")
            print("  9. Custom (OpenAI compatible)")
        else:
            print("\nğŸŒ é…ç½®APIæ¨¡å‹")
            print("\né€‰æ‹©APIæä¾›æ–¹ï¼š")
            print("  1. ç¡…åŸºæµåŠ¨ (SiliconFlow)")
            print("  2. DeepSeek")
            print("  3. ç«å±±å¼•æ“ (Volcengine Ark)")
            print("  4. OpenAI")
            print("  5. xAI (Grok)")
            print("  6. Google Gemini")
            print("  7. Anthropic Claude")
            print("  8. OpenRouter (å¤šæ¨¡å‹èšåˆ)")
            print("  9. è‡ªå®šä¹‰ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰")
        
        provider_choice = input(">>> ").strip() or "9"
        
        provider_map = {
            "1": ("siliconflow", "https://api.siliconflow.cn/v1"),
            "2": ("deepseek", "https://api.deepseek.com/v1"),
            "3": ("volcengine", "https://ark.cn-beijing.volces.com/api/v3"),
            "4": ("openai", "https://api.openai.com/v1"),
            "5": ("xai", "https://api.x.ai/v1"),
            "6": ("gemini", "https://generativelanguage.googleapis.com/v1beta/openai"),
            "7": ("claude", "https://api.anthropic.com/v1"),
            "8": ("openrouter", "https://openrouter.ai/api/v1"),
            "9": ("custom", "https://api.openai.com/v1"),
        }
        provider, default_base = provider_map.get(provider_choice, provider_map["9"])
        
        # æ˜¾ç¤ºæä¾›æ–¹è¯´æ˜
        provider_info = {
            "siliconflow": ("ç¡…åŸºæµåŠ¨", "å›½å†…å¹³å°ï¼Œæ”¯æŒå¤šç§å¼€æºæ¨¡å‹", "https://cloud.siliconflow.cn/"),
            "deepseek": ("DeepSeek", "å›½å†…AIï¼Œæ¨ç†èƒ½åŠ›å¼º", "https://platform.deepseek.com/"),
            "volcengine": ("ç«å±±å¼•æ“", "å­—èŠ‚è·³åŠ¨æ——ä¸‹ï¼Œè±†åŒ…æ¨¡å‹", "https://console.volcengine.com/ark"),
            "openai": ("OpenAI", "GPTç³»åˆ—æ¨¡å‹", "https://platform.openai.com/"),
            "xai": ("xAI", "é©¬æ–¯å…‹çš„Grokæ¨¡å‹", "https://x.ai/"),
            "gemini": ("Google Gemini", "è°·æ­ŒAIæ¨¡å‹", "https://aistudio.google.com/"),
            "claude": ("Anthropic Claude", "Claudeç³»åˆ—æ¨¡å‹", "https://console.anthropic.com/"),
            "openrouter": ("OpenRouter", "å¤šæ¨¡å‹èšåˆå¹³å°ï¼Œä¸€ä¸ªAPIè®¿é—®å¤šç§æ¨¡å‹", "https://openrouter.ai/"),
        }
        
        if provider in provider_info:
            name, desc, url = provider_info[provider]
            if CURRENT_LANGUAGE == "en":
                print(f"\nğŸ“Œ {name}: {desc}")
                print(f"   Get API key: {url}")
            else:
                print(f"\nğŸ“Œ {name}ï¼š{desc}")
                print(f"   è·å–APIå¯†é’¥ï¼š{url}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„å¯†é’¥
        provider_key_mapping = {
            "siliconflow": config.siliconflow_api_key,
            "deepseek": config.deepseek_api_key,
            "volcengine": config.volcengine_api_key,
            "openai": getattr(config, 'openai_api_key', ''),
            "xai": getattr(config, 'xai_api_key', ''),
            "gemini": getattr(config, 'gemini_api_key', ''),
            "claude": getattr(config, 'claude_api_key', ''),
            "openrouter": getattr(config, 'openrouter_api_key', ''),
        }
        saved_key = provider_key_mapping.get(provider, "")
        
        # é…ç½®API
        if CURRENT_LANGUAGE == "en":
            base_url = input(f"API Base URL (default: {default_base}): ").strip() or default_base
        else:
            base_url = input(f"APIåŸºç¡€åœ°å€ï¼ˆé»˜è®¤: {default_base}ï¼‰: ").strip() or default_base
        
        api_url = f"{base_url.rstrip('/')}/chat/completions"
        
        # APIå¯†é’¥
        if saved_key:
            if CURRENT_LANGUAGE == "en":
                print(f"ğŸ”‘ Found saved API key for {provider}")
                use_saved = input("Use saved key? (Y/n): ").strip().lower() != 'n'
            else:
                print(f"ğŸ”‘ æ‰¾åˆ°å·²ä¿å­˜çš„ {provider} APIå¯†é’¥")
                use_saved = input("ä½¿ç”¨å·²ä¿å­˜çš„å¯†é’¥ï¼Ÿ(Y/n): ").strip().lower() != 'n'
            
            if use_saved:
                api_key = saved_key
            else:
                api_key = input("API Key: ").strip()
        else:
            api_key = input("API Key: ").strip()
        
        if not api_key:
            print("âŒ API key required" if CURRENT_LANGUAGE == "en" else "âŒ å¿…é¡»æä¾›APIå¯†é’¥")
            return
        
        # ä¿å­˜å¯†é’¥åˆ°å…¨å±€é…ç½®
        provider_key_attr = {
            "siliconflow": "siliconflow_api_key",
            "deepseek": "deepseek_api_key",
            "volcengine": "volcengine_api_key",
            "openai": "openai_api_key",
            "xai": "xai_api_key",
            "gemini": "gemini_api_key",
            "claude": "claude_api_key",
            "openrouter": "openrouter_api_key",
        }
        if provider in provider_key_attr:
            setattr(config, provider_key_attr[provider], api_key)
        
        # æ˜¾ç¤ºæ¨èæ¨¡å‹
        recommended_models = {
            "siliconflow": ["Qwen/Qwen2.5-7B-Instruct", "Qwen/Qwen2.5-32B-Instruct", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"],
            "deepseek": ["deepseek-chat", "deepseek-reasoner"],
            "volcengine": ["doubao-pro-32k", "doubao-lite-32k"],
            "openai": ["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo", "o1-mini"],
            "xai": ["grok-beta", "grok-2-1212"],
            "gemini": ["gemini-2.0-flash-exp", "gemini-1.5-pro", "gemini-1.5-flash"],
            "claude": ["claude-3-5-sonnet-20241022", "claude-3-opus-20240229", "claude-3-haiku-20240307"],
            "openrouter": ["openai/gpt-4o", "anthropic/claude-3.5-sonnet", "google/gemini-pro", "meta-llama/llama-3.1-70b-instruct"],
        }
        
        if provider in recommended_models:
            if CURRENT_LANGUAGE == "en":
                print(f"\nğŸ“‹ Recommended models for {provider}:")
            else:
                print(f"\nğŸ“‹ {provider} æ¨èæ¨¡å‹ï¼š")
            for i, model in enumerate(recommended_models[provider], 1):
                print(f"  {i}. {model}")
        
        # å°è¯•è·å–æ¨¡å‹åˆ—è¡¨
        if CURRENT_LANGUAGE == "en":
            model_name = input("\nModel name (enter number or type name): ").strip()
        else:
            model_name = input("\næ¨¡å‹åç§°ï¼ˆè¾“å…¥ç¼–å·æˆ–ç›´æ¥è¾“å…¥åç§°ï¼‰: ").strip()
        
        # å¦‚æœè¾“å…¥çš„æ˜¯æ•°å­—ï¼Œè½¬æ¢ä¸ºæ¨¡å‹å
        if model_name.isdigit() and provider in recommended_models:
            idx = int(model_name) - 1
            models = recommended_models[provider]
            if 0 <= idx < len(models):
                model_name = models[idx]
        
        if not model_name:
            print("âŒ Model name required" if CURRENT_LANGUAGE == "en" else "âŒ å¿…é¡»æä¾›æ¨¡å‹åç§°")
            return
        
        # AIåç§°
        if CURRENT_LANGUAGE == "en":
            ai_name = input(f"Enter a name for this AI (default: {model_name}): ").strip() or model_name
        else:
            ai_name = input(f"ä¸ºè¿™ä¸ªAIèµ·ä¸ªåå­—ï¼ˆé»˜è®¤: {model_name}ï¼‰: ").strip() or model_name
        
        # æ·»åŠ åˆ°é…ç½®
        new_ai = {
            "name": ai_name,
            "type": "api",
            "model": model_name,
            "api_config": {
                "provider": provider,
                "base_url": base_url,
                "api_url": api_url,
                "api_key": api_key,
                "model": model_name
            }
        }
        config.extra_ai_models.append(new_ai)
        config.save_to_file(CONFIG_FILE_PATH)
        
        if CURRENT_LANGUAGE == "en":
            print(f"âœ… API AI model '{ai_name}' added successfully!")
        else:
            print(f"âœ… API AIæ¨¡å‹ '{ai_name}' æ·»åŠ æˆåŠŸï¼")

    def _list_ai_models(self):
        """åˆ—å‡ºæ‰€æœ‰AIæ¨¡å‹"""
        if CURRENT_LANGUAGE == "en":
            DisplayManager.print_header("ğŸ“‹ All AI Models")
            print("\nğŸ”¹ Built-in Models:")
            print(f"  1. Model 1: {config.model_1} ({'API' if config.model_1_use_api else 'Ollama'})")
            print(f"  2. Model 2: {config.model_2} ({'API' if config.model_2_use_api else 'Ollama'})")
            print(f"  3. Coordinator: {config.coordinator_model} ({'API' if config.coordinator_use_api else 'Ollama'})")
        else:
            DisplayManager.print_header("ğŸ“‹ æ‰€æœ‰AIæ¨¡å‹")
            print("\nğŸ”¹ å†…ç½®æ¨¡å‹ï¼š")
            print(f"  1. æ¨¡å‹1: {config.model_1} ({'API' if config.model_1_use_api else 'Ollama'})")
            print(f"  2. æ¨¡å‹2: {config.model_2} ({'API' if config.model_2_use_api else 'Ollama'})")
            print(f"  3. åè°ƒæ¨¡å‹: {config.coordinator_model} ({'API' if config.coordinator_use_api else 'Ollama'})")
        
        if config.extra_ai_models:
            if CURRENT_LANGUAGE == "en":
                print("\nğŸ”¸ Additional Models:")
            else:
                print("\nğŸ”¸ é¢å¤–æ·»åŠ çš„æ¨¡å‹ï¼š")
            for i, ai in enumerate(config.extra_ai_models, 1):
                ai_type = ai.get("type", "unknown")
                ai_name = ai.get("name", "Unknown")
                model = ai.get("model", "Unknown")
                print(f"  {i}. {ai_name} ({model}) [{ai_type.upper()}]")
        else:
            if CURRENT_LANGUAGE == "en":
                print("\nğŸ”¸ No additional models added. Use /addai to add more.")
            else:
                print("\nğŸ”¸ æš‚æ— é¢å¤–æ·»åŠ çš„æ¨¡å‹ã€‚ä½¿ç”¨ /addai æ·»åŠ æ›´å¤šã€‚")
        
        DisplayManager.print_separator()

    def _remove_ai_model(self):
        """ç§»é™¤AIæ¨¡å‹"""
        if not config.extra_ai_models:
            if CURRENT_LANGUAGE == "en":
                print("âŒ No additional AI models to remove")
            else:
                print("âŒ æ²¡æœ‰å¯ç§»é™¤çš„é¢å¤–AIæ¨¡å‹")
            return
        
        if CURRENT_LANGUAGE == "en":
            DisplayManager.print_header("â– Remove AI Model")
            print("Select model to remove:")
        else:
            DisplayManager.print_header("â– ç§»é™¤AIæ¨¡å‹")
            print("é€‰æ‹©è¦ç§»é™¤çš„æ¨¡å‹ï¼š")
        
        for i, ai in enumerate(config.extra_ai_models, 1):
            print(f"  {i}. {ai.get('name', 'Unknown')} ({ai.get('model', '')})")
        
        if CURRENT_LANGUAGE == "en":
            print(f"  0. Cancel")
        else:
            print(f"  0. å–æ¶ˆ")
        
        choice = input(">>> ").strip()
        
        if choice == "0" or not choice:
            return
        
        if choice.isdigit():
            idx = int(choice) - 1
            if 0 <= idx < len(config.extra_ai_models):
                removed = config.extra_ai_models.pop(idx)
                config.save_to_file(CONFIG_FILE_PATH)
                if CURRENT_LANGUAGE == "en":
                    print(f"âœ… Model '{removed.get('name', '')}' removed")
                else:
                    print(f"âœ… æ¨¡å‹ '{removed.get('name', '')}' å·²ç§»é™¤")
            else:
                print("âŒ Invalid selection" if CURRENT_LANGUAGE == "en" else "âŒ æ— æ•ˆé€‰æ‹©")
        else:
            print("âŒ Invalid input" if CURRENT_LANGUAGE == "en" else "âŒ æ— æ•ˆè¾“å…¥")

    @staticmethod
    def _show_roles():
        """æ˜¾ç¤ºå¯ç”¨è§’è‰² (Show available roles)"""
        print("\nğŸ­ å¯ç”¨è§’è‰² (Available roles) [æ”¯æŒè¾“å…¥æ•°å­—é€‰æ‹©/Select by number]ï¼š")
        roles = role_system.get_all_roles()
        for i, role in enumerate(roles, 1):
            print(f"  {i}. {role}")

    @staticmethod
    def _show_tags():
        """æ˜¾ç¤ºæ ‡ç­¾ç³»ç»Ÿ (Show tag system)"""
        print("\nğŸ·ï¸  æ ‡ç­¾ç³»ç»Ÿ (Tag System)ï¼š")
        for tag, roles in TAG_TO_ROLES.items():
            print(f"  {tag}: {', '.join(roles)}")

    @staticmethod
    def _configure_consensus():
        """é…ç½®å…±è¯†æ£€æµ‹ (Configure consensus detection)"""
        print("\nğŸ¯ å…±è¯†æ£€æµ‹é…ç½® (Consensus Detection Config)")
        ai_status = "å¼€å¯/On" if config.ai_consensus_analysis else "å…³é—­/Off"
        sum_status = "å¼€å¯/On" if config.auto_summarize_at_threshold else "å…³é—­/Off"
        print(f"å½“å‰è®¾ç½® (Current settings)ï¼š")
        print(f"  - AIå…±è¯†åˆ†æ (AI consensus analysis): {ai_status}")
        print(f"  - è‡ªåŠ¨æ€»ç»“ (Auto summary): {sum_status}")
        print(f"  - å…±è¯†é˜ˆå€¼ (Consensus threshold): {int(config.consensus_threshold * 100)}%")
        print(f"  - æ£€æµ‹èµ·å§‹å›åˆ (Start round): {config.consensus_check_start_round}")

        print(f"\né€‰é¡¹ (Options)ï¼š")
        ai_cur = "å¼€/On" if config.ai_consensus_analysis else "å…³/Off"
        sum_cur = "å¼€/On" if config.auto_summarize_at_threshold else "å…³/Off"
        print(f"  1. åˆ‡æ¢AIå…±è¯†åˆ†æ (Toggle AI analysis) [å½“å‰/Current: {ai_cur}]")
        print(f"  2. åˆ‡æ¢è‡ªåŠ¨æ€»ç»“ (Toggle auto summary) [å½“å‰/Current: {sum_cur}]")
        print(f"  3. è®¾ç½®å…±è¯†é˜ˆå€¼ (Set threshold) [å½“å‰/Current: {int(config.consensus_threshold * 100)}%]")
        print(f"  4. è®¾ç½®æ£€æµ‹èµ·å§‹å›åˆ (Set start round) [å½“å‰/Current: {config.consensus_check_start_round}]")

        choice = input("é€‰æ‹©/Select (1-4) æˆ–å›è½¦è¿”å›/Enter to return: ").strip()

        if choice == '1':
            config.ai_consensus_analysis = not config.ai_consensus_analysis
            status = "å¼€å¯/Enabled" if config.ai_consensus_analysis else "å…³é—­/Disabled"
            print(f"âœ… AIå…±è¯†åˆ†æ (AI consensus analysis)ï¼š{status}")
        elif choice == '2':
            config.auto_summarize_at_threshold = not config.auto_summarize_at_threshold
            status = "å¼€å¯/Enabled" if config.auto_summarize_at_threshold else "å…³é—­/Disabled"
            print(f"âœ… è‡ªåŠ¨æ€»ç»“ (Auto summary)ï¼š{status}")
        elif choice == '3':
            try:
                threshold = float(input("è¾“å…¥æ–°é˜ˆå€¼/Enter new threshold (0-100): ").strip()) / 100.0
                if 0.0 <= threshold <= 1.0:
                    config.consensus_threshold = threshold
                    print(f"âœ… å…±è¯†é˜ˆå€¼å·²è®¾ç½®ä¸º (Threshold set to) {int(threshold * 100)}%")
                else:
                    print("âŒ é˜ˆå€¼å¿…é¡»åœ¨ 0-100 ä¹‹é—´ (Threshold must be 0-100)")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­— (Please enter a valid number)")
        elif choice == '4':
            try:
                round_num = int(input("è¾“å…¥èµ·å§‹å›åˆæ•°/Enter start round (1-6): ").strip())
                if 1 <= round_num <= 6:
                    config.consensus_check_start_round = round_num
                    print(f"âœ… æ£€æµ‹èµ·å§‹å›åˆå·²è®¾ç½®ä¸ºç¬¬{round_num}å›åˆ (Start round set to {round_num})")
                else:
                    print("âŒ å›åˆæ•°å¿…é¡»åœ¨ 1-6 ä¹‹é—´ (Round must be 1-6)")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­— (Please enter a valid number)")
        elif choice == '':
            return
        else:
            print("âŒ æ— æ•ˆé€‰æ‹© (Invalid selection)")

    @staticmethod
    def _toggle_coordination_mode():
        """åˆ‡æ¢åè°ƒæ¨¡å¼ (Toggle coordination mode)"""
        current = config.coordination_mode
        new_mode = "user" if current == "auto" else "auto"
        config.coordination_mode = new_mode
        print(f"âœ… åè°ƒæ¨¡å¼å·²åˆ‡æ¢ (Coordination mode switched)ï¼š{current} -> {new_mode}")

    def _handle_interrupt(self):
        """å¤„ç†ä¸­æ–­ä¿¡å· (Handle interrupt signal)"""
        print("\n\nâš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å· (Interrupt signal detected)")
        choice = input("æ˜¯å¦é€€å‡ºç¨‹åºï¼Ÿ(Exit program?) (y/N): ").strip().lower()
        if choice == 'y':
            self._exit_program()

    def _configure_api_mode(self):
        """é…ç½®APIæ¨¡å¼ (Configure API mode)"""
        DisplayManager.print_header("ğŸ”— APIæ¨¡å¼é…ç½® (API Mode Configuration)")
        api_status = "å·²å¯ç”¨/Enabled" if config.api_mode_enabled else "æœªå¯ç”¨/Disabled"
        key_status = "å·²è®¾ç½®/Set" if config.api_key else "æœªè®¾ç½®/Not set"
        m1_api = "æ˜¯/Yes" if config.model_1_use_api else "å¦/No"
        m2_api = "æ˜¯/Yes" if config.model_2_use_api else "å¦/No"
        coord_api = "æ˜¯/Yes" if config.coordinator_use_api else "å¦/No"
        
        print(f"å½“å‰APIæ¨¡å¼çŠ¶æ€ (Current API mode)ï¼š{api_status}")
        print(f"APIæä¾›æ–¹ (API Provider)ï¼š{getattr(config, 'api_provider', 'custom')}")
        print(f"APIåŸºç¡€åœ°å€ (API Base URL)ï¼š{getattr(config, 'api_base_url', '')}")
        print(f"APIåœ°å€ (API URL)ï¼š{config.api_url}")
        print(f"APIæ¨¡å‹ (API Model)ï¼š{config.api_model}")
        print(f"APIå¯†é’¥ (API Key)ï¼š{key_status}")
        print(f"æ¨¡å‹1ä½¿ç”¨API (Model 1 uses API)ï¼š{m1_api}")
        print(f"æ¨¡å‹2ä½¿ç”¨API (Model 2 uses API)ï¼š{m2_api}")
        print(f"åè°ƒAIä½¿ç”¨API (Coordinator uses API)ï¼š{coord_api}")
        DisplayManager.print_separator()
        enable_api = InputValidator.get_yes_no_input("æ˜¯å¦å¯ç”¨APIæ¨¡å¼ï¼Ÿ(Enable API mode?) (y/n): ", default=config.api_mode_enabled)
        if enable_api:
            # é€ä¸ªé…ç½®ï¼šæ¨¡å‹1ã€æ¨¡å‹2ã€åè°ƒAI
            any_use_api = False
            if CURRENT_LANGUAGE == "en":
                targets = [
                    ("Model 1", "model_1"),
                    ("Model 2", "model_2"),
                    ("Coordinator", "coordinator"),
                ]
            else:
                targets = [
                    ("æ¨¡å‹1", "model_1"),
                    ("æ¨¡å‹2", "model_2"),
                    ("åè°ƒAI", "coordinator"),
                ]

            for label, key in targets:
                print("\n" + "-" * 40)
                if CURRENT_LANGUAGE == "en":
                    print(f"âš™ï¸  Configure API for {label}")
                else:
                    print(f"âš™ï¸  é…ç½® {label} çš„APIå‚æ•°")
                use_api_attr = f"{key}_use_api"
                current_use = getattr(config, use_api_attr, False)
                if CURRENT_LANGUAGE == "en":
                    use_api = InputValidator.get_yes_no_input(
                        f"Use external API for {label}? (Current: {'Yes' if current_use else 'No'})", default=current_use
                    )
                else:
                    use_api = InputValidator.get_yes_no_input(
                        f"{label} æ˜¯å¦ä½¿ç”¨å¤–éƒ¨APIï¼Ÿï¼ˆå½“å‰: {'æ˜¯' if current_use else 'å¦'}ï¼‰", default=current_use
                    )
                setattr(config, use_api_attr, use_api)

                if not use_api:
                    continue

                any_use_api = True

                # é€‰æ‹©æä¾›æ–¹
                provider_attr = f"{key}_api_provider"
                base_attr = f"{key}_api_base_url"
                url_attr = f"{key}_api_url"
                key_attr = f"{key}_api_key"
                model_attr = f"{key}_api_model"

                current_provider = getattr(config, provider_attr, "") or "custom"
                if CURRENT_LANGUAGE == "en":
                    print(f"\nğŸ¢ Select API provider for {label} (Current: {current_provider}):")
                    print("  1. SiliconFlow")
                    print("  2. DeepSeek")
                    print("  3. Volcengine Ark")
                    print("  4. Custom (OpenAI compatible)")
                    provider_choice = input("Enter number (1-4, Enter for current/custom): ").strip() or "4"
                else:
                    print(f"\nğŸ¢ ä¸º {label} é€‰æ‹©APIæä¾›æ–¹ï¼ˆå½“å‰: {current_provider}ï¼‰ï¼š")
                    print("  1. ç¡…åŸºæµåŠ¨ (SiliconFlow)")
                    print("  2. DeepSeek")
                    print("  3. ç«å±±å¼•æ“ (Volcengine Ark)")
                    print("  4. è‡ªå®šä¹‰ (å…¼å®¹OpenAIæ ¼å¼)")
                    provider_choice = input("è¾“å…¥ç¼–å·(1-4ï¼Œå›è½¦ä¿æŒå½“å‰/è‡ªå®šä¹‰): ").strip() or "4"

                provider_map = {
                    "1": ("siliconflow", "https://api.siliconflow.cn/v1"),
                    "2": ("deepseek", "https://api.deepseek.com/v1"),
                    "3": ("volcengine", "https://ark.cn-beijing.volces.com/api/v3"),
                    "4": (current_provider or "custom", getattr(config, base_attr, "") or getattr(config, "api_base_url", "https://api.openai.com/v1") or "https://api.openai.com/v1"),
                }
                provider, default_base = provider_map.get(provider_choice, provider_map["4"])
                setattr(config, provider_attr, provider)

                # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥æä¾›æ–¹çš„å·²ä¿å­˜å¯†é’¥ï¼ˆä»å…¨å±€æˆ–å…¶ä»–æ¨¡å‹é…ç½®ä¸­æŸ¥æ‰¾ï¼‰
                saved_keys_for_provider = {}
                provider_key_mapping = {
                    "siliconflow": "siliconflow_api_key",
                    "deepseek": "deepseek_api_key", 
                    "volcengine": "volcengine_api_key",
                }
                
                # æŸ¥æ‰¾å·²ä¿å­˜çš„å¯†é’¥
                global_saved_key = getattr(config, provider_key_mapping.get(provider, ""), "")
                existing_key_for_this = getattr(config, key_attr, "")
                
                # ä»å…¶ä»–æ¨¡å‹é…ç½®ä¸­æŸ¥æ‰¾åŒä¸€æä¾›æ–¹çš„å¯†é’¥
                for other_key in ["model_1", "model_2", "coordinator"]:
                    if other_key != key:
                        other_provider = getattr(config, f"{other_key}_api_provider", "")
                        if other_provider == provider:
                            other_key_value = getattr(config, f"{other_key}_api_key", "")
                            if other_key_value:
                                saved_keys_for_provider[other_key] = other_key_value
                
                # é…ç½®åŸºç¡€åœ°å€
                current_base = getattr(config, base_attr, "") or default_base
                if CURRENT_LANGUAGE == "en":
                    print("\nğŸ”§ Configure API Base URL:")
                    base_url = input(f"{label} API Base URL (Current: {current_base}): ").strip()
                else:
                    print("\nğŸ”§ é…ç½®APIåŸºç¡€åœ°å€ï¼š")
                    base_url = input(f"{label} APIåŸºç¡€åœ°å€ (å½“å‰: {current_base}): ").strip()
                if not base_url:
                    base_url = current_base
                base_url = base_url.rstrip("/")
                setattr(config, base_attr, base_url)

                # chat completions endpoint
                default_chat_url = f"{base_url}/chat/completions"
                current_chat = getattr(config, url_attr, "") or default_chat_url
                if CURRENT_LANGUAGE == "en":
                    api_url = input(f"{label} ChatCompletions URL (Current: {current_chat}): ").strip()
                else:
                    api_url = input(f"{label} ChatCompletionsåœ°å€ (å½“å‰: {current_chat}): ").strip()
                api_url = (api_url or current_chat).rstrip("/")
                setattr(config, url_attr, api_url)

                # API Keyï¼šæä¾›ä½¿ç”¨å·²ä¿å­˜å¯†é’¥æˆ–è¾“å…¥æ–°å¯†é’¥çš„é€‰é¡¹
                existing_key = existing_key_for_this or global_saved_key or config.api_key
                
                # å¦‚æœæœ‰å·²ä¿å­˜çš„å¯†é’¥ï¼ˆæ¥è‡ªåŒä¸€æä¾›æ–¹çš„å…¶ä»–é…ç½®ï¼‰
                if saved_keys_for_provider or existing_key:
                    if CURRENT_LANGUAGE == "en":
                        print(f"\nğŸ”‘ API Key Configuration:")
                        print("  1. Use saved key" + (" âœ… Key exists" if existing_key else ""))
                        if saved_keys_for_provider:
                            print(f"     (Same provider configured for: {', '.join(saved_keys_for_provider.keys())})")
                        print("  2. Enter new key")
                        key_choice = input("Select (1/2, Enter for saved): ").strip() or "1"
                    else:
                        print(f"\nğŸ”‘ APIå¯†é’¥é…ç½®ï¼š")
                        print("  1. ä½¿ç”¨å·²ä¿å­˜çš„å¯†é’¥" + (" âœ… å½“å‰å·²æœ‰å¯†é’¥" if existing_key else ""))
                        if saved_keys_for_provider:
                            print(f"     (åŒæä¾›æ–¹å…¶ä»–æ¨¡å‹å·²é…ç½®: {', '.join(saved_keys_for_provider.keys())})")
                        print("  2. è¾“å…¥æ–°çš„å¯†é’¥")
                        key_choice = input("è¯·é€‰æ‹© (1/2ï¼Œå›è½¦ä½¿ç”¨å·²ä¿å­˜): ").strip() or "1"
                    
                    if key_choice == "2":
                        if CURRENT_LANGUAGE == "en":
                            api_key_input = input(f"Enter API key for {label}: ").strip()
                        else:
                            api_key_input = input(f"è¯·è¾“å…¥ {label} çš„APIå¯†é’¥: ").strip()
                        if api_key_input:
                            setattr(config, key_attr, api_key_input)
                            # åŒæ—¶ä¿å­˜åˆ°æä¾›æ–¹å…¨å±€å¯†é’¥
                            if provider in provider_key_mapping:
                                setattr(config, provider_key_mapping[provider], api_key_input)
                            existing_key = api_key_input
                    else:
                        # ä½¿ç”¨å·²ä¿å­˜çš„å¯†é’¥
                        if not existing_key and saved_keys_for_provider:
                            # ä½¿ç”¨åŒä¸€æä¾›æ–¹å…¶ä»–æ¨¡å‹çš„å¯†é’¥
                            existing_key = list(saved_keys_for_provider.values())[0]
                        if existing_key:
                            setattr(config, key_attr, existing_key)
                            if CURRENT_LANGUAGE == "en":
                                print(f"   âœ… Using saved key")
                            else:
                                print(f"   âœ… å·²ä½¿ç”¨ä¿å­˜çš„å¯†é’¥")
                else:
                    # æ²¡æœ‰å·²ä¿å­˜çš„å¯†é’¥ï¼Œç›´æ¥è¾“å…¥
                    if CURRENT_LANGUAGE == "en":
                        api_key_input = input(f"{label} API Key: ").strip()
                    else:
                        api_key_input = input(f"{label} APIå¯†é’¥: ").strip()
                    if api_key_input:
                        setattr(config, key_attr, api_key_input)
                        # åŒæ—¶ä¿å­˜åˆ°æä¾›æ–¹å…¨å±€å¯†é’¥
                        if provider in provider_key_mapping:
                            setattr(config, provider_key_mapping[provider], api_key_input)
                        existing_key = api_key_input

                # å…ˆå°è¯•æ‹‰å–è¯¥æä¾›æ–¹çš„æ¨¡å‹åˆ—è¡¨
                models: List[str] = []
                if existing_key:
                    temp_client = APIClient(api_url=api_url, api_key=existing_key,
                                            model_name=getattr(config, model_attr, "") or config.api_model,
                                            timeout=config.timeout)
                    models = temp_client.list_models()

                current_model = getattr(config, model_attr, "") or config.api_model
                if models:
                    if CURRENT_LANGUAGE == "en":
                        print("\nğŸ“¦ Available models:")
                    else:
                        print("\nğŸ“¦ è·å–åˆ°å¯ç”¨æ¨¡å‹ï¼š")
                    for i, mid in enumerate(models, 1):
                        print(f"  {i}. {mid}")
                    if CURRENT_LANGUAGE == "en":
                        model_choice = input(f"{label} Select model (1-{len(models)}), or enter name (Enter to keep {current_model}): ").strip()
                    else:
                        model_choice = input(f"{label} é€‰æ‹©æ¨¡å‹ç¼–å·(1-{len(models)})ï¼Œæˆ–ç›´æ¥è¾“å…¥æ¨¡å‹å(å›è½¦ä¿ç•™å½“å‰ {current_model}): ").strip()
                    if model_choice.isdigit():
                        idx = int(model_choice)
                        if 1 <= idx <= len(models):
                            setattr(config, model_attr, models[idx - 1])
                    elif model_choice:
                        setattr(config, model_attr, model_choice)
                else:
                    if CURRENT_LANGUAGE == "en":
                        print(f"\nâš ï¸  Cannot auto-fetch model list for {label} (platform may not support /models, or key/network issue).")
                        api_model_input = input(f"Enter model name for {label} (Current: {current_model}): ").strip()
                    else:
                        print(f"\nâš ï¸  æ— æ³•è‡ªåŠ¨è·å– {label} çš„æ¨¡å‹åˆ—è¡¨ï¼ˆè¯¥å¹³å°å¯èƒ½ä¸æ”¯æŒ /modelsï¼Œæˆ–Key/ç½‘ç»œé—®é¢˜ï¼‰ã€‚")
                        api_model_input = input(f"è¯·è¾“å…¥ {label} ä½¿ç”¨çš„æ¨¡å‹åç§° (å½“å‰: {current_model}): ").strip()
                    if api_model_input:
                        setattr(config, model_attr, api_model_input)

            # è‹¥è‡³å°‘æœ‰ä¸€ä¸ªAIä½¿ç”¨APIï¼Œåˆ™è®¤ä¸ºAPIæ¨¡å¼å¼€å¯
            config.api_mode_enabled = any_use_api
            if not any_use_api:
                if CURRENT_LANGUAGE == "en":
                    print("âš ï¸  No AI configured to use API, disabling API mode, using local Ollama only.")
                else:
                    print("âš ï¸  æ‰€æœ‰AIéƒ½æœªé…ç½®ä½¿ç”¨APIï¼Œå°†å…³é—­APIæ¨¡å¼ï¼Œä»…ä½¿ç”¨æœ¬åœ°Ollamaã€‚")

            # ä¿å­˜é…ç½®
            config.save_to_file("macp_config.json")
            if CURRENT_LANGUAGE == "en":
                print("âœ… API configuration saved")
            else:
                print("âœ… APIé…ç½®å·²ä¿å­˜")

            # é‡æ–°åˆå§‹åŒ–è°ƒåº¦å™¨ä»¥åº”ç”¨æ–°é…ç½®
            if CURRENT_LANGUAGE == "en":
                print("\nğŸ”„ Reinitializing system...")
            else:
                print("\nğŸ”„ æ­£åœ¨é‡æ–°åˆå§‹åŒ–ç³»ç»Ÿ...")
            try:
                # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨å®ä¾‹
                new_scheduler = AICouncilScheduler()
                self.scheduler = new_scheduler
                if CURRENT_LANGUAGE == "en":
                    print("âœ… System reinitialized successfully")
                else:
                    print("âœ… ç³»ç»Ÿé‡æ–°åˆå§‹åŒ–å®Œæˆ")
            except (AICouncilException, requests.exceptions.RequestException, ValueError) as e:
                print(f"âŒ é‡æ–°åˆå§‹åŒ–å¤±è´¥ (Reinitialization failed): {e}")

        else:
            config.api_mode_enabled = False
            print("âœ… å·²ç¦ç”¨APIæ¨¡å¼ (API mode disabled)")

        DisplayManager.print_separator()

    def _exit_program(self):
        """é€€å‡ºç¨‹åº (Exit program)"""
        print(f"\nğŸ“Š ä¼šè¯ç»Ÿè®¡ (Session Statistics)ï¼š")
        print(f"  ä¼šè¯ID (Session ID)ï¼š{self.scheduler.session_id}")
        print(f"  æ€»è®°å½•æ•° (Total Records)ï¼š{len(self.scheduler.history_manager.history)}")
        print("\nğŸ‘‹ å†è§ï¼(Goodbye!)")

        # æ¸…ç†èµ„æº
        self.scheduler.cleanup()

        sys.exit(0)

    def _select_debate_roles(self) -> Tuple[str, str]:
        """é€‰æ‹©è¾©è®ºè§’è‰²"""
        self._show_roles()
        role1_input = input(f"\næ¨¡å‹1è§’è‰²ï¼ˆé»˜è®¤ï¼š{config.default_role_1}ï¼‰: ").strip()
        role2_input = input(f"æ¨¡å‹2è§’è‰²ï¼ˆé»˜è®¤ï¼š{config.default_role_2}ï¼‰: ").strip()

        role1 = InputValidator.validate_role_input(role1_input, role_system.get_all_roles())
        role2 = InputValidator.validate_role_input(role2_input, role_system.get_all_roles())
        return role1, role2

    @staticmethod
    def _handle_consensus_feedback(consensus_score: float, consensus_percentage: int,
                                 threshold_percentage: int, current_consensus_reached: bool) -> bool:
        """å¤„ç†AIå…±è¯†åˆ†æç»“æœå¹¶å†³å®šè¾©è®ºè¿›ç¨‹

        æ ¹æ®AIçš„å…±è¯†åˆ†æç»“æœå‘ç”¨æˆ·å±•ç¤ºå½“å‰è¾©è®ºçŠ¶æ€ï¼š
        1. æ˜¾ç¤ºå…±è¯†åº¦æ¡å½¢å›¾ï¼ˆè§†è§‰åŒ–è¡¨ç¤ºï¼‰
        2. æ ¹æ®å…±è¯†åº¦ç»™å‡ºä¸åŒçš„è¿›åº¦åé¦ˆ
        3. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è‡ªåŠ¨ç»“æŸé˜ˆå€¼ï¼ˆ70%ï¼‰
        4. è¿”å›æ˜¯å¦åº”è¯¥ç»“æŸè¾©è®ºçš„å†³ç­–

        è¿™æ˜¯å®ç°"æ™ºèƒ½è¾©è®ºç»“æŸ"çš„å…³é”®ç¯èŠ‚ï¼Œ
        è®©ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®AIçš„è¯­ä¹‰ç†è§£åšå‡ºåˆç†å†³ç­–
        """
        if consensus_score >= config.consensus_threshold:
            logger.info(f"âœ… å…±è¯†åº¦è¾¾æ ‡ ({consensus_percentage}%)ï¼Œè‡ªåŠ¨ç»“æŸè¾©è®º")
            print(f"\nğŸ¯ å…±è¯†åº¦å·²è¾¾åˆ°{consensus_percentage}%ï¼ˆâ‰¥{threshold_percentage}%é˜ˆå€¼ï¼‰ï¼Œè‡ªåŠ¨ç»“æŸè¾©è®ºå¹¶ç”Ÿæˆæ€»ç»“")
            return True
        elif consensus_score >= 0.5:
            remaining_to_threshold = threshold_percentage - consensus_percentage
            print(f"ğŸ“ˆ å…±è¯†åº¦{consensus_percentage}%ï¼Œè·ç¦»é˜ˆå€¼è¿˜å·®{remaining_to_threshold}%ï¼Œè¾©è®ºç»§ç»­...")
        else:
            print(f"âš–ï¸  å…±è¯†åº¦{consensus_percentage}%ï¼Œåˆ†æ­§æ˜æ˜¾ï¼Œç»§ç»­æ·±å…¥è¾©è®º...")

        return current_consensus_reached

# ==================== ã€ä¸»å‡½æ•°ã€‘ ====================
def main():
    """MACPç³»ç»Ÿä¸»å…¥å£å‡½æ•°

    æ‰§è¡Œå®Œæ•´çš„ç³»ç»Ÿåˆå§‹åŒ–å’Œè¿è¡Œæµç¨‹ï¼š
    1. æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯å’Œç‰ˆæœ¬å·
    2. åˆå§‹åŒ–AICouncilSchedulerï¼ˆæ ¸å¿ƒè°ƒåº¦å™¨ï¼‰
    3. åˆ›å»ºInteractiveInterfaceï¼ˆç”¨æˆ·ç•Œé¢ï¼‰
    4. å¯åŠ¨äº¤äº’å¼å‘½ä»¤å¾ªç¯
    5. å¤„ç†ç³»ç»Ÿå¼‚å¸¸å’Œæ¸…ç†èµ„æº

    è¿™æ˜¯æ•´ä¸ªåº”ç”¨ç¨‹åºçš„å¯åŠ¨ç‚¹ï¼Œè´Ÿè´£åè°ƒæ‰€æœ‰ç»„ä»¶çš„åˆå§‹åŒ–
    """
    print("ğŸš€ å¯åŠ¨MACPå¤šAIåä½œå¹³å° v5.0 - ç»ˆæä¼˜åŒ–ç‰ˆ")
    print("=" * 80)
    print("æ–°å¢åŠŸèƒ½ï¼š")
    print("1. âœ… æ¨¡å—åŒ–æ¶æ„ - ä»£ç æ›´æ¸…æ™°")
    print("2. âœ… å¢å¼ºé”™è¯¯å¤„ç† - æ›´ç¨³å®š")
    print("3. âœ… æ—¥å¿—ç³»ç»Ÿ - ä¾¿äºè°ƒè¯•")
    print("4. âœ… æ€§èƒ½ç›‘æ§ - å®æ—¶è·Ÿè¸ª")
    print("5. âœ… é…ç½®ç®¡ç† - åŠ¨æ€åŠ è½½")
    print("6. âœ… ä»£ç ä¼˜åŒ– - å‡å°‘å†—ä½™")
    print("7. âœ… ç±»å‹æ³¨è§£ - æ›´å¥½çš„ç»´æŠ¤æ€§")
    print("=" * 80)

    try:
        # åˆå§‹åŒ–è°ƒåº¦å™¨
        scheduler = AICouncilScheduler()

        # å¯åŠ¨äº¤äº’ç•Œé¢
        interface = InteractiveInterface(scheduler)
        interface.run()

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=e)
        print(f"\nâŒ ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ä»¥è·å–è¯¦ç»†ä¿¡æ¯")
        sys.exit(1)

if __name__ == "__main__":
    main()
